<!doctype html><html lang=zh-CN data-theme=light><head><meta charset=UTF-8><meta name=viewport content="width=device-width"><meta name=theme-color content="#222" media="(prefers-color-scheme: light)"><meta name=generator content="Hugo 0.143.1"><link rel="shortcut icon" type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=/imgs/icons/favicon_16x16_next.png><link rel=icon type=image/png sizes=32x32 href=/imgs/icons/favicon_32_32_next.png><link rel=apple-touch-icon sizes=180x180 href=/imgs/icons/apple_touch_icon_next.png><meta itemprop=name content="CUDA_DEVICE_MAX_CONNECTIONS"><meta itemprop=description content="个人博客，主要是零散的笔记。"><meta itemprop=datePublished zgotmplz><meta itemprop=dateModified zgotmplz><meta itemprop=image content="https://hxhue.github.io/imgs/371907.jpg"><meta itemprop=keywords content="ai-infra"><link type=text/css rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.2/css/all.min.css><link type=text/css rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css><link rel=stylesheet href=/css/main.min.bea76f574a755574e17d42bea39502a74ca3ca4db65807b8c82d3e26dcec8420.css><style type=text/css>.post-footer,.flinks-list-footer hr:after{content:"~ 我可是有底线的哟 ~"}</style><link rel=stylesheet type=text/css href=/css/style.css><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/github-markdown-css@5.3.0/github-markdown-dark.css><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js></script><script>MathJax={tex:{displayMath:[["$$","$$"],["\\[","\\]"]],inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"}}</script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.esm.min.mjs';
  mermaid.initialize({ startOnLoad: false });
  mermaid.mermaidAPI.initialize();
  window.mermaid = mermaid;
</script><script type=text/javascript>(function(){localDB={set:function(e,t,n){if(n===0)return;const s=new Date,o=n*864e5,i={value:t,expiry:s.getTime()+o};localStorage.setItem(e,JSON.stringify(i))},get:function(e){const t=localStorage.getItem(e);if(!t)return void 0;const n=JSON.parse(t),s=new Date;return s.getTime()>n.expiry?(localStorage.removeItem(e),void 0):n.value}},theme={active:function(){const e=localDB.get("theme");if(e==null)return;theme.toggle(e),window.matchMedia("(prefers-color-scheme: dark)").addListener(function(e){theme.toggle(e.matches?"dark":"light")})},toggle:function(e){document.documentElement.setAttribute("data-theme",e),localDB.set("theme",e,2);const t=document.querySelector("iframe.giscus-frame");if(t){const n={setConfig:{theme:e}};t.contentWindow.postMessage({giscus:n},"https://giscus.app")}}},theme.active()})(window)</script><script class=next-config data-name=page type=application/json>{"comments":false,"isHome":false,"isPage":true,"path":"CUDA_DEVICE_MAX_CONNECTIONS","permalink":"https://hxhue.github.io/posts/ai-infra/CUDA_DEVICE_MAX_CONNECTIONS/","title":"CUDA_DEVICE_MAX_CONNECTIONS","waline":{"js":[{"alias":"waline","alias_name":"@waline/client","file":"dist/pageview.js","name":"pageview","version":"2.13.0"},{"alias":"waline","alias_name":"@waline/client","file":"dist/comment.js","name":"comment","version":"2.13.0"}]}}</script><script type=text/javascript>document.addEventListener("DOMContentLoaded",()=>{var e=document.createElement("script");e.charset="UTF-8",e.src="",e.async=!1,e.defer=!0,document.head.appendChild(e),e.onload=function(){NexT.utils.fmtBusuanzi()}})</script><title>CUDA_DEVICE_MAX_CONNECTIONS - Bluegill</title><noscript><link rel=stylesheet href=/css/noscript.css></noscript></head><body itemscope itemtype=http://schema.org/WebPage><div class=headband></div><main class=main><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div class=toggle aria-label role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div></div><div class=site-meta><a href=/ class=brand rel=start><i class=logo-line></i><h1 class=site-title>Bluegill</h1><i class=logo-line></i></a><p class=site-subtitle itemprop=description></p></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href=/ class=hvr-icon-pulse rel=section><i class="fa fa-home hvr-icon"></i>首页</a></li><li class="menu-item menu-item-about"><a href=/about/ class=hvr-icon-pulse rel=section><i class="fa fa-user hvr-icon"></i>关于</a></li><li class="menu-item menu-item-archives"><a href=/archives/ class=hvr-icon-pulse rel=section><i class="fa fa-archive hvr-icon"></i>归档</a></li><li class="menu-item menu-item-categories"><a href=/categories/ class=hvr-icon-pulse rel=section><i class="fa fa-th hvr-icon"></i>分类</a></li><li class="menu-item menu-item-tags"><a href=/tags/ class=hvr-icon-pulse rel=section><i class="fa fa-hashtag hvr-icon"></i>标签</a></li><li class="menu-item menu-item-daily"><a href=/daily/ class=hvr-icon-pulse rel=section><i class="fa fa-newspaper hvr-icon"></i>随笔</a></li><li class="menu-item menu-item-discovery"><a href=https://rift-fear-f2c.notion.site/2025-1e354a33cfb1802c841bdf29f2f3dab3 class=hvr-icon-pulse rel=section><i class="fa fa-compass hvr-icon"></i>发现</a></li><li class="menu-item menu-item-search"><a role=button class="popup-trigger hvr-icon-pulse"><i class="fa fa-search fa-fw hvr-icon"></i>搜索</a></li></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon><i class="fa fa-search"></i></span><div class=search-input-container><input autocomplete=off autocapitalize=off maxlength=80 placeholder=搜索... spellcheck=false type=search class=search-input></div><span class=popup-btn-close role=button><i class="fa fa-times-circle"></i></span></div><div class="search-result-container no-result"><div class=search-result-icon><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></div><div class="toggle sidebar-toggle" role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div><aside class=sidebar><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class=sidebar-nav><li class=sidebar-nav-toc>文章目录</li><li class=sidebar-nav-overview>站点概览</li></ul><div class=sidebar-panel-container><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><nav id=TableOfContents><ul><li><a href=#环境变量解释>环境变量解释</a></li><li><a href=#megatron-lm-1f1b--ep-a2a-overlap>Megatron-LM 1F1B + EP A2A overlap</a></li><li><a href=#知乎---ring-attention-性能问题引发的计算通信-overlap-分析>知乎 - Ring Attention 性能问题引发的计算通信 overlap 分析</a></li></ul></nav></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author site-overview-item animated" itemprop=author itemscope itemtype=http://schema.org/Person><img class=site-author-image itemprop=image alt=🤖 src=/imgs/371907.jpg><p class=site-author-name itemprop=name>🤖</p><div class=site-description itemprop=description>个人博客，主要是零散的笔记。</div></div><div class="site-state-wrap site-overview-item animated"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/><span class=site-state-item-count>433</span>
<span class=site-state-item-name>日志</span></a></div><div class="site-state-item site-state-categories"><a href=/categories/><span class=site-state-item-count>12</span>
<span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/><span class=site-state-item-count>86</span>
<span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-social site-overview-item animated"><span class=links-of-social-item><a href=https://github.com/hxhue title="Github → https://github.com/hxhue" rel=noopener target=_blank><i class="fab fa-github fa-fw"></i>
Github
</a></span><span class=links-of-social-item><a href=/rss.xml title="RSS 订阅 → /rss.xml" rel=noopener target=_blank><i class="fa fa-rss fa-fw"></i>
RSS 订阅</a></span></div><div class="cc-license animated" itemprop=license><a href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh class=cc-opacity rel=noopener target=_blank title=共享知识><img src=/imgs/cc/big/by_nc_sa.svg alt=共享知识></a></div><div class="links-of-blogroll site-overview-item animated"><div class=links-of-blogroll-title><i class="fa fa-globe fa-fw"></i>
友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=https://shuai.guru/ title=https://shuai.guru/ target=_blank>shuai.guru</a></li></ul></div></div></div></div></aside><div class=sidebar-dimmer></div></header><div class=tool-buttons><div id=toggle-theme class=button title=深浅模式切换><i class="fas fa-adjust"></i></div><div class=back-to-top role=button title=返回顶部><i class="fa fa-arrow-up"></i>
<span>0%</span></div></div><div class=reading-progress-bar></div><noscript><div class=noscript-warning>Theme NexT works best with JavaScript enabled</div></noscript><div class="main-inner post posts-expand"><div class=post-block><article itemscope itemtype=http://schema.org/Article class=post-content lang><link itemprop=mainEntityOfPage href=https://hxhue.github.io/posts/ai-infra/CUDA_DEVICE_MAX_CONNECTIONS/><span hidden itemprop=author itemscope itemtype=http://schema.org/Person><meta itemprop=image content="/imgs/371907.jpg"><meta itemprop=name content="🤖"></span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization><meta itemprop=name content="🤖"><meta itemprop=description content="个人博客，主要是零散的笔记。"></span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork><meta itemprop=name content="CUDA_DEVICE_MAX_CONNECTIONS"><meta itemprop=description content="环境变量解释

CUDA_DEVICE_MAX_CONNECTIONS 这个环境变量在 Megatron-LM 源码中经常见到，主要是用来控制硬件调度 kernel 的顺序，以尽可能提高通信计算重叠场景的重叠率。
英伟达文档 cuda-environment-variables 说明了 CUDA_DEVICE_MAX_CONNECTIONS 默认为 8，可以设置为 1 到 32 的整数值，表示计算和拷贝队列的数量。（同样还有 CUDA_DEVICE_MAX_COPY_CONNECTIONS 表示对拷贝队列的设置（优先级高于 CUDA_DEVICE_MAX_CONNECTION）。）论坛回复 指出这个环境变量表示有多少个硬件队列和 CUDA 流发生映射关系。（如果这个值不够大则流之间存在虚假依赖。这样一来，设置为 1 就能完全保证 kernel 执行顺序和发起顺序一致。设置为 32 则尽可能设法让不同的流并发执行 kernels。）"></span><header class=post-header><h1 class=post-title itemprop="name headline">CUDA_DEVICE_MAX_CONNECTIONS</h1><div class=post-meta-container><div class=post-meta-items><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar"></i>
</span><span class=post-meta-item-text title=发表于>发表于：
</span><time title="创建时间：2025-09-07 00:00:00 +0800 CST" itemprop="dateCreated datePublished" datetime="2025-09-07 00:00:00 +0800 CST">2025-09-07</time></span></div><div class=post-meta-items><span class=post-meta-item title=字数><span class=post-meta-item-icon><i class="far fa-file-word"></i>
</span><span class=post-meta-item-text>字数：</span>
<span>1039</span>
</span><span class=post-meta-item title=阅读><span class=post-meta-item-icon><i class="far fa-clock"></i>
</span><span class=post-meta-item-text>阅读：&ap;</span>
<span>3分钟</span></span></div></div></header><div class=post-body itemprop=articleBody><h1 id=环境变量解释>环境变量解释
<a class=header-anchor href=#%e7%8e%af%e5%a2%83%e5%8f%98%e9%87%8f%e8%a7%a3%e9%87%8a></a></h1><p><code>CUDA_DEVICE_MAX_CONNECTIONS</code> 这个环境变量在 Megatron-LM 源码中经常见到，主要是用来控制硬件调度 kernel 的顺序，以尽可能提高通信计算重叠场景的重叠率。</p><p><a href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cuda-environment-variables title="英伟达文档 cuda-environment-variables" rel="noopener external nofollow noreferrer" target=_blank class=exturl>英伟达文档 cuda-environment-variables<i class="fa fa-external-link-alt"></i></a> 说明了 <code>CUDA_DEVICE_MAX_CONNECTIONS</code> 默认为 8，可以设置为 1 到 32 的整数值，表示计算和拷贝队列的数量。（同样还有 <code>CUDA_DEVICE_MAX_COPY_CONNECTIONS</code> 表示对拷贝队列的设置（优先级高于 <code>CUDA_DEVICE_MAX_CONNECTION</code>）。）<a href=https://forums.developer.nvidia.com/t/cuda-device-max-connections-and-pci-e-traffic/262962/2 title=论坛回复 rel="noopener external nofollow noreferrer" target=_blank class=exturl>论坛回复<i class="fa fa-external-link-alt"></i></a> 指出这个环境变量表示有多少个硬件队列和 CUDA 流发生映射关系。（如果这个值不够大则流之间存在虚假依赖。这样一来，设置为 1 就能完全保证 kernel 执行顺序和发起顺序一致。设置为 32 则尽可能设法让不同的流并发执行 kernels。）</p><h1 id=megatron-lm-1f1b--ep-a2a-overlap>Megatron-LM 1F1B + EP A2A overlap
<a class=header-anchor href=#megatron-lm-1f1b--ep-a2a-overlap></a></h1><p>Megatron-LM 2025.8.1 的提交支持了 1F1B + EP A2A overlap，也对 <code>CUDA_DEVICE_MAX_CONNECTIONS</code> 环境变量有描述。</p><p><a href=https://github.com/NVIDIA/Megatron-LM/blame/d338252b864bf3f24fea0e2f087b73f4f8de7b16/megatron/training/arguments.py#L850 title=代码 rel="noopener external nofollow noreferrer" target=_blank class=exturl>代码<i class="fa fa-external-link-alt"></i></a>：</p><p><img src=/posts/ai-infra/assets/Pasted%20image%2020250907124026.webp></p><p>思考：</p><ol><li>警告信息中暗示 Blackwell 可能更好处理同时开启 TP/CP 和 EP overlap 的情况。这个或许和 Blackwell 的 cluster launch control 功能有关。</li><li>对通信计算重叠使用 <code>CUDA_DEVICE_MAX_CONNECTIONS=32</code>，是说明两个一前一后的 microbatch 中各通信、计算 kernels 并不是精心重叠的、需要依赖硬件的并发调度提高效率吗？</li></ol><div class="markdown-alert markdown-alert-tip"><p class=markdown-alert-title><svg class="octicon octicon-light-bulb mr-2" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="M8 1.5c-2.363.0-4 1.69-4 3.75.0.984.424 1.625.984 2.304l.214.253c.223.264.47.556.673.848.284.411.537.896.621 1.49a.75.75.0 01-1.484.211c-.04-.282-.163-.547-.37-.847a8.456 8.456.0 00-.542-.68c-.084-.1-.173-.205-.268-.32C3.201 7.75 2.5 6.766 2.5 5.25 2.5 2.31 4.863.0 8 0s5.5 2.31 5.5 5.25c0 1.516-.701 2.5-1.328 3.259-.095.115-.184.22-.268.319-.207.245-.383.453-.541.681-.208.3-.33.565-.37.847a.751.751.0 01-1.485-.212c.084-.593.337-1.078.621-1.489.203-.292.45-.584.673-.848.075-.088.147-.173.213-.253.561-.679.985-1.32.985-2.304.0-2.06-1.637-3.75-4-3.75zM5.75 12h4.5a.75.75.0 010 1.5h-4.5a.75.75.0 010-1.5zM6 15.25a.75.75.0 01.75-.75h2.5a.75.75.0 010 1.5h-2.5A.75.75.0 016 15.25z"/></svg>Tip</p><p>1F1B + EP A2A overlap 的发展：</p><ol><li>2025.3 小红书和英伟达中国发 <a href=https://developer.nvidia.com/zh-cn/blog/1f1b-moe-a2a-computing-overlap/ title=博客 rel="noopener external nofollow noreferrer" target=_blank class=exturl>博客<i class="fa fa-external-link-alt"></i></a> 介绍了这个方案。</li><li>2025.6 <a href=https://arxiv.org/abs/2506.05767 title=dots.llm1 rel="noopener external nofollow noreferrer" target=_blank class=exturl>dots.llm1<i class="fa fa-external-link-alt"></i></a> 使用了这一方案。</li><li>2025.7 <a href=https://arxiv.org/abs/2507.20534 title="Kimi K2" rel="noopener external nofollow noreferrer" target=_blank class=exturl>Kimi K2<i class="fa fa-external-link-alt"></i></a> 使用了这一方案。</li><li>2025.8 Megatron-LM 将这个方案 <a href=https://github.com/NVIDIA/Megatron-LM/commit/ae1c88296f465ab4ac9c503d75a57ba4044c47d1 title=提交 rel="noopener external nofollow noreferrer" target=_blank class=exturl>提交<i class="fa fa-external-link-alt"></i></a> 到主分支。</li></ol><p>也可以看出来 Megatron-LM 在流水线调度问题上始终不愿放弃 1F1B，就像 CP 方案不愿接入 Ulysses SP 一样。可能是代码上有较多对 1F1B 的依赖，比如 TP 反向的 bulk overlap 等。</p></div><h1 id=知乎---ring-attention-性能问题引发的计算通信-overlap-分析>知乎 - Ring Attention 性能问题引发的计算通信 overlap 分析
<a class=header-anchor href=#%e7%9f%a5%e4%b9%8e---ring-attention-%e6%80%a7%e8%83%bd%e9%97%ae%e9%a2%98%e5%bc%95%e5%8f%91%e7%9a%84%e8%ae%a1%e7%ae%97%e9%80%9a%e4%bf%a1-overlap-%e5%88%86%e6%9e%90></a></h1><p><a href=https://zhuanlan.zhihu.com/p/706805407 title=https://zhuanlan.zhihu.com/p/706805407 rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://zhuanlan.zhihu.com/p/706805407<i class="fa fa-external-link-alt"></i></a></p><p>这也是一篇很有质量的分析通信计算重叠的文章。文章给出了一个默认状态下的通信计算例子（使用 <code>CUDA_DEVICE_MAX_CONNECTIONS</code> 默认值 8）：</p><p><img src=/posts/ai-infra/assets/Pasted%20image%2020250907125036.webp></p><p>可以看到只要通信 kernel 比计算 kernel 晚发起，两者就不能很好重叠，尽管它们之间没有依赖关系。</p><p>作者还尝试了不用 <code>CUDA_DEVICE_MAX_CONNECTIONS</code> 环境变量，而是手动在两个流之间添加同步，使得计算比通信晚发起，结果 overlap 效果也变好了，达到了相似的效果。</p><p><img src=/posts/ai-infra/assets/Pasted%20image%2020250907125112.webp width=600>（图片来自文章）</p><p>为什么无手动同步时，也经常观察到通信 kernel 等计算 kernel 呢？我认为：</p><ol><li><strong>如果发起的 torch 集合通信操作使用到了某个输入张量，需要同步该张量所在的流。</strong><a href=https://deepwiki.com/search/torch-allgather-torchcsrcdistr_f50f0d53-507a-4363-91c6-146d978762c8 title="DeepWiki 链接" rel="noopener external nofollow noreferrer" target=_blank class=exturl>DeepWiki 链接<i class="fa fa-external-link-alt"></i></a> 指出了代码中的显式同步操作。</li><li><strong>计算流 kernel 先发起，占满了资源，等到计算 kernel 要结束的时候资源才腾出来</strong>。分析：光是 1 还不能解释上面 Perfetto 界面显示 GEMM kernel 快要结束、但还没结束时，通信 kernel 就已经开始执行的情况。我相信代码里面肯定是先发起了通信操作，再发起计算操作的，因此通信操作的事件同步点在计算操作之前，不会存在阻塞问题。但由于没有设置 <code>CUDA_DEVICE_MAX_CONNECTIONS=1</code>，先发起的通信 kernel 没有先调度，导致计算 kernel 占满了 SMs，只有快结束的时候通信 kernel 才得以运行。</li></ol></div><footer class=post-footer><div class=post-tags><a href=/tags/ai-infra>ai-infra</a></div><div class=addthis_inline_share_toolbox style=text-align:center></div><hr><div class=post-nav><div class="post-nav-next post-nav-item"></div><div class="post-nav-prev post-nav-item"><a href=/posts/ai-infra/Megatron-LM-%E8%AE%BA%E6%96%87-PP-%E5%92%8C-VPP-%E7%94%BB%E6%B3%95%E7%96%91%E9%97%AE/ rel=prev title="Megatron-LM 论文 PP 和 VPP 画法疑问">Megatron-LM 论文 PP 和 VPP 画法疑问
<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>&copy;
<span itemprop=copyrightYear>2023 - 2025
</span><span class=with-love><i class="fa fa-heart"></i>
</span><span class=author itemprop=copyrightHolder>🤖</span></div><div class=powered-by>由 <a href=https://gohugo.io title=0.143.1 target=_blank>Hugo</a> & <a href=https://github.com/hugo-next/hugo-theme-next title=4.5.3 target=_blank>Hugo NexT.Gemini</a> 强力驱动</div></div></footer><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js defer></script><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.1.0/medium-zoom.min.js defer></script><script class=next-config data-name=main type=application/json>{"bookmark":{"color":"#222","enable":false,"save":"manual"},"copybtn":true,"darkmode":false,"hostname":"https://hxhue.github.io/","i18n":{"ds_day":" 天前","ds_days":" 天 ","ds_hour":" 小时前","ds_hours":" 小时 ","ds_just":"刚刚","ds_min":" 分钟前","ds_mins":" 分钟","ds_month":" 个月前","ds_years":" 年 ","empty":"没有找到任何搜索结果：${query}","hits":"找到 ${hits} 个搜索结果","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","placeholder":"搜索..."},"lang":"zh-CN","lazyload":false,"localSearch":{"enable":true,"limit":1e3,"path":"/searchindexes.xml","preload":false,"topnperarticle":-1,"trigger":"auto","unescape":false},"motion":{"async":true,"enable":false,"transition":{"collheader":"fadeInLeft","menu_item":"fadeInDown","postblock":"fadeIn","postbody":"fadeInDown","postheader":"fadeInDown","sidebar":"fadeInUp"}},"postmeta":{"comments":{"enable":false,"plugin":"waline"},"views":{"enable":false,"plugin":"busuanzi"}},"root":"/","scheme":"Gemini","sidebar":{"display":"post","offset":12,"padding":18,"position":"left","width":256},"vendor":{"plugins":"cdnjs","router":"https://cdnjs.cloudflare.com/ajax/libs"},"version":"4.5.3"}</script><script type=text/javascript src=/js/main.min.37ba8b54f9d4d784d08028c45eea93b5d4e13eda8ee7fb0d2edd6f3fac66cfd2.js defer></script></body></html>