<!doctype html><html lang=zh-CN data-theme=light><head><meta charset=UTF-8><meta name=viewport content="width=device-width"><meta name=theme-color content="#222" media="(prefers-color-scheme: light)"><meta name=generator content="Hugo 0.143.1"><link rel="shortcut icon" type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=/imgs/icons/favicon_16x16_next.png><link rel=icon type=image/png sizes=32x32 href=/imgs/icons/favicon_32_32_next.png><link rel=apple-touch-icon sizes=180x180 href=/imgs/icons/apple_touch_icon_next.png><meta itemprop=name content="PyTorch C++ 代码生成"><meta itemprop=description content="个人博客，主要是零散的笔记。"><meta itemprop=datePublished zgotmplz><meta itemprop=dateModified zgotmplz><meta itemprop=image content="https://hxhue.github.io/imgs/371907.jpg"><meta itemprop=keywords content="torch"><link type=text/css rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.2/css/all.min.css><link type=text/css rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css><link rel=stylesheet href=/css/main.min.bea76f574a755574e17d42bea39502a74ca3ca4db65807b8c82d3e26dcec8420.css><style type=text/css>.post-footer,.flinks-list-footer hr:after{content:"~ 我可是有底线的哟 ~"}</style><link rel=stylesheet type=text/css href=/css/style.css><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/github-markdown-css@5.3.0/github-markdown-dark.css><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js></script><script>MathJax={tex:{displayMath:[["$$","$$"],["\\[","\\]"]],inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"}}</script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.esm.min.mjs';
  mermaid.initialize({ startOnLoad: false });
  mermaid.mermaidAPI.initialize();
  window.mermaid = mermaid;
</script><script type=text/javascript>(function(){localDB={set:function(e,t,n){if(n===0)return;const s=new Date,o=n*864e5,i={value:t,expiry:s.getTime()+o};localStorage.setItem(e,JSON.stringify(i))},get:function(e){const t=localStorage.getItem(e);if(!t)return void 0;const n=JSON.parse(t),s=new Date;return s.getTime()>n.expiry?(localStorage.removeItem(e),void 0):n.value}},theme={active:function(){const e=localDB.get("theme");if(e==null)return;theme.toggle(e),window.matchMedia("(prefers-color-scheme: dark)").addListener(function(e){theme.toggle(e.matches?"dark":"light")})},toggle:function(e){document.documentElement.setAttribute("data-theme",e),localDB.set("theme",e,2);const t=document.querySelector("iframe.giscus-frame");if(t){const n={setConfig:{theme:e}};t.contentWindow.postMessage({giscus:n},"https://giscus.app")}}},theme.active()})(window)</script><script class=next-config data-name=page type=application/json>{"comments":false,"isHome":false,"isPage":true,"path":"2.-PyTorch-C++-%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90","permalink":"https://hxhue.github.io/posts/programming/python/2.-PyTorch-C++-%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90/","title":"PyTorch C++ 代码生成","waline":{"js":[{"alias":"waline","alias_name":"@waline/client","file":"dist/pageview.js","name":"pageview","version":"2.13.0"},{"alias":"waline","alias_name":"@waline/client","file":"dist/comment.js","name":"comment","version":"2.13.0"}]}}</script><script type=text/javascript>document.addEventListener("DOMContentLoaded",()=>{var e=document.createElement("script");e.charset="UTF-8",e.src="",e.async=!1,e.defer=!0,document.head.appendChild(e),e.onload=function(){NexT.utils.fmtBusuanzi()}})</script><title>PyTorch C++ 代码生成 - Bluegill</title><noscript><link rel=stylesheet href=/css/noscript.css></noscript></head><body itemscope itemtype=http://schema.org/WebPage><div class=headband></div><main class=main><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div class=toggle aria-label role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div></div><div class=site-meta><a href=/ class=brand rel=start><i class=logo-line></i><h1 class=site-title>Bluegill</h1><i class=logo-line></i></a><p class=site-subtitle itemprop=description></p></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href=/ class=hvr-icon-pulse rel=section><i class="fa fa-home hvr-icon"></i>首页</a></li><li class="menu-item menu-item-about"><a href=/about/ class=hvr-icon-pulse rel=section><i class="fa fa-user hvr-icon"></i>关于</a></li><li class="menu-item menu-item-archives"><a href=/archives/ class=hvr-icon-pulse rel=section><i class="fa fa-archive hvr-icon"></i>归档</a></li><li class="menu-item menu-item-categories"><a href=/categories/ class=hvr-icon-pulse rel=section><i class="fa fa-th hvr-icon"></i>分类</a></li><li class="menu-item menu-item-tags"><a href=/tags/ class=hvr-icon-pulse rel=section><i class="fa fa-hashtag hvr-icon"></i>标签</a></li><li class="menu-item menu-item-daily"><a href=/daily/ class=hvr-icon-pulse rel=section><i class="fa fa-newspaper hvr-icon"></i>随笔</a></li><li class="menu-item menu-item-discovery"><a href=https://rift-fear-f2c.notion.site/2025-1e354a33cfb1802c841bdf29f2f3dab3 class=hvr-icon-pulse rel=section><i class="fa fa-compass hvr-icon"></i>发现</a></li><li class="menu-item menu-item-search"><a role=button class="popup-trigger hvr-icon-pulse"><i class="fa fa-search fa-fw hvr-icon"></i>搜索</a></li></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon><i class="fa fa-search"></i></span><div class=search-input-container><input autocomplete=off autocapitalize=off maxlength=80 placeholder=搜索... spellcheck=false type=search class=search-input></div><span class=popup-btn-close role=button><i class="fa fa-times-circle"></i></span></div><div class="search-result-container no-result"><div class=search-result-icon><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></div><div class="toggle sidebar-toggle" role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div><aside class=sidebar><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class=sidebar-nav><li class=sidebar-nav-toc>文章目录</li><li class=sidebar-nav-overview>站点概览</li></ul><div class=sidebar-panel-container><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><nav id=TableOfContents><ul><li><a href=#一些问题>一些问题</a></li><li><a href=#提要>提要</a></li><li><a href=#生成代码>生成代码</a></li><li><a href=#非-structured-算子以-index_put_-为例>非 <code>structured</code> 算子：以 <code>index_put_</code> 为例</a><ul><li><a href=#追踪-index_put_-的派发>追踪 <code>index_put_</code> 的派发</a></li><li><a href=#追踪-index_putout-的-autogen>追踪 <code>index_put.out</code> 的 autogen</a></li></ul></li><li><a href=#structured-算子以-index_out-为例><code>structured</code> 算子：以 <code>index_out</code> 为例</a><ul><li><a href=#生成的代码>生成的代码</a></li><li><a href=#impl><code>impl</code></a></li><li><a href=#meta><code>meta</code></a></li><li><a href=#放在一起看>放在一起看</a></li><li><a href=#再来看-mimplat-名字空间而非-atcuda>再来看 <code>m.impl</code>（<code>at</code> 名字空间，而非 <code>at::cuda</code>）</a></li></ul></li></ul></nav></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author site-overview-item animated" itemprop=author itemscope itemtype=http://schema.org/Person><img class=site-author-image itemprop=image alt=🤖 src=/imgs/371907.jpg><p class=site-author-name itemprop=name>🤖</p><div class=site-description itemprop=description>个人博客，主要是零散的笔记。</div></div><div class="site-state-wrap site-overview-item animated"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/><span class=site-state-item-count>433</span>
<span class=site-state-item-name>日志</span></a></div><div class="site-state-item site-state-categories"><a href=/categories/><span class=site-state-item-count>12</span>
<span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/><span class=site-state-item-count>86</span>
<span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-social site-overview-item animated"><span class=links-of-social-item><a href=https://github.com/hxhue title="Github → https://github.com/hxhue" rel=noopener target=_blank><i class="fab fa-github fa-fw"></i>
Github
</a></span><span class=links-of-social-item><a href=/rss.xml title="RSS 订阅 → /rss.xml" rel=noopener target=_blank><i class="fa fa-rss fa-fw"></i>
RSS 订阅</a></span></div><div class="cc-license animated" itemprop=license><a href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh class=cc-opacity rel=noopener target=_blank title=共享知识><img src=/imgs/cc/big/by_nc_sa.svg alt=共享知识></a></div><div class="links-of-blogroll site-overview-item animated"><div class=links-of-blogroll-title><i class="fa fa-globe fa-fw"></i>
友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=https://shuai.guru/ title=https://shuai.guru/ target=_blank>shuai.guru</a></li></ul></div></div></div></div></aside><div class=sidebar-dimmer></div></header><div class=tool-buttons><div id=toggle-theme class=button title=深浅模式切换><i class="fas fa-adjust"></i></div><div class=back-to-top role=button title=返回顶部><i class="fa fa-arrow-up"></i>
<span>0%</span></div></div><div class=reading-progress-bar></div><noscript><div class=noscript-warning>Theme NexT works best with JavaScript enabled</div></noscript><div class="main-inner post posts-expand"><div class=post-block><article itemscope itemtype=http://schema.org/Article class=post-content lang><link itemprop=mainEntityOfPage href=https://hxhue.github.io/posts/programming/python/2.-PyTorch-C++-%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90/><span hidden itemprop=author itemscope itemtype=http://schema.org/Person><meta itemprop=image content="/imgs/371907.jpg"><meta itemprop=name content="🤖"></span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization><meta itemprop=name content="🤖"><meta itemprop=description content="个人博客，主要是零散的笔记。"></span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork><meta itemprop=name content="PyTorch C++ 代码生成"><meta itemprop=description content='一些问题

为什么有些会生成 at::cuda 名字空间的函数，有些不会？（待解决）
提要

本文说明了 m.impl("index_put.out", ...) 到 at::native::index_put 的调用路径。结合 
  
  
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    PyTorch C++ 函数派发 中 at::native::index_put_ → at::_index_put_impl_ → index_put_stub 的调用路径，补全了从 m.impl 到 stub 的全路径。'></span><header class=post-header><h1 class=post-title itemprop="name headline">PyTorch C++ 代码生成</h1><div class=post-meta-container><div class=post-meta-items><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar"></i>
</span><span class=post-meta-item-text title=发表于>发表于：
</span><time title="创建时间：2025-06-30 00:00:00 +0800 CST" itemprop="dateCreated datePublished" datetime="2025-06-30 00:00:00 +0800 CST">2025-06-30
</time></span><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar-check"></i>
</span><span class=post-meta-item-text title=更新于>更新于：
</span><time title=修改时间：2025-10-02T00:00:00+08:00 itemprop=dateModified datetime=2025-10-02T00:00:00+08:00>2025-10-02</time></span></div><div class=post-meta-items><span class=post-meta-item title=字数><span class=post-meta-item-icon><i class="far fa-file-word"></i>
</span><span class=post-meta-item-text>字数：</span>
<span>3145</span>
</span><span class=post-meta-item title=阅读><span class=post-meta-item-icon><i class="far fa-clock"></i>
</span><span class=post-meta-item-text>阅读：&ap;</span>
<span>7分钟</span></span></div></div></header><div class=post-body itemprop=articleBody><h1 id=一些问题>一些问题
<a class=header-anchor href=#%e4%b8%80%e4%ba%9b%e9%97%ae%e9%a2%98></a></h1><p>为什么有些会生成 at::cuda 名字空间的函数，有些不会？（待解决）</p><h1 id=提要>提要
<a class=header-anchor href=#%e6%8f%90%e8%a6%81></a></h1><p><strong>本文说明了 <code>m.impl("index_put.out", ...)</code> 到 <code>at::native::index_put</code> 的调用路径</strong>。结合
<a href=/posts/programming/python/1.-PyTorch-C++-%E5%87%BD%E6%95%B0%E6%B4%BE%E5%8F%91/ title="PyTorch C++ 函数派发">PyTorch C++ 函数派发</a> 中 <code>at::native::index_put_</code> → <code>at::_index_put_impl_</code> → <code>index_put_stub</code> 的调用路径，补全了从 m.impl 到 stub 的全路径。</p><p><strong>本文说明了 <code>m.impl</code> / <code>at::cuda::index_out</code> → <code>index_stub</code> 的调用路径</strong>。准确来说是介绍了 <code>at::cuda::index_out</code> 调用 meta 和 impl 的过程，meta 中对下标做预处理（包括 kBool 转 kLong 下标），impl 中调用 <code>index_stub</code> 进行计算。和 <code>index_out</code> 不同，<code>index_put_</code> 函数没有出现在 <code>at::cuda</code> 名字空间中，取而代之的是 <code>at::cuda::_index_put_impl_</code>。</p><p>在这两个例子中，能找到的函数有：</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span>理解 native_functions.yaml 中的函数定义在哪里
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>非 structured 情况：
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#719e07>-</span> func: index_put_(...)
</span></span><span style=display:flex><span>  dispatch:
</span></span><span style=display:flex><span>	  CompositeExplicitAutograd: index_put_ # 默认名字空间是 aten
</span></span><span style=display:flex><span>			\ at<span style=color:#719e07>::</span>index_put_ (build<span style=color:#719e07>/</span>aten<span style=color:#719e07>/</span>src<span style=color:#719e07>/</span>ATen<span style=color:#719e07>/</span>Functions.h)
</span></span><span style=display:flex><span>			 \ at<span style=color:#719e07>::</span>_ops<span style=color:#719e07>::</span>index_put_<span style=color:#719e07>::</span>call (build<span style=color:#719e07>/</span>aten<span style=color:#719e07>/</span>src<span style=color:#719e07>/</span>ATen<span style=color:#719e07>/</span>OperatorsEverything.cpp)
</span></span><span style=display:flex><span>			  \ c10<span style=color:#719e07>::</span>Dispatcher<span style=color:#719e07>::</span>singleton()
</span></span><span style=display:flex><span>			   <span style=color:#719e07>|</span>  .findSchemaOrThrow(index_put_<span style=color:#719e07>::</span>name, index_put_<span style=color:#719e07>::</span>overload_name)
</span></span><span style=display:flex><span>			   <span style=color:#719e07>|</span>  .typed<span style=color:#719e07>&lt;</span>index_put_<span style=color:#719e07>::</span>schema<span style=color:#719e07>&gt;</span>().call
</span></span><span style=display:flex><span>				 <span style=color:#719e07>|</span><span style=color:#719e07>struct</span> <span style=color:#268bd2>TORCH_API</span> index_put_ (build<span style=color:#719e07>/</span>aten<span style=color:#719e07>/</span>src<span style=color:#719e07>/</span>ATen<span style=color:#719e07>/</span>MethodOperators.h)
</span></span><span style=display:flex><span>				 <span style=color:#719e07>|</span>  ... name <span style=color:#719e07>=</span> <span style=color:#2aa198>&#34;aten::index_put_&#34;</span>;
</span></span><span style=display:flex><span>				 <span style=color:#719e07>|</span>  ... overload_name <span style=color:#719e07>=</span> <span style=color:#2aa198>&#34;&#34;</span>;
</span></span><span style=display:flex><span>				 \ m.impl(<span style=color:#2aa198>&#34;index_put_&#34;</span>, ...) (build<span style=color:#719e07>/</span>aten<span style=color:#719e07>/</span>src<span style=color:#719e07>/</span>ATen<span style=color:#719e07>/</span>RegisterCompositeExplicitAutogradEverything.cpp)
</span></span><span style=display:flex><span>				  \ at<span style=color:#719e07>::</span>native<span style=color:#719e07>::</span>index_put_ (aten<span style=color:#719e07>/</span>src<span style=color:#719e07>/</span>ATen<span style=color:#719e07>/</span>native<span style=color:#719e07>/</span>TensorAdvancedIndexing.cpp)
</span></span><span style=display:flex><span>				   \ at<span style=color:#719e07>::</span>_index_put_impl_
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>结论是按照 dispatch: yy: xx 字段生成 at<span style=color:#719e07>::</span>xx 函数，最终调用到 at<span style=color:#719e07>::</span>native<span style=color:#719e07>::</span>xx 函数。
</span></span><span style=display:flex><span>                                                       <span style=color:#719e07>^^^^^^^^^^^^^^</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>如果是 structured: True，就对应 at<span style=color:#719e07>::</span>native<span style=color:#719e07>::</span>structured_xx<span style=color:#719e07>::</span>impl，在代码中通常为
</span></span><span style=display:flex><span>at<span style=color:#719e07>::</span>native 名字空间下的 TORCH_IMPL_FUNC(xx)。
</span></span><span style=display:flex><span>                      <span style=color:#719e07>^^^^^^^^^^^^^^^^^^^</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>如果是 structured_delegate: zz，可能得去找 zz 的定义。
</span></span></code></pre></div><p>至于为什么有时候会生成 at::cuda 下的函数，有时候不会，这个不清楚。</p><h1 id=生成代码>生成代码
<a class=header-anchor href=#%e7%94%9f%e6%88%90%e4%bb%a3%e7%a0%81></a></h1><p>可以用 <code>python3 -m torchgen.gen</code> 来生成一部分代码，这个过程比较快，也不用准备好全套的构建环境。接下来以 <code>index_put_</code> 为例来看生成结果。</p><h1 id=非-structured-算子以-index_put_-为例>非 <code>structured</code> 算子：以 <code>index_put_</code> 为例
<a class=header-anchor href=#%e9%9d%9e-structured-%e7%ae%97%e5%ad%90%e4%bb%a5-index_put_-%e4%b8%ba%e4%be%8b></a></h1><h2 id=追踪-index_put_-的派发>追踪 <code>index_put_</code> 的派发
<a class=header-anchor href=#%e8%bf%bd%e8%b8%aa-index_put_-%e7%9a%84%e6%b4%be%e5%8f%91></a></h2><p>在 native_functions.yaml 中：</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#719e07>-</span> func: index_put_(Tensor(a<span style=color:#719e07>!</span>) self, Tensor<span style=color:#719e07>?</span>[] indices, Tensor values, <span style=color:#dc322f>bool</span> accumulate<span style=color:#719e07>=</span>False) <span style=color:#719e07>-&gt;</span> Tensor(a<span style=color:#719e07>!</span>)
</span></span><span style=display:flex><span>  device_check: NoCheck   # delegate to _index_put_impl_, which leverages TensorIterator
</span></span><span style=display:flex><span>  variants: function, method
</span></span><span style=display:flex><span>  dispatch:
</span></span><span style=display:flex><span>    CompositeExplicitAutograd: index_put_
</span></span><span style=display:flex><span>  autogen: index_put.out
</span></span><span style=display:flex><span>  <span style=color:#719e07># NB: The following functions are declared in aten/src/ATen/templates/TensorBody.h and defined in aten/src/ATen/TensorIndexing.cpp:
</span></span></span><span style=display:flex><span><span style=color:#719e07></span>  <span style=color:#719e07># - Tensor &amp; Tensor::index_put_(ArrayRef&lt;TensorIndex&gt; indices, Tensor const &amp; rhs)
</span></span></span><span style=display:flex><span><span style=color:#719e07></span>  <span style=color:#719e07># - Tensor &amp; Tensor::index_put_(ArrayRef&lt;TensorIndex&gt; indices, Scalar v)
</span></span></span><span style=display:flex><span><span style=color:#719e07></span>  <span style=color:#719e07># - Tensor &amp; Tensor::index_put_(std::initializer_list&lt;TensorIndex&gt; indices, Tensor const &amp; rhs)
</span></span></span><span style=display:flex><span><span style=color:#719e07></span>  <span style=color:#719e07># - Tensor &amp; Tensor::index_put_(std::initializer_list&lt;TensorIndex&gt; indices, Scalar v)
</span></span></span></code></pre></div><p>首先是会有个 <code>TORCH_LIBRARY_IMPL</code>，然后里面会有 <code>m.impl("名称")</code>，可以按照 <code>m.impl</code> 来搜索。</p><p>在 build/aten/src/ATen/RegisterCompositeExplicitAutogradEverything.cpp 中（在 torch/library.h 中有 <code>torch::Library</code> 类型，该类型有下面看到的 <code>impl</code> 方法，还有其他地方常看到的 <code>def</code> 方法）：</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#719e07>namespace</span> {
</span></span><span style=display:flex><span>at<span style=color:#719e07>::</span>Tensor wrapper_CompositeExplicitAutograd__index_put(<span style=color:#719e07>const</span> at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> self, <span style=color:#719e07>const</span> c10<span style=color:#719e07>::</span>List<span style=color:#719e07>&lt;::</span>std<span style=color:#719e07>::</span>optional<span style=color:#719e07>&lt;</span>at<span style=color:#719e07>::</span>Tensor<span style=color:#719e07>&gt;&gt;</span> <span style=color:#719e07>&amp;</span> indices, <span style=color:#719e07>const</span> at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> values, <span style=color:#dc322f>bool</span> accumulate) {
</span></span><span style=display:flex><span>    <span style=color:#586e75>// No device check
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>  <span style=color:#586e75>// DeviceGuard omitted
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>  <span style=color:#719e07>return</span> at<span style=color:#719e07>::</span>native<span style=color:#719e07>::</span>index_put(self, indices, values, accumulate);
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>} <span style=color:#586e75>// anonymous namespace
</span></span></span><span style=display:flex><span><span style=color:#586e75></span><span style=color:#719e07>namespace</span> {
</span></span><span style=display:flex><span>at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> wrapper_CompositeExplicitAutograd_out_index_put_out(<span style=color:#719e07>const</span> at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> self, <span style=color:#719e07>const</span> c10<span style=color:#719e07>::</span>List<span style=color:#719e07>&lt;::</span>std<span style=color:#719e07>::</span>optional<span style=color:#719e07>&lt;</span>at<span style=color:#719e07>::</span>Tensor<span style=color:#719e07>&gt;&gt;</span> <span style=color:#719e07>&amp;</span> indices, <span style=color:#719e07>const</span> at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> values, <span style=color:#dc322f>bool</span> accumulate, at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> out) {
</span></span><span style=display:flex><span>    <span style=color:#586e75>// No device check
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>  <span style=color:#586e75>// DeviceGuard omitted
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>  <span style=color:#719e07>return</span> at<span style=color:#719e07>::</span>native<span style=color:#719e07>::</span>index_put_out(self, indices, values, accumulate, out);
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>} <span style=color:#586e75>// anonymous namespace
</span></span></span><span style=display:flex><span><span style=color:#586e75></span><span style=color:#719e07>namespace</span> {
</span></span><span style=display:flex><span>at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> wrapper_CompositeExplicitAutograd__index_put_(at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> self, <span style=color:#719e07>const</span> c10<span style=color:#719e07>::</span>List<span style=color:#719e07>&lt;::</span>std<span style=color:#719e07>::</span>optional<span style=color:#719e07>&lt;</span>at<span style=color:#719e07>::</span>Tensor<span style=color:#719e07>&gt;&gt;</span> <span style=color:#719e07>&amp;</span> indices, <span style=color:#719e07>const</span> at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> values, <span style=color:#dc322f>bool</span> accumulate) {
</span></span><span style=display:flex><span>    <span style=color:#586e75>// No device check
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>  <span style=color:#586e75>// DeviceGuard omitted
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>  <span style=color:#719e07>return</span> at<span style=color:#719e07>::</span>native<span style=color:#719e07>::</span>index_put_(self, indices, values, accumulate);
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>} <span style=color:#586e75>// anonymous namespace
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>TORCH_LIBRARY_IMPL(aten, CompositeExplicitAutograd, m) {
</span></span><span style=display:flex><span>    m.impl(<span style=color:#2aa198>&#34;index_put&#34;</span>,
</span></span><span style=display:flex><span>TORCH_FN(wrapper_CompositeExplicitAutograd__index_put));
</span></span><span style=display:flex><span>m.impl(<span style=color:#2aa198>&#34;index_put.out&#34;</span>,
</span></span><span style=display:flex><span>TORCH_FN(wrapper_CompositeExplicitAutograd_out_index_put_out));
</span></span><span style=display:flex><span>m.impl(<span style=color:#2aa198>&#34;index_put_&#34;</span>,
</span></span><span style=display:flex><span>TORCH_FN(wrapper_CompositeExplicitAutograd__index_put_));
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>从这里我们发现一个规律：很多在 native_functions.yaml 中注册的函数会转而调用 <code>at::native::</code> 中的函数，这些函数是在 aten/src/ATen/native/ 文件夹下有定义的，比如 <code>at::native::index_put</code> 和 <code>at::native::index_put_</code> 。也有一些函数比如 <code>at::native::index_put_out</code> 是用代码生成的，如 native_functions.yaml 中 <code>index_put_</code> 函数的 autogen 字段所述。</p><p>我们先记住 <code>m.impl</code> 能记录函数派发，暂时不展开。</p><h2 id=追踪-index_putout-的-autogen>追踪 <code>index_put.out</code> 的 autogen
<a class=header-anchor href=#%e8%bf%bd%e8%b8%aa-index_putout-%e7%9a%84-autogen></a></h2><p>前面我们已经看到了 <code>index_put</code> 对应 <code>at::native::index_put</code>，<code>index_put_</code> 对应 <code>at::native::index_put_</code>，而且都能在 aten/src/ATen/native/TensorAdvancedIndexing.cpp 中找到定义，但是 <code>at::native::index_put_out</code> 是找不到定义的。<strong>这一个小节用来阅读 <code>autogen</code> 字段请求生成的代码。</strong></p><p>代码生成后，在 build/aten/src/ATen/CompositeViewCopyKernels.cpp 中（和 <code>autogen: index_put.out</code> 对应）：</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#586e75>// namespace at::native
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>
</span></span><span style=display:flex><span>at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> index_put_out(<span style=color:#719e07>const</span> at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> self, <span style=color:#719e07>const</span> c10<span style=color:#719e07>::</span>List<span style=color:#719e07>&lt;::</span>std<span style=color:#719e07>::</span>optional<span style=color:#719e07>&lt;</span>at<span style=color:#719e07>::</span>Tensor<span style=color:#719e07>&gt;&gt;</span> <span style=color:#719e07>&amp;</span> indices, <span style=color:#719e07>const</span> at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> values, <span style=color:#dc322f>bool</span> accumulate, at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> out) {
</span></span><span style=display:flex><span>  <span style=color:#719e07>auto</span> tmp_output <span style=color:#719e07>=</span> at<span style=color:#719e07>::</span>_ops<span style=color:#719e07>::</span>index_put<span style=color:#719e07>::</span>call(self, indices, values, accumulate);
</span></span><span style=display:flex><span>  resize_out_helper(out, tmp_output);
</span></span><span style=display:flex><span>  copy_arg(out, tmp_output);
</span></span><span style=display:flex><span>  <span style=color:#719e07>return</span> out;
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>在 build/aten/src/ATen/OperatorsEverything.cpp 中：</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#586e75>// namespace at::_ops
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>
</span></span><span style=display:flex><span><span style=color:#586e75>// aten::index_put(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False) -&gt; Tensor
</span></span></span><span style=display:flex><span><span style=color:#586e75></span><span style=color:#719e07>static</span> C10_NOINLINE c10<span style=color:#719e07>::</span>TypedOperatorHandle<span style=color:#719e07>&lt;</span>index_put<span style=color:#719e07>::</span>schema<span style=color:#719e07>&gt;</span> create_index_put_typed_handle() {
</span></span><span style=display:flex><span>  <span style=color:#719e07>return</span> c10<span style=color:#719e07>::</span>Dispatcher<span style=color:#719e07>::</span>singleton()
</span></span><span style=display:flex><span>      .findSchemaOrThrow(index_put<span style=color:#719e07>::</span>name, index_put<span style=color:#719e07>::</span>overload_name)
</span></span><span style=display:flex><span>      .typed<span style=color:#719e07>&lt;</span>index_put<span style=color:#719e07>::</span>schema<span style=color:#719e07>&gt;</span>();
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#586e75>// ✅
</span></span></span><span style=display:flex><span><span style=color:#586e75>// aten::index_put(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False) -&gt; Tensor
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>at<span style=color:#719e07>::</span>Tensor index_put<span style=color:#719e07>::</span>call(<span style=color:#719e07>const</span> at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> self, <span style=color:#719e07>const</span> c10<span style=color:#719e07>::</span>List<span style=color:#719e07>&lt;::</span>std<span style=color:#719e07>::</span>optional<span style=color:#719e07>&lt;</span>at<span style=color:#719e07>::</span>Tensor<span style=color:#719e07>&gt;&gt;</span> <span style=color:#719e07>&amp;</span> indices, <span style=color:#719e07>const</span> at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> values, <span style=color:#dc322f>bool</span> accumulate) {
</span></span><span style=display:flex><span>    <span style=color:#719e07>static</span> <span style=color:#719e07>auto</span> op <span style=color:#719e07>=</span> create_index_put_typed_handle();
</span></span><span style=display:flex><span>    <span style=color:#719e07>return</span> op.call(self, indices, values, accumulate);
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#586e75>// aten::index_put(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False) -&gt; Tensor
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>at<span style=color:#719e07>::</span>Tensor index_put<span style=color:#719e07>::</span>redispatch(c10<span style=color:#719e07>::</span>DispatchKeySet dispatchKeySet, <span style=color:#719e07>const</span> at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> self, <span style=color:#719e07>const</span> c10<span style=color:#719e07>::</span>List<span style=color:#719e07>&lt;::</span>std<span style=color:#719e07>::</span>optional<span style=color:#719e07>&lt;</span>at<span style=color:#719e07>::</span>Tensor<span style=color:#719e07>&gt;&gt;</span> <span style=color:#719e07>&amp;</span> indices, <span style=color:#719e07>const</span> at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> values, <span style=color:#dc322f>bool</span> accumulate) {
</span></span><span style=display:flex><span>    <span style=color:#719e07>static</span> <span style=color:#719e07>auto</span> op <span style=color:#719e07>=</span> create_index_put_typed_handle();
</span></span><span style=display:flex><span>    <span style=color:#719e07>return</span> op.redispatch(dispatchKeySet, self, indices, values, accumulate);
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>这个 <code>findSchemaOrThrow</code> 函数在 aten/src/ATen/core/dispatch/Dispatcher.cpp 中定义。aten/src/ATen/core/dispatch/Dispatcher.h 则有对函数接口较为详细的说明。具体来说会从自己的 table 来查找字符串名，看看有没有结果，具体来说是 <code>operatorLookupTable_.read</code>。</p><p>这里还用到了 <code>index_put</code>，可以搜索 <code>struct TORCH_API index_put</code>：</p><p><img alt=image.png src=/posts/programming/python/assets/struct%20TORCH_API%20index_put.webp width=400></p><p>定义为：</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#719e07>struct</span> <span style=color:#268bd2>TORCH_API</span> index_put {
</span></span><span style=display:flex><span>  <span style=color:#719e07>using</span> schema <span style=color:#719e07>=</span> at<span style=color:#719e07>::</span>Tensor (<span style=color:#719e07>const</span> at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span>, <span style=color:#719e07>const</span> c10<span style=color:#719e07>::</span>List<span style=color:#719e07>&lt;::</span>std<span style=color:#719e07>::</span>optional<span style=color:#719e07>&lt;</span>at<span style=color:#719e07>::</span>Tensor<span style=color:#719e07>&gt;&gt;</span> <span style=color:#719e07>&amp;</span>, <span style=color:#719e07>const</span> at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span>, <span style=color:#dc322f>bool</span>);
</span></span><span style=display:flex><span>  <span style=color:#719e07>using</span> ptr_schema <span style=color:#719e07>=</span> schema<span style=color:#719e07>*</span>;
</span></span><span style=display:flex><span>  <span style=color:#586e75>// See Note [static constexpr char* members for windows NVCC]
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>  <span style=color:#719e07>static</span> <span style=color:#719e07>constexpr</span> <span style=color:#719e07>const</span> <span style=color:#dc322f>char</span><span style=color:#719e07>*</span> name <span style=color:#719e07>=</span> <span style=color:#2aa198>&#34;aten::index_put&#34;</span>;
</span></span><span style=display:flex><span>  <span style=color:#719e07>static</span> <span style=color:#719e07>constexpr</span> <span style=color:#719e07>const</span> <span style=color:#dc322f>char</span><span style=color:#719e07>*</span> overload_name <span style=color:#719e07>=</span> <span style=color:#2aa198>&#34;&#34;</span>;
</span></span><span style=display:flex><span>  <span style=color:#719e07>static</span> <span style=color:#719e07>constexpr</span> <span style=color:#719e07>const</span> <span style=color:#dc322f>char</span><span style=color:#719e07>*</span> schema_str <span style=color:#719e07>=</span> <span style=color:#2aa198>&#34;index_put(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False) -&gt; Tensor&#34;</span>;
</span></span><span style=display:flex><span>  <span style=color:#719e07>static</span> at<span style=color:#719e07>::</span>Tensor call(<span style=color:#719e07>const</span> at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> self, <span style=color:#719e07>const</span> c10<span style=color:#719e07>::</span>List<span style=color:#719e07>&lt;::</span>std<span style=color:#719e07>::</span>optional<span style=color:#719e07>&lt;</span>at<span style=color:#719e07>::</span>Tensor<span style=color:#719e07>&gt;&gt;</span> <span style=color:#719e07>&amp;</span> indices, <span style=color:#719e07>const</span> at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> values, <span style=color:#dc322f>bool</span> accumulate);
</span></span><span style=display:flex><span>  <span style=color:#719e07>static</span> at<span style=color:#719e07>::</span>Tensor redispatch(c10<span style=color:#719e07>::</span>DispatchKeySet dispatchKeySet, <span style=color:#719e07>const</span> at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> self, <span style=color:#719e07>const</span> c10<span style=color:#719e07>::</span>List<span style=color:#719e07>&lt;::</span>std<span style=color:#719e07>::</span>optional<span style=color:#719e07>&lt;</span>at<span style=color:#719e07>::</span>Tensor<span style=color:#719e07>&gt;&gt;</span> <span style=color:#719e07>&amp;</span> indices, <span style=color:#719e07>const</span> at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> values, <span style=color:#dc322f>bool</span> accumulate);
</span></span><span style=display:flex><span>};
</span></span></code></pre></div><p>总体来说调用链是：</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span>m.impl(<span style=color:#2aa198>&#34;index_put.out&#34;</span>, ...) <span style=color:#586e75>// build/aten/src/ATen/RegisterCompositeExplicitAutogradEverything.cpp
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>	at<span style=color:#719e07>::</span>native<span style=color:#719e07>::</span>index_put_out
</span></span><span style=display:flex><span>		at<span style=color:#719e07>::</span>_ops<span style=color:#719e07>::</span>index_put<span style=color:#719e07>::</span>call
</span></span><span style=display:flex><span>			...
</span></span></code></pre></div><p>所以说 <code>at::native::index_put_out</code> 实际上会派发给 <code>at::native::index_put</code>。</p><p><em>是谁把算子写进查找表的呢？</em></p><p>相对地，<code>Dispatcher::findOrRegisterName_</code> 会调用 <code>operatorLookupTable_.write</code> ，这样就能将算子注册进去。这个函数被 <code>register{Name,Impl,Def}</code> 调用，可以按照扩展正则表达式搜索 <code>\.register(Name|Impl|Def)\(</code>。然后发现 <code>RegisterOperators::registerOp_</code> 在调用 <code>registerDef</code> 和 <code>registerImpl</code>。</p><p>链路：</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#719e07>explicit</span> <span style=color:#268bd2>RegisterOperators</span>(<span style=color:#719e07>const</span> std<span style=color:#719e07>::</span>string<span style=color:#719e07>&amp;</span>, FuncType<span style=color:#719e07>&amp;&amp;</span>, Options<span style=color:#719e07>&amp;&amp;</span>) <span style=color:#586e75>// （可选）
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>	c10<span style=color:#719e07>::</span>RegisterOperators<span style=color:#719e07>::</span>op
</span></span><span style=display:flex><span>		c10<span style=color:#719e07>::</span>RegisterOperators<span style=color:#719e07>::</span>checkSchemaAndRegisterOp_
</span></span><span style=display:flex><span>			c10<span style=color:#719e07>::</span>RegisterOperators<span style=color:#719e07>::</span>registerOp_ <span style=color:#586e75>// 会获取 Dispatcher 的单例，往里面加东西
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>				c10<span style=color:#719e07>::</span>Dispatcher<span style=color:#719e07>::</span>singleton().<span style=color:#719e07>register</span>{Name,Def,Impl}
</span></span><span style=display:flex><span>					c10<span style=color:#719e07>::</span>Dispatcher<span style=color:#719e07>::</span>findOrRegisterName_
</span></span><span style=display:flex><span>						c10<span style=color:#719e07>::</span>Dispatcher<span style=color:#719e07>::</span>operatorLookupTable_.write
</span></span></code></pre></div><p><code>RegisterOperators</code> 似乎是个临时类型，拿来注册用的。</p><p>还有一条是 <code>Library</code> 的路径：</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span>torch<span style=color:#719e07>::</span>Library<span style=color:#719e07>::</span>_{impl,def}
</span></span><span style=display:flex><span>	c10<span style=color:#719e07>::</span>RegisterOperators<span style=color:#719e07>::</span>registerOp_ <span style=color:#586e75>// 会获取 Dispatcher 的单例，往里面加东西
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>		c10<span style=color:#719e07>::</span>Dispatcher<span style=color:#719e07>::</span>singleton().<span style=color:#719e07>register</span>{Name,Def,Impl}
</span></span><span style=display:flex><span>			c10<span style=color:#719e07>::</span>Dispatcher<span style=color:#719e07>::</span>findOrRegisterName_
</span></span><span style=display:flex><span>				c10<span style=color:#719e07>::</span>Dispatcher<span style=color:#719e07>::</span>operatorLookupTable_.write
</span></span></code></pre></div><p>aten/src/ATen/core/op_registration/README.md 有相关的说明，可以看看理清思路。</p><p>回忆一下，之前已经有 <code>m.impl("index_put", ...)</code> 了，它转而调用了 <code>at::native::index_put</code>，因此这个表项已经被注册过了。现在可以把调用链补充完整：</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#586e75>// build/aten/src/ATen/RegisterCompositeExplicitAutogradEverything.cpp
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>m.impl(<span style=color:#2aa198>&#34;index_put.out&#34;</span>, ...)
</span></span><span style=display:flex><span>	at<span style=color:#719e07>::</span>native<span style=color:#719e07>::</span>index_put_out   <span style=color:#586e75>// build/aten/src/ATen/CompositeViewCopyKernels.cpp
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>		at<span style=color:#719e07>::</span>_ops<span style=color:#719e07>::</span>index_put<span style=color:#719e07>::</span>call <span style=color:#586e75>// build/aten/src/ATen/OperatorsEverything.cpp
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>		  <span style=color:#719e07>|</span>
</span></span><span style=display:flex><span>		  <span style=color:#719e07>|</span> find op from dispatcher
</span></span><span style=display:flex><span>		  <span style=color:#719e07>|</span>
</span></span><span style=display:flex><span>			<span style=color:#586e75>// build/aten/src/ATen/RegisterCompositeExplicitAutogradEverything.cpp
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>			m.impl(<span style=color:#2aa198>&#34;index_put&#34;</span>, ...)
</span></span><span style=display:flex><span>				<span style=color:#586e75>// build/aten/src/ATen/RegisterCompositeExplicitAutogradEverything.cpp
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>				TORCH_FN(wrapper_CompositeExplicitAutograd__index_put)
</span></span><span style=display:flex><span>					<span style=color:#586e75>// aten/src/ATen/native/TensorAdvancedIndexing.cpp
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>					at<span style=color:#719e07>::</span>native<span style=color:#719e07>::</span>index_put
</span></span></code></pre></div><p>通过再次派发，autogen 生成的函数能调用原函数，从而减少了编写接口的负担。</p><h1 id=structured-算子以-index_out-为例><code>structured</code> 算子：以 <code>index_out</code> 为例
<a class=header-anchor href=#structured-%e7%ae%97%e5%ad%90%e4%bb%a5-index_out-%e4%b8%ba%e4%be%8b></a></h1><p><code>at::cuda::index_out</code> 这个函数被 <code>at::native::masked_select_out_cuda_impl</code> 调用了，我之前也一直疑惑 <code>masked_select</code> 是在哪里调用 <code>nonzero</code> 的，现在就通过这个例子来查找调用路径。我们先来看 <code>at::cuda::index_out</code> 函数的实现，接着看 <code>m.impl</code> 是怎么实现的。</p><p>在 native_functions.yaml 中找到的最相关的记录为：</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#719e07>-</span> func: index.Tensor_out(Tensor self, Tensor<span style=color:#719e07>?</span>[] indices, <span style=color:#719e07>*</span>, Tensor(a<span style=color:#719e07>!</span>) out) <span style=color:#719e07>-&gt;</span> Tensor(a<span style=color:#719e07>!</span>)
</span></span><span style=display:flex><span>  device_check: NoCheck
</span></span><span style=display:flex><span>  structured: True
</span></span><span style=display:flex><span>  structured_inherits: TensorIteratorBase
</span></span><span style=display:flex><span>  precomputed:
</span></span><span style=display:flex><span>  <span style=color:#719e07>-</span> indices <span style=color:#719e07>-&gt;</span> DimVector sizes, DimVector strides
</span></span><span style=display:flex><span>  dispatch:
</span></span><span style=display:flex><span>    CPU, CUDA, MPS: index_out
</span></span></code></pre></div><p>注意这个 <code>structured</code> 标记。很多算子在 native_functions.yaml 中都会有 <code>structured</code> 标记，或者 <code>structured_delegate</code> 标记。在代码生成之后，相关声明也会生成在 build/aten/src/ATen/NativeFunctions.h 文件中。按我理解，<code>structured</code> 把算子的运算流程分离成几步，允许开发者对每个方法分别定义。</p><h2 id=生成的代码>生成的代码
<a class=header-anchor href=#%e7%94%9f%e6%88%90%e7%9a%84%e4%bb%a3%e7%a0%81></a></h2><p>在 build/aten/src/ATen/RegisterCUDAEverything.cpp 中：</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#719e07>namespace</span> at {
</span></span><span style=display:flex><span>at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> wrapper_CUDA_index_out_Tensor_out(<span style=color:#719e07>const</span> at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> self, <span style=color:#719e07>const</span> c10<span style=color:#719e07>::</span>List<span style=color:#719e07>&lt;::</span>std<span style=color:#719e07>::</span>optional<span style=color:#719e07>&lt;</span>at<span style=color:#719e07>::</span>Tensor<span style=color:#719e07>&gt;&gt;</span> <span style=color:#719e07>&amp;</span> indices, at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> out) {
</span></span><span style=display:flex><span>  <span style=color:#586e75>// No device check
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>structured_index_out_out <span style=color:#268bd2>op</span>(out);
</span></span><span style=display:flex><span><span style=color:#719e07>auto</span> precompute <span style=color:#719e07>=</span> op.meta(self, at<span style=color:#719e07>::</span>IOptTensorListRef(indices));
</span></span><span style=display:flex><span>(<span style=color:#dc322f>void</span>)precompute;
</span></span><span style=display:flex><span>op.impl(self, precompute.sizes, precompute.strides, op.maybe_get_output(<span style=color:#2aa198>0</span>));
</span></span><span style=display:flex><span><span style=color:#719e07>if</span> (op.proxy_outputs_[<span style=color:#2aa198>0</span>].has_value()) op.outputs_[<span style=color:#2aa198>0</span>].get().copy_(<span style=color:#719e07>*</span>op.proxy_outputs_[<span style=color:#2aa198>0</span>]);
</span></span><span style=display:flex><span><span style=color:#719e07>return</span> out;
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#719e07>namespace</span> cuda {
</span></span><span style=display:flex><span>at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> index_out(at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> out, <span style=color:#719e07>const</span> at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> self, <span style=color:#719e07>const</span> c10<span style=color:#719e07>::</span>List<span style=color:#719e07>&lt;::</span>std<span style=color:#719e07>::</span>optional<span style=color:#719e07>&lt;</span>at<span style=color:#719e07>::</span>Tensor<span style=color:#719e07>&gt;&gt;</span> <span style=color:#719e07>&amp;</span> indices) {
</span></span><span style=display:flex><span><span style=color:#719e07>return</span> <span style=color:#268bd2>wrapper_CUDA_index_out_Tensor_out</span>(self, indices, out);
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>最主要的是 <code>structured_index_out_out</code> 这个类，还有 <code>meta</code> 和 <code>impl</code> 两个方法。</p><h2 id=impl><code>impl</code>
<a class=header-anchor href=#impl></a></h2><p>先看定义（<code>impl</code>）。在 aten/src/ATen/native/TensorAdvancedIndexing.cpp 中：</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span>TORCH_IMPL_FUNC(index_out)
</span></span><span style=display:flex><span>(<span style=color:#719e07>const</span> Tensor<span style=color:#719e07>&amp;</span> self, DimVector sizes, DimVector strides, <span style=color:#719e07>const</span> Tensor<span style=color:#719e07>&amp;</span> result) {
</span></span><span style=display:flex><span>  index_stub(device_type(), <span style=color:#719e07>*</span><span style=color:#719e07>this</span>, sizes, strides);
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>而 <code>TORCH_IMPL_FUNC</code> 定义如下：</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#719e07>#define TORCH_IMPL_FUNC(name) void structured_##name::impl
</span></span></span></code></pre></div><p>因此会展开成这样（因为是宏展开，代码里面搜不到）：</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#dc322f>void</span> structured_index_out<span style=color:#719e07>::</span>impl
</span></span><span style=display:flex><span>(<span style=color:#719e07>const</span> Tensor<span style=color:#719e07>&amp;</span> self, DimVector sizes, DimVector strides, <span style=color:#719e07>const</span> Tensor<span style=color:#719e07>&amp;</span> result) {
</span></span><span style=display:flex><span>  index_stub(device_type(), <span style=color:#719e07>*</span><span style=color:#719e07>this</span>, sizes, strides);
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>对应的声明是代码生成的，在 build/aten/src/ATen/NativeFunctions.h 中：</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#719e07>struct</span> <span style=color:#268bd2>TORCH_API</span> structured_index_out : <span style=color:#719e07>public</span> at<span style=color:#719e07>::</span>meta<span style=color:#719e07>::</span>structured_index_Tensor {
</span></span><span style=display:flex><span><span style=color:#dc322f>void</span> <span style=color:#268bd2>impl</span>(<span style=color:#719e07>const</span> at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> self, at<span style=color:#719e07>::</span>DimVector sizes, at<span style=color:#719e07>::</span>DimVector strides, <span style=color:#719e07>const</span> at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> out);
</span></span><span style=display:flex><span>};
</span></span></code></pre></div><h2 id=meta><code>meta</code>
<a class=header-anchor href=#meta></a></h2><p>接着看预处理（<code>meta</code>）。在 aten/src/ATen/TensorMeta.h 中：</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#586e75>// ...
</span></span></span><span style=display:flex><span><span style=color:#586e75></span><span style=color:#719e07>#define TORCH_PRECOMPUTE_META_FUNC(name) \
</span></span></span><span style=display:flex><span><span style=color:#719e07>  structured_##name::meta_return_ty structured_##name::meta
</span></span></span><span style=display:flex><span><span style=color:#719e07>#define TORCH_PRECOMPUTE_META_FUNC2(name, overload) \
</span></span></span><span style=display:flex><span><span style=color:#719e07>  structured_##name##_##overload::meta_return_ty    \
</span></span></span><span style=display:flex><span><span style=color:#719e07>      structured_##name##_##overload::meta
</span></span></span><span style=display:flex><span><span style=color:#719e07></span>
</span></span><span style=display:flex><span><span style=color:#586e75>// Use this to create a precompute struct in a meta function.
</span></span></span><span style=display:flex><span><span style=color:#586e75></span><span style=color:#719e07>#define TORCH_PRECOMPUTE_STRUCT(name) structured_##name::precompute_out&lt;&gt;
</span></span></span><span style=display:flex><span><span style=color:#719e07>#define TORCH_PRECOMPUTE_STRUCT2(name, overload) \
</span></span></span><span style=display:flex><span><span style=color:#719e07>  structured_##name##_##overload::precompute_out&lt;&gt;
</span></span></span></code></pre></div><p>在 aten/src/ATen/native/TensorAdvancedIndexing.cpp 中：</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span>TORCH_PRECOMPUTE_META_FUNC2(index, Tensor)
</span></span><span style=display:flex><span>(<span style=color:#719e07>const</span> Tensor<span style=color:#719e07>&amp;</span> self, at<span style=color:#719e07>::</span>IOptTensorListRef indices) {
</span></span><span style=display:flex><span>  <span style=color:#586e75>// ...
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>  <span style=color:#719e07>auto</span> info <span style=color:#719e07>=</span> at<span style=color:#719e07>::</span>native<span style=color:#719e07>::</span>make_info(self, std<span style=color:#719e07>::</span>move(indices));
</span></span><span style=display:flex><span>  build_index_op(<span style=color:#719e07>*</span><span style=color:#719e07>this</span>, info, result);
</span></span><span style=display:flex><span>  <span style=color:#719e07>return</span> <span style=color:#268bd2>TORCH_PRECOMPUTE_STRUCT2</span>(index, Tensor)()
</span></span><span style=display:flex><span>      .set_sizes(std<span style=color:#719e07>::</span>move(info.indexed_sizes))
</span></span><span style=display:flex><span>      .set_strides(std<span style=color:#719e07>::</span>move(info.indexed_strides));
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>因此 <code>structured_index_Tensor</code> 的 <code>meta</code> 函数能对 indices 进行预计算，在 <code>make_info</code> 中将 BoolTensor 转换成 LongTensor（代码略），并获取 sizes、strides 信息。</p><h2 id=放在一起看>放在一起看
<a class=header-anchor href=#%e6%94%be%e5%9c%a8%e4%b8%80%e8%b5%b7%e7%9c%8b></a></h2><p>我们找到的 impl 是 <code>structured_index_out::impl</code>，找到的 meta 是 <code>structured_index_Tensor::meta</code>，还要将他们和 <code>structured_index_out_out</code> 联系起来。</p><p>在 build/aten/src/ATen/RegisterCUDAEverything.cpp 中：</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#719e07>struct</span> <span style=color:#268bd2>structured_index_out_out</span> <span style=color:#719e07>final</span> <span style=color:#719e07>:</span> <span style=color:#719e07>public</span> at<span style=color:#719e07>::</span>native<span style=color:#719e07>::</span>structured_index_out
</span></span></code></pre></div><p>因为继承关系，<code>structured_index_out_out</code> 自然得到了 <code>structured_index_out::impl</code>。</p><p>在 build/aten/src/ATen/NativeFunctions.h 中：</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#719e07>struct</span> <span style=color:#268bd2>TORCH_API</span> structured_index_out : <span style=color:#719e07>public</span> at<span style=color:#719e07>::</span>meta<span style=color:#719e07>::</span>structured_index_Tensor
</span></span></code></pre></div><p>因为继承关系，<code>structured_index_out</code> 又得到了 <code>structured_index_Tensor::meta</code>。</p><p>继承链为：</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#719e07>-</span> at<span style=color:#719e07>::</span>TensorIteratorBase
</span></span><span style=display:flex><span>  <span style=color:#719e07>-</span> at<span style=color:#719e07>::</span>meta<span style=color:#719e07>::</span>structured_index_Tensor (provides meta)
</span></span><span style=display:flex><span>	  <span style=color:#719e07>-</span> at<span style=color:#719e07>::</span>native<span style=color:#719e07>::</span>structured_index_out (provides impl)
</span></span><span style=display:flex><span>		  <span style=color:#719e07>-</span> at<span style=color:#719e07>::</span>native<span style=color:#719e07>::</span>structured_index_out_out
</span></span></code></pre></div><p>所以 masked_select 调用了 index_out，后者按顺序调用 structured_index_out_out 的 meta 和 impl 方法。meta 方法来自 at::meta::structured_index_Tensor::meta，内部会调用 expandTensors 函数将 mask 转换成 LongTensor 下标（代码略）。impl 方法来自 at::native::structured_index_out::impl，会调用 index_stub。</p><h2 id=再来看-mimplat-名字空间而非-atcuda>再来看 <code>m.impl</code>（<code>at</code> 名字空间，而非 <code>at::cuda</code>）
<a class=header-anchor href=#%e5%86%8d%e6%9d%a5%e7%9c%8b-mimplat-%e5%90%8d%e5%ad%97%e7%a9%ba%e9%97%b4%e8%80%8c%e9%9d%9e-atcuda></a></h2><p>也是结构化地先 meta 后 impl 的实现方式，和非 structured 的算子生成方式不同。这里在 wrapper 中没有调用 at::native 名字空间函数（<code>at::native::index_out</code> ），所以不需要为其提供定义，代码其他地方也找不到。回忆前面，index_out 的定义实际上是通过 <code>TORCH_IMPL_FUNC(index_out)</code> 提供的，即 <code>at::native::structured_index_out::impl</code>，并非 <code>at::native::index_out</code> 函数。</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#719e07>namespace</span> at {
</span></span><span style=display:flex><span>at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> wrapper_CUDA_index_out_Tensor_out(<span style=color:#719e07>const</span> at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> self, <span style=color:#719e07>const</span> c10<span style=color:#719e07>::</span>List<span style=color:#719e07>&lt;::</span>std<span style=color:#719e07>::</span>optional<span style=color:#719e07>&lt;</span>at<span style=color:#719e07>::</span>Tensor<span style=color:#719e07>&gt;&gt;</span> <span style=color:#719e07>&amp;</span> indices, at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> out) {
</span></span><span style=display:flex><span>  <span style=color:#586e75>// No device check
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>structured_index_out_out <span style=color:#268bd2>op</span>(out);
</span></span><span style=display:flex><span><span style=color:#719e07>auto</span> precompute <span style=color:#719e07>=</span> op.meta(self, at<span style=color:#719e07>::</span>IOptTensorListRef(indices));
</span></span><span style=display:flex><span>(<span style=color:#dc322f>void</span>)precompute;
</span></span><span style=display:flex><span>op.impl(self, precompute.sizes, precompute.strides, op.maybe_get_output(<span style=color:#2aa198>0</span>));
</span></span><span style=display:flex><span><span style=color:#719e07>if</span> (op.proxy_outputs_[<span style=color:#2aa198>0</span>].has_value()) op.outputs_[<span style=color:#2aa198>0</span>].get().copy_(<span style=color:#719e07>*</span>op.proxy_outputs_[<span style=color:#2aa198>0</span>]);
</span></span><span style=display:flex><span><span style=color:#719e07>return</span> out;
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>TORCH_LIBRARY_IMPL(aten, CUDA, m) {
</span></span><span style=display:flex><span>    m.impl(<span style=color:#2aa198>&#34;index.Tensor&#34;</span>, TORCH_FN(wrapper_CUDA_index_Tensor));
</span></span><span style=display:flex><span>m.impl(<span style=color:#2aa198>&#34;index.Tensor_out&#34;</span>, TORCH_FN(wrapper_CUDA_index_out_Tensor_out));
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>注意到这个 <code>at::wrapper_CUDA_index_out_Tensor_out</code> 在前面也用到了！实际上就是相同的实现，在 <code>m.impl</code> 用到了，在 <code>at::cuda::index_out</code> 中也用到了。</p></div><footer class=post-footer><div class=post-tags><a href=/tags/torch>torch</a></div><div class=addthis_inline_share_toolbox style=text-align:center></div><hr><div class=post-nav><div class="post-nav-next post-nav-item"><a href=/posts/cli/bash/diff-%E4%B8%A4%E4%B8%AA%E6%96%87%E4%BB%B6%E5%A4%B9/ rel=next title="diff 两个文件夹"><i class="fa fa-chevron-left"></i> diff 两个文件夹</a></div><div class="post-nav-prev post-nav-item"><a href=/posts/programming/python/1.-PyTorch-C++-%E5%87%BD%E6%95%B0%E6%B4%BE%E5%8F%91/ rel=prev title="PyTorch C++ 函数派发">PyTorch C++ 函数派发
<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>&copy;
<span itemprop=copyrightYear>2023 - 2025
</span><span class=with-love><i class="fa fa-heart"></i>
</span><span class=author itemprop=copyrightHolder>🤖</span></div><div class=powered-by>由 <a href=https://gohugo.io title=0.143.1 target=_blank>Hugo</a> & <a href=https://github.com/hugo-next/hugo-theme-next title=4.5.3 target=_blank>Hugo NexT.Gemini</a> 强力驱动</div></div></footer><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js defer></script><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.1.0/medium-zoom.min.js defer></script><script class=next-config data-name=main type=application/json>{"bookmark":{"color":"#222","enable":false,"save":"manual"},"copybtn":true,"darkmode":false,"hostname":"https://hxhue.github.io/","i18n":{"ds_day":" 天前","ds_days":" 天 ","ds_hour":" 小时前","ds_hours":" 小时 ","ds_just":"刚刚","ds_min":" 分钟前","ds_mins":" 分钟","ds_month":" 个月前","ds_years":" 年 ","empty":"没有找到任何搜索结果：${query}","hits":"找到 ${hits} 个搜索结果","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","placeholder":"搜索..."},"lang":"zh-CN","lazyload":false,"localSearch":{"enable":true,"limit":1e3,"path":"/searchindexes.xml","preload":false,"topnperarticle":-1,"trigger":"auto","unescape":false},"motion":{"async":true,"enable":false,"transition":{"collheader":"fadeInLeft","menu_item":"fadeInDown","postblock":"fadeIn","postbody":"fadeInDown","postheader":"fadeInDown","sidebar":"fadeInUp"}},"postmeta":{"comments":{"enable":false,"plugin":"waline"},"views":{"enable":false,"plugin":"busuanzi"}},"root":"/","scheme":"Gemini","sidebar":{"display":"post","offset":12,"padding":18,"position":"left","width":256},"vendor":{"plugins":"cdnjs","router":"https://cdnjs.cloudflare.com/ajax/libs"},"version":"4.5.3"}</script><script type=text/javascript src=/js/main.min.37ba8b54f9d4d784d08028c45eea93b5d4e13eda8ee7fb0d2edd6f3fac66cfd2.js defer></script></body></html>