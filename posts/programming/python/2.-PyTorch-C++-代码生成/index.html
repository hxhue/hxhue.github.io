<!doctype html><html lang=zh-CN data-theme=light><head><meta charset=UTF-8><meta name=viewport content="width=device-width"><meta name=theme-color content="#222" media="(prefers-color-scheme: light)"><meta name=generator content="Hugo 0.143.1"><link rel="shortcut icon" type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=/imgs/icons/favicon_16x16_next.png><link rel=icon type=image/png sizes=32x32 href=/imgs/icons/favicon_32_32_next.png><link rel=apple-touch-icon sizes=180x180 href=/imgs/icons/apple_touch_icon_next.png><meta itemprop=name content="PyTorch C++ ä»£ç ç”Ÿæˆ"><meta itemprop=description content="ä¸ªäººåšå®¢ï¼Œä¸»è¦æ˜¯é›¶æ•£çš„ç¬”è®°ã€‚"><meta itemprop=datePublished zgotmplz><meta itemprop=dateModified zgotmplz><meta itemprop=image content="https://hxhue.github.io/imgs/371907.jpg"><meta itemprop=keywords content="torch"><link type=text/css rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.2/css/all.min.css><link type=text/css rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css><link rel=stylesheet href=/css/main.min.bea76f574a755574e17d42bea39502a74ca3ca4db65807b8c82d3e26dcec8420.css><style type=text/css>.post-footer,.flinks-list-footer hr:after{content:"~ æˆ‘å¯æ˜¯æœ‰åº•çº¿çš„å“Ÿ ~"}</style><link rel=stylesheet type=text/css href=/css/style.css><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/github-markdown-css@5.3.0/github-markdown-dark.css><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js></script><script>MathJax={tex:{displayMath:[["$$","$$"],["\\[","\\]"]],inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"}}</script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.esm.min.mjs';
  mermaid.initialize({ startOnLoad: false });
  mermaid.mermaidAPI.initialize();
  window.mermaid = mermaid;
</script><script type=text/javascript>(function(){localDB={set:function(e,t,n){if(n===0)return;const s=new Date,o=n*864e5,i={value:t,expiry:s.getTime()+o};localStorage.setItem(e,JSON.stringify(i))},get:function(e){const t=localStorage.getItem(e);if(!t)return void 0;const n=JSON.parse(t),s=new Date;return s.getTime()>n.expiry?(localStorage.removeItem(e),void 0):n.value}},theme={active:function(){const e=localDB.get("theme");if(e==null)return;theme.toggle(e),window.matchMedia("(prefers-color-scheme: dark)").addListener(function(e){theme.toggle(e.matches?"dark":"light")})},toggle:function(e){document.documentElement.setAttribute("data-theme",e),localDB.set("theme",e,2);const t=document.querySelector("iframe.giscus-frame");if(t){const n={setConfig:{theme:e}};t.contentWindow.postMessage({giscus:n},"https://giscus.app")}}},theme.active()})(window)</script><script class=next-config data-name=page type=application/json>{"comments":false,"isHome":false,"isPage":true,"path":"2.-PyTorch-C++-%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90","permalink":"https://hxhue.github.io/posts/programming/python/2.-PyTorch-C++-%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90/","title":"PyTorch C++ ä»£ç ç”Ÿæˆ","waline":{"js":[{"alias":"waline","alias_name":"@waline/client","file":"dist/pageview.js","name":"pageview","version":"2.13.0"},{"alias":"waline","alias_name":"@waline/client","file":"dist/comment.js","name":"comment","version":"2.13.0"}]}}</script><script type=text/javascript>document.addEventListener("DOMContentLoaded",()=>{var e=document.createElement("script");e.charset="UTF-8",e.src="",e.async=!1,e.defer=!0,document.head.appendChild(e),e.onload=function(){NexT.utils.fmtBusuanzi()}})</script><title>PyTorch C++ ä»£ç ç”Ÿæˆ - Bluegill</title><noscript><link rel=stylesheet href=/css/noscript.css></noscript></head><body itemscope itemtype=http://schema.org/WebPage><div class=headband></div><main class=main><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div class=toggle aria-label role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div></div><div class=site-meta><a href=/ class=brand rel=start><i class=logo-line></i><h1 class=site-title>Bluegill</h1><i class=logo-line></i></a><p class=site-subtitle itemprop=description></p></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href=/ class=hvr-icon-pulse rel=section><i class="fa fa-home hvr-icon"></i>é¦–é¡µ</a></li><li class="menu-item menu-item-about"><a href=/about/ class=hvr-icon-pulse rel=section><i class="fa fa-user hvr-icon"></i>å…³äº</a></li><li class="menu-item menu-item-archives"><a href=/archives/ class=hvr-icon-pulse rel=section><i class="fa fa-archive hvr-icon"></i>å½’æ¡£</a></li><li class="menu-item menu-item-categories"><a href=/categories/ class=hvr-icon-pulse rel=section><i class="fa fa-th hvr-icon"></i>åˆ†ç±»</a></li><li class="menu-item menu-item-tags"><a href=/tags/ class=hvr-icon-pulse rel=section><i class="fa fa-hashtag hvr-icon"></i>æ ‡ç­¾</a></li><li class="menu-item menu-item-daily"><a href=/daily/ class=hvr-icon-pulse rel=section><i class="fa fa-newspaper hvr-icon"></i>éšç¬”</a></li><li class="menu-item menu-item-discovery"><a href=https://rift-fear-f2c.notion.site/2025-1e354a33cfb1802c841bdf29f2f3dab3 class=hvr-icon-pulse rel=section><i class="fa fa-compass hvr-icon"></i>å‘ç°</a></li><li class="menu-item menu-item-search"><a role=button class="popup-trigger hvr-icon-pulse"><i class="fa fa-search fa-fw hvr-icon"></i>æœç´¢</a></li></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon><i class="fa fa-search"></i></span><div class=search-input-container><input autocomplete=off autocapitalize=off maxlength=80 placeholder=æœç´¢... spellcheck=false type=search class=search-input></div><span class=popup-btn-close role=button><i class="fa fa-times-circle"></i></span></div><div class="search-result-container no-result"><div class=search-result-icon><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></div><div class="toggle sidebar-toggle" role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div><aside class=sidebar><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class=sidebar-nav><li class=sidebar-nav-toc>æ–‡ç« ç›®å½•</li><li class=sidebar-nav-overview>ç«™ç‚¹æ¦‚è§ˆ</li></ul><div class=sidebar-panel-container><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><nav id=TableOfContents><ul><li><a href=#ä¸€äº›é—®é¢˜>ä¸€äº›é—®é¢˜</a></li><li><a href=#æè¦>æè¦</a></li><li><a href=#ç”Ÿæˆä»£ç >ç”Ÿæˆä»£ç </a></li><li><a href=#é-structured-ç®—å­ä»¥-index_put_-ä¸ºä¾‹>é <code>structured</code> ç®—å­ï¼šä»¥ <code>index_put_</code> ä¸ºä¾‹</a><ul><li><a href=#è¿½è¸ª-index_put_-çš„æ´¾å‘>è¿½è¸ª <code>index_put_</code> çš„æ´¾å‘</a></li><li><a href=#è¿½è¸ª-index_putout-çš„-autogen>è¿½è¸ª <code>index_put.out</code> çš„ autogen</a></li></ul></li><li><a href=#structured-ç®—å­ä»¥-index_out-ä¸ºä¾‹><code>structured</code> ç®—å­ï¼šä»¥ <code>index_out</code> ä¸ºä¾‹</a><ul><li><a href=#ç”Ÿæˆçš„ä»£ç >ç”Ÿæˆçš„ä»£ç </a></li><li><a href=#impl><code>impl</code></a></li><li><a href=#meta><code>meta</code></a></li><li><a href=#æ”¾åœ¨ä¸€èµ·çœ‹>æ”¾åœ¨ä¸€èµ·çœ‹</a></li><li><a href=#å†æ¥çœ‹-mimplat-åå­—ç©ºé—´è€Œé-atcuda>å†æ¥çœ‹ <code>m.impl</code>ï¼ˆ<code>at</code> åå­—ç©ºé—´ï¼Œè€Œé <code>at::cuda</code>ï¼‰</a></li></ul></li></ul></nav></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author site-overview-item animated" itemprop=author itemscope itemtype=http://schema.org/Person><img class=site-author-image itemprop=image alt=ğŸ¤– src=/imgs/371907.jpg><p class=site-author-name itemprop=name>ğŸ¤–</p><div class=site-description itemprop=description>ä¸ªäººåšå®¢ï¼Œä¸»è¦æ˜¯é›¶æ•£çš„ç¬”è®°ã€‚</div></div><div class="site-state-wrap site-overview-item animated"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/><span class=site-state-item-count>433</span>
<span class=site-state-item-name>æ—¥å¿—</span></a></div><div class="site-state-item site-state-categories"><a href=/categories/><span class=site-state-item-count>12</span>
<span class=site-state-item-name>åˆ†ç±»</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/><span class=site-state-item-count>86</span>
<span class=site-state-item-name>æ ‡ç­¾</span></a></div></nav></div><div class="links-of-social site-overview-item animated"><span class=links-of-social-item><a href=https://github.com/hxhue title="Github â†’ https://github.com/hxhue" rel=noopener target=_blank><i class="fab fa-github fa-fw"></i>
Github
</a></span><span class=links-of-social-item><a href=/rss.xml title="RSS è®¢é˜… â†’ /rss.xml" rel=noopener target=_blank><i class="fa fa-rss fa-fw"></i>
RSS è®¢é˜…</a></span></div><div class="cc-license animated" itemprop=license><a href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh class=cc-opacity rel=noopener target=_blank title=å…±äº«çŸ¥è¯†><img src=/imgs/cc/big/by_nc_sa.svg alt=å…±äº«çŸ¥è¯†></a></div><div class="links-of-blogroll site-overview-item animated"><div class=links-of-blogroll-title><i class="fa fa-globe fa-fw"></i>
å‹æƒ…é“¾æ¥</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=https://shuai.guru/ title=https://shuai.guru/ target=_blank>shuai.guru</a></li></ul></div></div></div></div></aside><div class=sidebar-dimmer></div></header><div class=tool-buttons><div id=toggle-theme class=button title=æ·±æµ…æ¨¡å¼åˆ‡æ¢><i class="fas fa-adjust"></i></div><div class=back-to-top role=button title=è¿”å›é¡¶éƒ¨><i class="fa fa-arrow-up"></i>
<span>0%</span></div></div><div class=reading-progress-bar></div><noscript><div class=noscript-warning>Theme NexT works best with JavaScript enabled</div></noscript><div class="main-inner post posts-expand"><div class=post-block><article itemscope itemtype=http://schema.org/Article class=post-content lang><link itemprop=mainEntityOfPage href=https://hxhue.github.io/posts/programming/python/2.-PyTorch-C++-%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90/><span hidden itemprop=author itemscope itemtype=http://schema.org/Person><meta itemprop=image content="/imgs/371907.jpg"><meta itemprop=name content="ğŸ¤–"></span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization><meta itemprop=name content="ğŸ¤–"><meta itemprop=description content="ä¸ªäººåšå®¢ï¼Œä¸»è¦æ˜¯é›¶æ•£çš„ç¬”è®°ã€‚"></span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork><meta itemprop=name content="PyTorch C++ ä»£ç ç”Ÿæˆ"><meta itemprop=description content='ä¸€äº›é—®é¢˜

ä¸ºä»€ä¹ˆæœ‰äº›ä¼šç”Ÿæˆ at::cuda åå­—ç©ºé—´çš„å‡½æ•°ï¼Œæœ‰äº›ä¸ä¼šï¼Ÿï¼ˆå¾…è§£å†³ï¼‰
æè¦

æœ¬æ–‡è¯´æ˜äº† m.impl("index_put.out", ...) åˆ° at::native::index_put çš„è°ƒç”¨è·¯å¾„ã€‚ç»“åˆ 
  
  
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    PyTorch C++ å‡½æ•°æ´¾å‘ ä¸­ at::native::index_put_ â†’ at::_index_put_impl_ â†’ index_put_stub çš„è°ƒç”¨è·¯å¾„ï¼Œè¡¥å…¨äº†ä» m.impl åˆ° stub çš„å…¨è·¯å¾„ã€‚'></span><header class=post-header><h1 class=post-title itemprop="name headline">PyTorch C++ ä»£ç ç”Ÿæˆ</h1><div class=post-meta-container><div class=post-meta-items><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar"></i>
</span><span class=post-meta-item-text title=å‘è¡¨äº>å‘è¡¨äºï¼š
</span><time title="åˆ›å»ºæ—¶é—´ï¼š2025-06-30 00:00:00 +0800 CST" itemprop="dateCreated datePublished" datetime="2025-06-30 00:00:00 +0800 CST">2025-06-30
</time></span><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar-check"></i>
</span><span class=post-meta-item-text title=æ›´æ–°äº>æ›´æ–°äºï¼š
</span><time title=ä¿®æ”¹æ—¶é—´ï¼š2025-10-02T00:00:00+08:00 itemprop=dateModified datetime=2025-10-02T00:00:00+08:00>2025-10-02</time></span></div><div class=post-meta-items><span class=post-meta-item title=å­—æ•°><span class=post-meta-item-icon><i class="far fa-file-word"></i>
</span><span class=post-meta-item-text>å­—æ•°ï¼š</span>
<span>3145</span>
</span><span class=post-meta-item title=é˜…è¯»><span class=post-meta-item-icon><i class="far fa-clock"></i>
</span><span class=post-meta-item-text>é˜…è¯»ï¼š&ap;</span>
<span>7åˆ†é’Ÿ</span></span></div></div></header><div class=post-body itemprop=articleBody><h1 id=ä¸€äº›é—®é¢˜>ä¸€äº›é—®é¢˜
<a class=header-anchor href=#%e4%b8%80%e4%ba%9b%e9%97%ae%e9%a2%98></a></h1><p>ä¸ºä»€ä¹ˆæœ‰äº›ä¼šç”Ÿæˆ at::cuda åå­—ç©ºé—´çš„å‡½æ•°ï¼Œæœ‰äº›ä¸ä¼šï¼Ÿï¼ˆå¾…è§£å†³ï¼‰</p><h1 id=æè¦>æè¦
<a class=header-anchor href=#%e6%8f%90%e8%a6%81></a></h1><p><strong>æœ¬æ–‡è¯´æ˜äº† <code>m.impl("index_put.out", ...)</code> åˆ° <code>at::native::index_put</code> çš„è°ƒç”¨è·¯å¾„</strong>ã€‚ç»“åˆ
<a href=/posts/programming/python/1.-PyTorch-C++-%E5%87%BD%E6%95%B0%E6%B4%BE%E5%8F%91/ title="PyTorch C++ å‡½æ•°æ´¾å‘">PyTorch C++ å‡½æ•°æ´¾å‘</a> ä¸­ <code>at::native::index_put_</code> â†’ <code>at::_index_put_impl_</code> â†’ <code>index_put_stub</code> çš„è°ƒç”¨è·¯å¾„ï¼Œè¡¥å…¨äº†ä» m.impl åˆ° stub çš„å…¨è·¯å¾„ã€‚</p><p><strong>æœ¬æ–‡è¯´æ˜äº† <code>m.impl</code> / <code>at::cuda::index_out</code> â†’ <code>index_stub</code> çš„è°ƒç”¨è·¯å¾„</strong>ã€‚å‡†ç¡®æ¥è¯´æ˜¯ä»‹ç»äº† <code>at::cuda::index_out</code> è°ƒç”¨ meta å’Œ impl çš„è¿‡ç¨‹ï¼Œmeta ä¸­å¯¹ä¸‹æ ‡åšé¢„å¤„ç†ï¼ˆåŒ…æ‹¬ kBool è½¬ kLong ä¸‹æ ‡ï¼‰ï¼Œimpl ä¸­è°ƒç”¨ <code>index_stub</code> è¿›è¡Œè®¡ç®—ã€‚å’Œ <code>index_out</code> ä¸åŒï¼Œ<code>index_put_</code> å‡½æ•°æ²¡æœ‰å‡ºç°åœ¨ <code>at::cuda</code> åå­—ç©ºé—´ä¸­ï¼Œå–è€Œä»£ä¹‹çš„æ˜¯ <code>at::cuda::_index_put_impl_</code>ã€‚</p><p>åœ¨è¿™ä¸¤ä¸ªä¾‹å­ä¸­ï¼Œèƒ½æ‰¾åˆ°çš„å‡½æ•°æœ‰ï¼š</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span>ç†è§£ native_functions.yaml ä¸­çš„å‡½æ•°å®šä¹‰åœ¨å“ªé‡Œ
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>é structured æƒ…å†µï¼š
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#719e07>-</span> func: index_put_(...)
</span></span><span style=display:flex><span>  dispatch:
</span></span><span style=display:flex><span>	  CompositeExplicitAutograd: index_put_ # é»˜è®¤åå­—ç©ºé—´æ˜¯ aten
</span></span><span style=display:flex><span>			\ at<span style=color:#719e07>::</span>index_put_ (build<span style=color:#719e07>/</span>aten<span style=color:#719e07>/</span>src<span style=color:#719e07>/</span>ATen<span style=color:#719e07>/</span>Functions.h)
</span></span><span style=display:flex><span>			 \ at<span style=color:#719e07>::</span>_ops<span style=color:#719e07>::</span>index_put_<span style=color:#719e07>::</span>call (build<span style=color:#719e07>/</span>aten<span style=color:#719e07>/</span>src<span style=color:#719e07>/</span>ATen<span style=color:#719e07>/</span>OperatorsEverything.cpp)
</span></span><span style=display:flex><span>			  \ c10<span style=color:#719e07>::</span>Dispatcher<span style=color:#719e07>::</span>singleton()
</span></span><span style=display:flex><span>			   <span style=color:#719e07>|</span>  .findSchemaOrThrow(index_put_<span style=color:#719e07>::</span>name, index_put_<span style=color:#719e07>::</span>overload_name)
</span></span><span style=display:flex><span>			   <span style=color:#719e07>|</span>  .typed<span style=color:#719e07>&lt;</span>index_put_<span style=color:#719e07>::</span>schema<span style=color:#719e07>&gt;</span>().call
</span></span><span style=display:flex><span>				 <span style=color:#719e07>|</span><span style=color:#719e07>struct</span> <span style=color:#268bd2>TORCH_API</span> index_put_ (build<span style=color:#719e07>/</span>aten<span style=color:#719e07>/</span>src<span style=color:#719e07>/</span>ATen<span style=color:#719e07>/</span>MethodOperators.h)
</span></span><span style=display:flex><span>				 <span style=color:#719e07>|</span>  ... name <span style=color:#719e07>=</span> <span style=color:#2aa198>&#34;aten::index_put_&#34;</span>;
</span></span><span style=display:flex><span>				 <span style=color:#719e07>|</span>  ... overload_name <span style=color:#719e07>=</span> <span style=color:#2aa198>&#34;&#34;</span>;
</span></span><span style=display:flex><span>				 \ m.impl(<span style=color:#2aa198>&#34;index_put_&#34;</span>, ...) (build<span style=color:#719e07>/</span>aten<span style=color:#719e07>/</span>src<span style=color:#719e07>/</span>ATen<span style=color:#719e07>/</span>RegisterCompositeExplicitAutogradEverything.cpp)
</span></span><span style=display:flex><span>				  \ at<span style=color:#719e07>::</span>native<span style=color:#719e07>::</span>index_put_ (aten<span style=color:#719e07>/</span>src<span style=color:#719e07>/</span>ATen<span style=color:#719e07>/</span>native<span style=color:#719e07>/</span>TensorAdvancedIndexing.cpp)
</span></span><span style=display:flex><span>				   \ at<span style=color:#719e07>::</span>_index_put_impl_
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ç»“è®ºæ˜¯æŒ‰ç…§ dispatch: yy: xx å­—æ®µç”Ÿæˆ at<span style=color:#719e07>::</span>xx å‡½æ•°ï¼Œæœ€ç»ˆè°ƒç”¨åˆ° at<span style=color:#719e07>::</span>native<span style=color:#719e07>::</span>xx å‡½æ•°ã€‚
</span></span><span style=display:flex><span>                                                       <span style=color:#719e07>^^^^^^^^^^^^^^</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>å¦‚æœæ˜¯ structured: Trueï¼Œå°±å¯¹åº” at<span style=color:#719e07>::</span>native<span style=color:#719e07>::</span>structured_xx<span style=color:#719e07>::</span>implï¼Œåœ¨ä»£ç ä¸­é€šå¸¸ä¸º
</span></span><span style=display:flex><span>at<span style=color:#719e07>::</span>native åå­—ç©ºé—´ä¸‹çš„ TORCH_IMPL_FUNC(xx)ã€‚
</span></span><span style=display:flex><span>                      <span style=color:#719e07>^^^^^^^^^^^^^^^^^^^</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>å¦‚æœæ˜¯ structured_delegate: zzï¼Œå¯èƒ½å¾—å»æ‰¾ zz çš„å®šä¹‰ã€‚
</span></span></code></pre></div><p>è‡³äºä¸ºä»€ä¹ˆæœ‰æ—¶å€™ä¼šç”Ÿæˆ at::cuda ä¸‹çš„å‡½æ•°ï¼Œæœ‰æ—¶å€™ä¸ä¼šï¼Œè¿™ä¸ªä¸æ¸…æ¥šã€‚</p><h1 id=ç”Ÿæˆä»£ç >ç”Ÿæˆä»£ç 
<a class=header-anchor href=#%e7%94%9f%e6%88%90%e4%bb%a3%e7%a0%81></a></h1><p>å¯ä»¥ç”¨ <code>python3 -m torchgen.gen</code> æ¥ç”Ÿæˆä¸€éƒ¨åˆ†ä»£ç ï¼Œè¿™ä¸ªè¿‡ç¨‹æ¯”è¾ƒå¿«ï¼Œä¹Ÿä¸ç”¨å‡†å¤‡å¥½å…¨å¥—çš„æ„å»ºç¯å¢ƒã€‚æ¥ä¸‹æ¥ä»¥ <code>index_put_</code> ä¸ºä¾‹æ¥çœ‹ç”Ÿæˆç»“æœã€‚</p><h1 id=é-structured-ç®—å­ä»¥-index_put_-ä¸ºä¾‹>é <code>structured</code> ç®—å­ï¼šä»¥ <code>index_put_</code> ä¸ºä¾‹
<a class=header-anchor href=#%e9%9d%9e-structured-%e7%ae%97%e5%ad%90%e4%bb%a5-index_put_-%e4%b8%ba%e4%be%8b></a></h1><h2 id=è¿½è¸ª-index_put_-çš„æ´¾å‘>è¿½è¸ª <code>index_put_</code> çš„æ´¾å‘
<a class=header-anchor href=#%e8%bf%bd%e8%b8%aa-index_put_-%e7%9a%84%e6%b4%be%e5%8f%91></a></h2><p>åœ¨ native_functions.yaml ä¸­ï¼š</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#719e07>-</span> func: index_put_(Tensor(a<span style=color:#719e07>!</span>) self, Tensor<span style=color:#719e07>?</span>[] indices, Tensor values, <span style=color:#dc322f>bool</span> accumulate<span style=color:#719e07>=</span>False) <span style=color:#719e07>-&gt;</span> Tensor(a<span style=color:#719e07>!</span>)
</span></span><span style=display:flex><span>  device_check: NoCheck   # delegate to _index_put_impl_, which leverages TensorIterator
</span></span><span style=display:flex><span>  variants: function, method
</span></span><span style=display:flex><span>  dispatch:
</span></span><span style=display:flex><span>    CompositeExplicitAutograd: index_put_
</span></span><span style=display:flex><span>  autogen: index_put.out
</span></span><span style=display:flex><span>  <span style=color:#719e07># NB: The following functions are declared in aten/src/ATen/templates/TensorBody.h and defined in aten/src/ATen/TensorIndexing.cpp:
</span></span></span><span style=display:flex><span><span style=color:#719e07></span>  <span style=color:#719e07># - Tensor &amp; Tensor::index_put_(ArrayRef&lt;TensorIndex&gt; indices, Tensor const &amp; rhs)
</span></span></span><span style=display:flex><span><span style=color:#719e07></span>  <span style=color:#719e07># - Tensor &amp; Tensor::index_put_(ArrayRef&lt;TensorIndex&gt; indices, Scalar v)
</span></span></span><span style=display:flex><span><span style=color:#719e07></span>  <span style=color:#719e07># - Tensor &amp; Tensor::index_put_(std::initializer_list&lt;TensorIndex&gt; indices, Tensor const &amp; rhs)
</span></span></span><span style=display:flex><span><span style=color:#719e07></span>  <span style=color:#719e07># - Tensor &amp; Tensor::index_put_(std::initializer_list&lt;TensorIndex&gt; indices, Scalar v)
</span></span></span></code></pre></div><p>é¦–å…ˆæ˜¯ä¼šæœ‰ä¸ª <code>TORCH_LIBRARY_IMPL</code>ï¼Œç„¶åé‡Œé¢ä¼šæœ‰ <code>m.impl("åç§°")</code>ï¼Œå¯ä»¥æŒ‰ç…§ <code>m.impl</code> æ¥æœç´¢ã€‚</p><p>åœ¨ build/aten/src/ATen/RegisterCompositeExplicitAutogradEverything.cpp ä¸­ï¼ˆåœ¨ torch/library.h ä¸­æœ‰ <code>torch::Library</code> ç±»å‹ï¼Œè¯¥ç±»å‹æœ‰ä¸‹é¢çœ‹åˆ°çš„ <code>impl</code> æ–¹æ³•ï¼Œè¿˜æœ‰å…¶ä»–åœ°æ–¹å¸¸çœ‹åˆ°çš„ <code>def</code> æ–¹æ³•ï¼‰ï¼š</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#719e07>namespace</span> {
</span></span><span style=display:flex><span>at<span style=color:#719e07>::</span>Tensor wrapper_CompositeExplicitAutograd__index_put(<span style=color:#719e07>const</span> at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> self, <span style=color:#719e07>const</span> c10<span style=color:#719e07>::</span>List<span style=color:#719e07>&lt;::</span>std<span style=color:#719e07>::</span>optional<span style=color:#719e07>&lt;</span>at<span style=color:#719e07>::</span>Tensor<span style=color:#719e07>&gt;&gt;</span> <span style=color:#719e07>&amp;</span> indices, <span style=color:#719e07>const</span> at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> values, <span style=color:#dc322f>bool</span> accumulate) {
</span></span><span style=display:flex><span>    <span style=color:#586e75>// No device check
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>  <span style=color:#586e75>// DeviceGuard omitted
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>  <span style=color:#719e07>return</span> at<span style=color:#719e07>::</span>native<span style=color:#719e07>::</span>index_put(self, indices, values, accumulate);
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>} <span style=color:#586e75>// anonymous namespace
</span></span></span><span style=display:flex><span><span style=color:#586e75></span><span style=color:#719e07>namespace</span> {
</span></span><span style=display:flex><span>at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> wrapper_CompositeExplicitAutograd_out_index_put_out(<span style=color:#719e07>const</span> at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> self, <span style=color:#719e07>const</span> c10<span style=color:#719e07>::</span>List<span style=color:#719e07>&lt;::</span>std<span style=color:#719e07>::</span>optional<span style=color:#719e07>&lt;</span>at<span style=color:#719e07>::</span>Tensor<span style=color:#719e07>&gt;&gt;</span> <span style=color:#719e07>&amp;</span> indices, <span style=color:#719e07>const</span> at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> values, <span style=color:#dc322f>bool</span> accumulate, at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> out) {
</span></span><span style=display:flex><span>    <span style=color:#586e75>// No device check
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>  <span style=color:#586e75>// DeviceGuard omitted
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>  <span style=color:#719e07>return</span> at<span style=color:#719e07>::</span>native<span style=color:#719e07>::</span>index_put_out(self, indices, values, accumulate, out);
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>} <span style=color:#586e75>// anonymous namespace
</span></span></span><span style=display:flex><span><span style=color:#586e75></span><span style=color:#719e07>namespace</span> {
</span></span><span style=display:flex><span>at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> wrapper_CompositeExplicitAutograd__index_put_(at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> self, <span style=color:#719e07>const</span> c10<span style=color:#719e07>::</span>List<span style=color:#719e07>&lt;::</span>std<span style=color:#719e07>::</span>optional<span style=color:#719e07>&lt;</span>at<span style=color:#719e07>::</span>Tensor<span style=color:#719e07>&gt;&gt;</span> <span style=color:#719e07>&amp;</span> indices, <span style=color:#719e07>const</span> at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> values, <span style=color:#dc322f>bool</span> accumulate) {
</span></span><span style=display:flex><span>    <span style=color:#586e75>// No device check
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>  <span style=color:#586e75>// DeviceGuard omitted
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>  <span style=color:#719e07>return</span> at<span style=color:#719e07>::</span>native<span style=color:#719e07>::</span>index_put_(self, indices, values, accumulate);
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>} <span style=color:#586e75>// anonymous namespace
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>TORCH_LIBRARY_IMPL(aten, CompositeExplicitAutograd, m) {
</span></span><span style=display:flex><span>    m.impl(<span style=color:#2aa198>&#34;index_put&#34;</span>,
</span></span><span style=display:flex><span>TORCH_FN(wrapper_CompositeExplicitAutograd__index_put));
</span></span><span style=display:flex><span>m.impl(<span style=color:#2aa198>&#34;index_put.out&#34;</span>,
</span></span><span style=display:flex><span>TORCH_FN(wrapper_CompositeExplicitAutograd_out_index_put_out));
</span></span><span style=display:flex><span>m.impl(<span style=color:#2aa198>&#34;index_put_&#34;</span>,
</span></span><span style=display:flex><span>TORCH_FN(wrapper_CompositeExplicitAutograd__index_put_));
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>ä»è¿™é‡Œæˆ‘ä»¬å‘ç°ä¸€ä¸ªè§„å¾‹ï¼šå¾ˆå¤šåœ¨ native_functions.yaml ä¸­æ³¨å†Œçš„å‡½æ•°ä¼šè½¬è€Œè°ƒç”¨ <code>at::native::</code> ä¸­çš„å‡½æ•°ï¼Œè¿™äº›å‡½æ•°æ˜¯åœ¨ aten/src/ATen/native/ æ–‡ä»¶å¤¹ä¸‹æœ‰å®šä¹‰çš„ï¼Œæ¯”å¦‚ <code>at::native::index_put</code> å’Œ <code>at::native::index_put_</code> ã€‚ä¹Ÿæœ‰ä¸€äº›å‡½æ•°æ¯”å¦‚ <code>at::native::index_put_out</code> æ˜¯ç”¨ä»£ç ç”Ÿæˆçš„ï¼Œå¦‚ native_functions.yaml ä¸­ <code>index_put_</code> å‡½æ•°çš„ autogen å­—æ®µæ‰€è¿°ã€‚</p><p>æˆ‘ä»¬å…ˆè®°ä½ <code>m.impl</code> èƒ½è®°å½•å‡½æ•°æ´¾å‘ï¼Œæš‚æ—¶ä¸å±•å¼€ã€‚</p><h2 id=è¿½è¸ª-index_putout-çš„-autogen>è¿½è¸ª <code>index_put.out</code> çš„ autogen
<a class=header-anchor href=#%e8%bf%bd%e8%b8%aa-index_putout-%e7%9a%84-autogen></a></h2><p>å‰é¢æˆ‘ä»¬å·²ç»çœ‹åˆ°äº† <code>index_put</code> å¯¹åº” <code>at::native::index_put</code>ï¼Œ<code>index_put_</code> å¯¹åº” <code>at::native::index_put_</code>ï¼Œè€Œä¸”éƒ½èƒ½åœ¨ aten/src/ATen/native/TensorAdvancedIndexing.cpp ä¸­æ‰¾åˆ°å®šä¹‰ï¼Œä½†æ˜¯ <code>at::native::index_put_out</code> æ˜¯æ‰¾ä¸åˆ°å®šä¹‰çš„ã€‚<strong>è¿™ä¸€ä¸ªå°èŠ‚ç”¨æ¥é˜…è¯» <code>autogen</code> å­—æ®µè¯·æ±‚ç”Ÿæˆçš„ä»£ç ã€‚</strong></p><p>ä»£ç ç”Ÿæˆåï¼Œåœ¨ build/aten/src/ATen/CompositeViewCopyKernels.cpp ä¸­ï¼ˆå’Œ <code>autogen: index_put.out</code> å¯¹åº”ï¼‰ï¼š</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#586e75>// namespace at::native
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>
</span></span><span style=display:flex><span>at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> index_put_out(<span style=color:#719e07>const</span> at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> self, <span style=color:#719e07>const</span> c10<span style=color:#719e07>::</span>List<span style=color:#719e07>&lt;::</span>std<span style=color:#719e07>::</span>optional<span style=color:#719e07>&lt;</span>at<span style=color:#719e07>::</span>Tensor<span style=color:#719e07>&gt;&gt;</span> <span style=color:#719e07>&amp;</span> indices, <span style=color:#719e07>const</span> at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> values, <span style=color:#dc322f>bool</span> accumulate, at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> out) {
</span></span><span style=display:flex><span>  <span style=color:#719e07>auto</span> tmp_output <span style=color:#719e07>=</span> at<span style=color:#719e07>::</span>_ops<span style=color:#719e07>::</span>index_put<span style=color:#719e07>::</span>call(self, indices, values, accumulate);
</span></span><span style=display:flex><span>  resize_out_helper(out, tmp_output);
</span></span><span style=display:flex><span>  copy_arg(out, tmp_output);
</span></span><span style=display:flex><span>  <span style=color:#719e07>return</span> out;
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>åœ¨ build/aten/src/ATen/OperatorsEverything.cpp ä¸­ï¼š</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#586e75>// namespace at::_ops
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>
</span></span><span style=display:flex><span><span style=color:#586e75>// aten::index_put(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False) -&gt; Tensor
</span></span></span><span style=display:flex><span><span style=color:#586e75></span><span style=color:#719e07>static</span> C10_NOINLINE c10<span style=color:#719e07>::</span>TypedOperatorHandle<span style=color:#719e07>&lt;</span>index_put<span style=color:#719e07>::</span>schema<span style=color:#719e07>&gt;</span> create_index_put_typed_handle() {
</span></span><span style=display:flex><span>  <span style=color:#719e07>return</span> c10<span style=color:#719e07>::</span>Dispatcher<span style=color:#719e07>::</span>singleton()
</span></span><span style=display:flex><span>      .findSchemaOrThrow(index_put<span style=color:#719e07>::</span>name, index_put<span style=color:#719e07>::</span>overload_name)
</span></span><span style=display:flex><span>      .typed<span style=color:#719e07>&lt;</span>index_put<span style=color:#719e07>::</span>schema<span style=color:#719e07>&gt;</span>();
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#586e75>// âœ…
</span></span></span><span style=display:flex><span><span style=color:#586e75>// aten::index_put(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False) -&gt; Tensor
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>at<span style=color:#719e07>::</span>Tensor index_put<span style=color:#719e07>::</span>call(<span style=color:#719e07>const</span> at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> self, <span style=color:#719e07>const</span> c10<span style=color:#719e07>::</span>List<span style=color:#719e07>&lt;::</span>std<span style=color:#719e07>::</span>optional<span style=color:#719e07>&lt;</span>at<span style=color:#719e07>::</span>Tensor<span style=color:#719e07>&gt;&gt;</span> <span style=color:#719e07>&amp;</span> indices, <span style=color:#719e07>const</span> at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> values, <span style=color:#dc322f>bool</span> accumulate) {
</span></span><span style=display:flex><span>    <span style=color:#719e07>static</span> <span style=color:#719e07>auto</span> op <span style=color:#719e07>=</span> create_index_put_typed_handle();
</span></span><span style=display:flex><span>    <span style=color:#719e07>return</span> op.call(self, indices, values, accumulate);
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#586e75>// aten::index_put(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False) -&gt; Tensor
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>at<span style=color:#719e07>::</span>Tensor index_put<span style=color:#719e07>::</span>redispatch(c10<span style=color:#719e07>::</span>DispatchKeySet dispatchKeySet, <span style=color:#719e07>const</span> at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> self, <span style=color:#719e07>const</span> c10<span style=color:#719e07>::</span>List<span style=color:#719e07>&lt;::</span>std<span style=color:#719e07>::</span>optional<span style=color:#719e07>&lt;</span>at<span style=color:#719e07>::</span>Tensor<span style=color:#719e07>&gt;&gt;</span> <span style=color:#719e07>&amp;</span> indices, <span style=color:#719e07>const</span> at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> values, <span style=color:#dc322f>bool</span> accumulate) {
</span></span><span style=display:flex><span>    <span style=color:#719e07>static</span> <span style=color:#719e07>auto</span> op <span style=color:#719e07>=</span> create_index_put_typed_handle();
</span></span><span style=display:flex><span>    <span style=color:#719e07>return</span> op.redispatch(dispatchKeySet, self, indices, values, accumulate);
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>è¿™ä¸ª <code>findSchemaOrThrow</code> å‡½æ•°åœ¨ aten/src/ATen/core/dispatch/Dispatcher.cpp ä¸­å®šä¹‰ã€‚aten/src/ATen/core/dispatch/Dispatcher.h åˆ™æœ‰å¯¹å‡½æ•°æ¥å£è¾ƒä¸ºè¯¦ç»†çš„è¯´æ˜ã€‚å…·ä½“æ¥è¯´ä¼šä»è‡ªå·±çš„ table æ¥æŸ¥æ‰¾å­—ç¬¦ä¸²åï¼Œçœ‹çœ‹æœ‰æ²¡æœ‰ç»“æœï¼Œå…·ä½“æ¥è¯´æ˜¯ <code>operatorLookupTable_.read</code>ã€‚</p><p>è¿™é‡Œè¿˜ç”¨åˆ°äº† <code>index_put</code>ï¼Œå¯ä»¥æœç´¢ <code>struct TORCH_API index_put</code>ï¼š</p><p><img alt=image.png src=/posts/programming/python/assets/struct%20TORCH_API%20index_put.webp width=400></p><p>å®šä¹‰ä¸ºï¼š</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#719e07>struct</span> <span style=color:#268bd2>TORCH_API</span> index_put {
</span></span><span style=display:flex><span>  <span style=color:#719e07>using</span> schema <span style=color:#719e07>=</span> at<span style=color:#719e07>::</span>Tensor (<span style=color:#719e07>const</span> at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span>, <span style=color:#719e07>const</span> c10<span style=color:#719e07>::</span>List<span style=color:#719e07>&lt;::</span>std<span style=color:#719e07>::</span>optional<span style=color:#719e07>&lt;</span>at<span style=color:#719e07>::</span>Tensor<span style=color:#719e07>&gt;&gt;</span> <span style=color:#719e07>&amp;</span>, <span style=color:#719e07>const</span> at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span>, <span style=color:#dc322f>bool</span>);
</span></span><span style=display:flex><span>  <span style=color:#719e07>using</span> ptr_schema <span style=color:#719e07>=</span> schema<span style=color:#719e07>*</span>;
</span></span><span style=display:flex><span>  <span style=color:#586e75>// See Note [static constexpr char* members for windows NVCC]
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>  <span style=color:#719e07>static</span> <span style=color:#719e07>constexpr</span> <span style=color:#719e07>const</span> <span style=color:#dc322f>char</span><span style=color:#719e07>*</span> name <span style=color:#719e07>=</span> <span style=color:#2aa198>&#34;aten::index_put&#34;</span>;
</span></span><span style=display:flex><span>  <span style=color:#719e07>static</span> <span style=color:#719e07>constexpr</span> <span style=color:#719e07>const</span> <span style=color:#dc322f>char</span><span style=color:#719e07>*</span> overload_name <span style=color:#719e07>=</span> <span style=color:#2aa198>&#34;&#34;</span>;
</span></span><span style=display:flex><span>  <span style=color:#719e07>static</span> <span style=color:#719e07>constexpr</span> <span style=color:#719e07>const</span> <span style=color:#dc322f>char</span><span style=color:#719e07>*</span> schema_str <span style=color:#719e07>=</span> <span style=color:#2aa198>&#34;index_put(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False) -&gt; Tensor&#34;</span>;
</span></span><span style=display:flex><span>  <span style=color:#719e07>static</span> at<span style=color:#719e07>::</span>Tensor call(<span style=color:#719e07>const</span> at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> self, <span style=color:#719e07>const</span> c10<span style=color:#719e07>::</span>List<span style=color:#719e07>&lt;::</span>std<span style=color:#719e07>::</span>optional<span style=color:#719e07>&lt;</span>at<span style=color:#719e07>::</span>Tensor<span style=color:#719e07>&gt;&gt;</span> <span style=color:#719e07>&amp;</span> indices, <span style=color:#719e07>const</span> at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> values, <span style=color:#dc322f>bool</span> accumulate);
</span></span><span style=display:flex><span>  <span style=color:#719e07>static</span> at<span style=color:#719e07>::</span>Tensor redispatch(c10<span style=color:#719e07>::</span>DispatchKeySet dispatchKeySet, <span style=color:#719e07>const</span> at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> self, <span style=color:#719e07>const</span> c10<span style=color:#719e07>::</span>List<span style=color:#719e07>&lt;::</span>std<span style=color:#719e07>::</span>optional<span style=color:#719e07>&lt;</span>at<span style=color:#719e07>::</span>Tensor<span style=color:#719e07>&gt;&gt;</span> <span style=color:#719e07>&amp;</span> indices, <span style=color:#719e07>const</span> at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> values, <span style=color:#dc322f>bool</span> accumulate);
</span></span><span style=display:flex><span>};
</span></span></code></pre></div><p>æ€»ä½“æ¥è¯´è°ƒç”¨é“¾æ˜¯ï¼š</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span>m.impl(<span style=color:#2aa198>&#34;index_put.out&#34;</span>, ...) <span style=color:#586e75>// build/aten/src/ATen/RegisterCompositeExplicitAutogradEverything.cpp
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>	at<span style=color:#719e07>::</span>native<span style=color:#719e07>::</span>index_put_out
</span></span><span style=display:flex><span>		at<span style=color:#719e07>::</span>_ops<span style=color:#719e07>::</span>index_put<span style=color:#719e07>::</span>call
</span></span><span style=display:flex><span>			...
</span></span></code></pre></div><p>æ‰€ä»¥è¯´ <code>at::native::index_put_out</code> å®é™…ä¸Šä¼šæ´¾å‘ç»™ <code>at::native::index_put</code>ã€‚</p><p><em>æ˜¯è°æŠŠç®—å­å†™è¿›æŸ¥æ‰¾è¡¨çš„å‘¢ï¼Ÿ</em></p><p>ç›¸å¯¹åœ°ï¼Œ<code>Dispatcher::findOrRegisterName_</code> ä¼šè°ƒç”¨ <code>operatorLookupTable_.write</code> ï¼Œè¿™æ ·å°±èƒ½å°†ç®—å­æ³¨å†Œè¿›å»ã€‚è¿™ä¸ªå‡½æ•°è¢« <code>register{Name,Impl,Def}</code> è°ƒç”¨ï¼Œå¯ä»¥æŒ‰ç…§æ‰©å±•æ­£åˆ™è¡¨è¾¾å¼æœç´¢ <code>\.register(Name|Impl|Def)\(</code>ã€‚ç„¶åå‘ç° <code>RegisterOperators::registerOp_</code> åœ¨è°ƒç”¨ <code>registerDef</code> å’Œ <code>registerImpl</code>ã€‚</p><p>é“¾è·¯ï¼š</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#719e07>explicit</span> <span style=color:#268bd2>RegisterOperators</span>(<span style=color:#719e07>const</span> std<span style=color:#719e07>::</span>string<span style=color:#719e07>&amp;</span>, FuncType<span style=color:#719e07>&amp;&amp;</span>, Options<span style=color:#719e07>&amp;&amp;</span>) <span style=color:#586e75>// ï¼ˆå¯é€‰ï¼‰
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>	c10<span style=color:#719e07>::</span>RegisterOperators<span style=color:#719e07>::</span>op
</span></span><span style=display:flex><span>		c10<span style=color:#719e07>::</span>RegisterOperators<span style=color:#719e07>::</span>checkSchemaAndRegisterOp_
</span></span><span style=display:flex><span>			c10<span style=color:#719e07>::</span>RegisterOperators<span style=color:#719e07>::</span>registerOp_ <span style=color:#586e75>// ä¼šè·å– Dispatcher çš„å•ä¾‹ï¼Œå¾€é‡Œé¢åŠ ä¸œè¥¿
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>				c10<span style=color:#719e07>::</span>Dispatcher<span style=color:#719e07>::</span>singleton().<span style=color:#719e07>register</span>{Name,Def,Impl}
</span></span><span style=display:flex><span>					c10<span style=color:#719e07>::</span>Dispatcher<span style=color:#719e07>::</span>findOrRegisterName_
</span></span><span style=display:flex><span>						c10<span style=color:#719e07>::</span>Dispatcher<span style=color:#719e07>::</span>operatorLookupTable_.write
</span></span></code></pre></div><p><code>RegisterOperators</code> ä¼¼ä¹æ˜¯ä¸ªä¸´æ—¶ç±»å‹ï¼Œæ‹¿æ¥æ³¨å†Œç”¨çš„ã€‚</p><p>è¿˜æœ‰ä¸€æ¡æ˜¯ <code>Library</code> çš„è·¯å¾„ï¼š</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span>torch<span style=color:#719e07>::</span>Library<span style=color:#719e07>::</span>_{impl,def}
</span></span><span style=display:flex><span>	c10<span style=color:#719e07>::</span>RegisterOperators<span style=color:#719e07>::</span>registerOp_ <span style=color:#586e75>// ä¼šè·å– Dispatcher çš„å•ä¾‹ï¼Œå¾€é‡Œé¢åŠ ä¸œè¥¿
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>		c10<span style=color:#719e07>::</span>Dispatcher<span style=color:#719e07>::</span>singleton().<span style=color:#719e07>register</span>{Name,Def,Impl}
</span></span><span style=display:flex><span>			c10<span style=color:#719e07>::</span>Dispatcher<span style=color:#719e07>::</span>findOrRegisterName_
</span></span><span style=display:flex><span>				c10<span style=color:#719e07>::</span>Dispatcher<span style=color:#719e07>::</span>operatorLookupTable_.write
</span></span></code></pre></div><p>aten/src/ATen/core/op_registration/README.md æœ‰ç›¸å…³çš„è¯´æ˜ï¼Œå¯ä»¥çœ‹çœ‹ç†æ¸…æ€è·¯ã€‚</p><p>å›å¿†ä¸€ä¸‹ï¼Œä¹‹å‰å·²ç»æœ‰ <code>m.impl("index_put", ...)</code> äº†ï¼Œå®ƒè½¬è€Œè°ƒç”¨äº† <code>at::native::index_put</code>ï¼Œå› æ­¤è¿™ä¸ªè¡¨é¡¹å·²ç»è¢«æ³¨å†Œè¿‡äº†ã€‚ç°åœ¨å¯ä»¥æŠŠè°ƒç”¨é“¾è¡¥å……å®Œæ•´ï¼š</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#586e75>// build/aten/src/ATen/RegisterCompositeExplicitAutogradEverything.cpp
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>m.impl(<span style=color:#2aa198>&#34;index_put.out&#34;</span>, ...)
</span></span><span style=display:flex><span>	at<span style=color:#719e07>::</span>native<span style=color:#719e07>::</span>index_put_out   <span style=color:#586e75>// build/aten/src/ATen/CompositeViewCopyKernels.cpp
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>		at<span style=color:#719e07>::</span>_ops<span style=color:#719e07>::</span>index_put<span style=color:#719e07>::</span>call <span style=color:#586e75>// build/aten/src/ATen/OperatorsEverything.cpp
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>		  <span style=color:#719e07>|</span>
</span></span><span style=display:flex><span>		  <span style=color:#719e07>|</span> find op from dispatcher
</span></span><span style=display:flex><span>		  <span style=color:#719e07>|</span>
</span></span><span style=display:flex><span>			<span style=color:#586e75>// build/aten/src/ATen/RegisterCompositeExplicitAutogradEverything.cpp
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>			m.impl(<span style=color:#2aa198>&#34;index_put&#34;</span>, ...)
</span></span><span style=display:flex><span>				<span style=color:#586e75>// build/aten/src/ATen/RegisterCompositeExplicitAutogradEverything.cpp
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>				TORCH_FN(wrapper_CompositeExplicitAutograd__index_put)
</span></span><span style=display:flex><span>					<span style=color:#586e75>// aten/src/ATen/native/TensorAdvancedIndexing.cpp
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>					at<span style=color:#719e07>::</span>native<span style=color:#719e07>::</span>index_put
</span></span></code></pre></div><p>é€šè¿‡å†æ¬¡æ´¾å‘ï¼Œautogen ç”Ÿæˆçš„å‡½æ•°èƒ½è°ƒç”¨åŸå‡½æ•°ï¼Œä»è€Œå‡å°‘äº†ç¼–å†™æ¥å£çš„è´Ÿæ‹…ã€‚</p><h1 id=structured-ç®—å­ä»¥-index_out-ä¸ºä¾‹><code>structured</code> ç®—å­ï¼šä»¥ <code>index_out</code> ä¸ºä¾‹
<a class=header-anchor href=#structured-%e7%ae%97%e5%ad%90%e4%bb%a5-index_out-%e4%b8%ba%e4%be%8b></a></h1><p><code>at::cuda::index_out</code> è¿™ä¸ªå‡½æ•°è¢« <code>at::native::masked_select_out_cuda_impl</code> è°ƒç”¨äº†ï¼Œæˆ‘ä¹‹å‰ä¹Ÿä¸€ç›´ç–‘æƒ‘ <code>masked_select</code> æ˜¯åœ¨å“ªé‡Œè°ƒç”¨ <code>nonzero</code> çš„ï¼Œç°åœ¨å°±é€šè¿‡è¿™ä¸ªä¾‹å­æ¥æŸ¥æ‰¾è°ƒç”¨è·¯å¾„ã€‚æˆ‘ä»¬å…ˆæ¥çœ‹ <code>at::cuda::index_out</code> å‡½æ•°çš„å®ç°ï¼Œæ¥ç€çœ‹ <code>m.impl</code> æ˜¯æ€ä¹ˆå®ç°çš„ã€‚</p><p>åœ¨ native_functions.yaml ä¸­æ‰¾åˆ°çš„æœ€ç›¸å…³çš„è®°å½•ä¸ºï¼š</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#719e07>-</span> func: index.Tensor_out(Tensor self, Tensor<span style=color:#719e07>?</span>[] indices, <span style=color:#719e07>*</span>, Tensor(a<span style=color:#719e07>!</span>) out) <span style=color:#719e07>-&gt;</span> Tensor(a<span style=color:#719e07>!</span>)
</span></span><span style=display:flex><span>  device_check: NoCheck
</span></span><span style=display:flex><span>  structured: True
</span></span><span style=display:flex><span>  structured_inherits: TensorIteratorBase
</span></span><span style=display:flex><span>  precomputed:
</span></span><span style=display:flex><span>  <span style=color:#719e07>-</span> indices <span style=color:#719e07>-&gt;</span> DimVector sizes, DimVector strides
</span></span><span style=display:flex><span>  dispatch:
</span></span><span style=display:flex><span>    CPU, CUDA, MPS: index_out
</span></span></code></pre></div><p>æ³¨æ„è¿™ä¸ª <code>structured</code> æ ‡è®°ã€‚å¾ˆå¤šç®—å­åœ¨ native_functions.yaml ä¸­éƒ½ä¼šæœ‰ <code>structured</code> æ ‡è®°ï¼Œæˆ–è€… <code>structured_delegate</code> æ ‡è®°ã€‚åœ¨ä»£ç ç”Ÿæˆä¹‹åï¼Œç›¸å…³å£°æ˜ä¹Ÿä¼šç”Ÿæˆåœ¨ build/aten/src/ATen/NativeFunctions.h æ–‡ä»¶ä¸­ã€‚æŒ‰æˆ‘ç†è§£ï¼Œ<code>structured</code> æŠŠç®—å­çš„è¿ç®—æµç¨‹åˆ†ç¦»æˆå‡ æ­¥ï¼Œå…è®¸å¼€å‘è€…å¯¹æ¯ä¸ªæ–¹æ³•åˆ†åˆ«å®šä¹‰ã€‚</p><h2 id=ç”Ÿæˆçš„ä»£ç >ç”Ÿæˆçš„ä»£ç 
<a class=header-anchor href=#%e7%94%9f%e6%88%90%e7%9a%84%e4%bb%a3%e7%a0%81></a></h2><p>åœ¨ build/aten/src/ATen/RegisterCUDAEverything.cpp ä¸­ï¼š</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#719e07>namespace</span> at {
</span></span><span style=display:flex><span>at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> wrapper_CUDA_index_out_Tensor_out(<span style=color:#719e07>const</span> at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> self, <span style=color:#719e07>const</span> c10<span style=color:#719e07>::</span>List<span style=color:#719e07>&lt;::</span>std<span style=color:#719e07>::</span>optional<span style=color:#719e07>&lt;</span>at<span style=color:#719e07>::</span>Tensor<span style=color:#719e07>&gt;&gt;</span> <span style=color:#719e07>&amp;</span> indices, at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> out) {
</span></span><span style=display:flex><span>  <span style=color:#586e75>// No device check
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>structured_index_out_out <span style=color:#268bd2>op</span>(out);
</span></span><span style=display:flex><span><span style=color:#719e07>auto</span> precompute <span style=color:#719e07>=</span> op.meta(self, at<span style=color:#719e07>::</span>IOptTensorListRef(indices));
</span></span><span style=display:flex><span>(<span style=color:#dc322f>void</span>)precompute;
</span></span><span style=display:flex><span>op.impl(self, precompute.sizes, precompute.strides, op.maybe_get_output(<span style=color:#2aa198>0</span>));
</span></span><span style=display:flex><span><span style=color:#719e07>if</span> (op.proxy_outputs_[<span style=color:#2aa198>0</span>].has_value()) op.outputs_[<span style=color:#2aa198>0</span>].get().copy_(<span style=color:#719e07>*</span>op.proxy_outputs_[<span style=color:#2aa198>0</span>]);
</span></span><span style=display:flex><span><span style=color:#719e07>return</span> out;
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#719e07>namespace</span> cuda {
</span></span><span style=display:flex><span>at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> index_out(at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> out, <span style=color:#719e07>const</span> at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> self, <span style=color:#719e07>const</span> c10<span style=color:#719e07>::</span>List<span style=color:#719e07>&lt;::</span>std<span style=color:#719e07>::</span>optional<span style=color:#719e07>&lt;</span>at<span style=color:#719e07>::</span>Tensor<span style=color:#719e07>&gt;&gt;</span> <span style=color:#719e07>&amp;</span> indices) {
</span></span><span style=display:flex><span><span style=color:#719e07>return</span> <span style=color:#268bd2>wrapper_CUDA_index_out_Tensor_out</span>(self, indices, out);
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>æœ€ä¸»è¦çš„æ˜¯ <code>structured_index_out_out</code> è¿™ä¸ªç±»ï¼Œè¿˜æœ‰ <code>meta</code> å’Œ <code>impl</code> ä¸¤ä¸ªæ–¹æ³•ã€‚</p><h2 id=impl><code>impl</code>
<a class=header-anchor href=#impl></a></h2><p>å…ˆçœ‹å®šä¹‰ï¼ˆ<code>impl</code>ï¼‰ã€‚åœ¨ aten/src/ATen/native/TensorAdvancedIndexing.cpp ä¸­ï¼š</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span>TORCH_IMPL_FUNC(index_out)
</span></span><span style=display:flex><span>(<span style=color:#719e07>const</span> Tensor<span style=color:#719e07>&amp;</span> self, DimVector sizes, DimVector strides, <span style=color:#719e07>const</span> Tensor<span style=color:#719e07>&amp;</span> result) {
</span></span><span style=display:flex><span>  index_stub(device_type(), <span style=color:#719e07>*</span><span style=color:#719e07>this</span>, sizes, strides);
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>è€Œ <code>TORCH_IMPL_FUNC</code> å®šä¹‰å¦‚ä¸‹ï¼š</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#719e07>#define TORCH_IMPL_FUNC(name) void structured_##name::impl
</span></span></span></code></pre></div><p>å› æ­¤ä¼šå±•å¼€æˆè¿™æ ·ï¼ˆå› ä¸ºæ˜¯å®å±•å¼€ï¼Œä»£ç é‡Œé¢æœä¸åˆ°ï¼‰ï¼š</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#dc322f>void</span> structured_index_out<span style=color:#719e07>::</span>impl
</span></span><span style=display:flex><span>(<span style=color:#719e07>const</span> Tensor<span style=color:#719e07>&amp;</span> self, DimVector sizes, DimVector strides, <span style=color:#719e07>const</span> Tensor<span style=color:#719e07>&amp;</span> result) {
</span></span><span style=display:flex><span>  index_stub(device_type(), <span style=color:#719e07>*</span><span style=color:#719e07>this</span>, sizes, strides);
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>å¯¹åº”çš„å£°æ˜æ˜¯ä»£ç ç”Ÿæˆçš„ï¼Œåœ¨ build/aten/src/ATen/NativeFunctions.h ä¸­ï¼š</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#719e07>struct</span> <span style=color:#268bd2>TORCH_API</span> structured_index_out : <span style=color:#719e07>public</span> at<span style=color:#719e07>::</span>meta<span style=color:#719e07>::</span>structured_index_Tensor {
</span></span><span style=display:flex><span><span style=color:#dc322f>void</span> <span style=color:#268bd2>impl</span>(<span style=color:#719e07>const</span> at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> self, at<span style=color:#719e07>::</span>DimVector sizes, at<span style=color:#719e07>::</span>DimVector strides, <span style=color:#719e07>const</span> at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> out);
</span></span><span style=display:flex><span>};
</span></span></code></pre></div><h2 id=meta><code>meta</code>
<a class=header-anchor href=#meta></a></h2><p>æ¥ç€çœ‹é¢„å¤„ç†ï¼ˆ<code>meta</code>ï¼‰ã€‚åœ¨ aten/src/ATen/TensorMeta.h ä¸­ï¼š</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#586e75>// ...
</span></span></span><span style=display:flex><span><span style=color:#586e75></span><span style=color:#719e07>#define TORCH_PRECOMPUTE_META_FUNC(name) \
</span></span></span><span style=display:flex><span><span style=color:#719e07>  structured_##name::meta_return_ty structured_##name::meta
</span></span></span><span style=display:flex><span><span style=color:#719e07>#define TORCH_PRECOMPUTE_META_FUNC2(name, overload) \
</span></span></span><span style=display:flex><span><span style=color:#719e07>  structured_##name##_##overload::meta_return_ty    \
</span></span></span><span style=display:flex><span><span style=color:#719e07>      structured_##name##_##overload::meta
</span></span></span><span style=display:flex><span><span style=color:#719e07></span>
</span></span><span style=display:flex><span><span style=color:#586e75>// Use this to create a precompute struct in a meta function.
</span></span></span><span style=display:flex><span><span style=color:#586e75></span><span style=color:#719e07>#define TORCH_PRECOMPUTE_STRUCT(name) structured_##name::precompute_out&lt;&gt;
</span></span></span><span style=display:flex><span><span style=color:#719e07>#define TORCH_PRECOMPUTE_STRUCT2(name, overload) \
</span></span></span><span style=display:flex><span><span style=color:#719e07>  structured_##name##_##overload::precompute_out&lt;&gt;
</span></span></span></code></pre></div><p>åœ¨ aten/src/ATen/native/TensorAdvancedIndexing.cpp ä¸­ï¼š</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span>TORCH_PRECOMPUTE_META_FUNC2(index, Tensor)
</span></span><span style=display:flex><span>(<span style=color:#719e07>const</span> Tensor<span style=color:#719e07>&amp;</span> self, at<span style=color:#719e07>::</span>IOptTensorListRef indices) {
</span></span><span style=display:flex><span>  <span style=color:#586e75>// ...
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>  <span style=color:#719e07>auto</span> info <span style=color:#719e07>=</span> at<span style=color:#719e07>::</span>native<span style=color:#719e07>::</span>make_info(self, std<span style=color:#719e07>::</span>move(indices));
</span></span><span style=display:flex><span>  build_index_op(<span style=color:#719e07>*</span><span style=color:#719e07>this</span>, info, result);
</span></span><span style=display:flex><span>  <span style=color:#719e07>return</span> <span style=color:#268bd2>TORCH_PRECOMPUTE_STRUCT2</span>(index, Tensor)()
</span></span><span style=display:flex><span>      .set_sizes(std<span style=color:#719e07>::</span>move(info.indexed_sizes))
</span></span><span style=display:flex><span>      .set_strides(std<span style=color:#719e07>::</span>move(info.indexed_strides));
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>å› æ­¤ <code>structured_index_Tensor</code> çš„ <code>meta</code> å‡½æ•°èƒ½å¯¹ indices è¿›è¡Œé¢„è®¡ç®—ï¼Œåœ¨ <code>make_info</code> ä¸­å°† BoolTensor è½¬æ¢æˆ LongTensorï¼ˆä»£ç ç•¥ï¼‰ï¼Œå¹¶è·å– sizesã€strides ä¿¡æ¯ã€‚</p><h2 id=æ”¾åœ¨ä¸€èµ·çœ‹>æ”¾åœ¨ä¸€èµ·çœ‹
<a class=header-anchor href=#%e6%94%be%e5%9c%a8%e4%b8%80%e8%b5%b7%e7%9c%8b></a></h2><p>æˆ‘ä»¬æ‰¾åˆ°çš„ impl æ˜¯ <code>structured_index_out::impl</code>ï¼Œæ‰¾åˆ°çš„ meta æ˜¯ <code>structured_index_Tensor::meta</code>ï¼Œè¿˜è¦å°†ä»–ä»¬å’Œ <code>structured_index_out_out</code> è”ç³»èµ·æ¥ã€‚</p><p>åœ¨ build/aten/src/ATen/RegisterCUDAEverything.cpp ä¸­ï¼š</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#719e07>struct</span> <span style=color:#268bd2>structured_index_out_out</span> <span style=color:#719e07>final</span> <span style=color:#719e07>:</span> <span style=color:#719e07>public</span> at<span style=color:#719e07>::</span>native<span style=color:#719e07>::</span>structured_index_out
</span></span></code></pre></div><p>å› ä¸ºç»§æ‰¿å…³ç³»ï¼Œ<code>structured_index_out_out</code> è‡ªç„¶å¾—åˆ°äº† <code>structured_index_out::impl</code>ã€‚</p><p>åœ¨ build/aten/src/ATen/NativeFunctions.h ä¸­ï¼š</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#719e07>struct</span> <span style=color:#268bd2>TORCH_API</span> structured_index_out : <span style=color:#719e07>public</span> at<span style=color:#719e07>::</span>meta<span style=color:#719e07>::</span>structured_index_Tensor
</span></span></code></pre></div><p>å› ä¸ºç»§æ‰¿å…³ç³»ï¼Œ<code>structured_index_out</code> åˆå¾—åˆ°äº† <code>structured_index_Tensor::meta</code>ã€‚</p><p>ç»§æ‰¿é“¾ä¸ºï¼š</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#719e07>-</span> at<span style=color:#719e07>::</span>TensorIteratorBase
</span></span><span style=display:flex><span>  <span style=color:#719e07>-</span> at<span style=color:#719e07>::</span>meta<span style=color:#719e07>::</span>structured_index_Tensor (provides meta)
</span></span><span style=display:flex><span>	  <span style=color:#719e07>-</span> at<span style=color:#719e07>::</span>native<span style=color:#719e07>::</span>structured_index_out (provides impl)
</span></span><span style=display:flex><span>		  <span style=color:#719e07>-</span> at<span style=color:#719e07>::</span>native<span style=color:#719e07>::</span>structured_index_out_out
</span></span></code></pre></div><p>æ‰€ä»¥ masked_select è°ƒç”¨äº† index_outï¼Œåè€…æŒ‰é¡ºåºè°ƒç”¨ structured_index_out_out çš„ meta å’Œ impl æ–¹æ³•ã€‚meta æ–¹æ³•æ¥è‡ª at::meta::structured_index_Tensor::metaï¼Œå†…éƒ¨ä¼šè°ƒç”¨ expandTensors å‡½æ•°å°† mask è½¬æ¢æˆ LongTensor ä¸‹æ ‡ï¼ˆä»£ç ç•¥ï¼‰ã€‚impl æ–¹æ³•æ¥è‡ª at::native::structured_index_out::implï¼Œä¼šè°ƒç”¨ index_stubã€‚</p><h2 id=å†æ¥çœ‹-mimplat-åå­—ç©ºé—´è€Œé-atcuda>å†æ¥çœ‹ <code>m.impl</code>ï¼ˆ<code>at</code> åå­—ç©ºé—´ï¼Œè€Œé <code>at::cuda</code>ï¼‰
<a class=header-anchor href=#%e5%86%8d%e6%9d%a5%e7%9c%8b-mimplat-%e5%90%8d%e5%ad%97%e7%a9%ba%e9%97%b4%e8%80%8c%e9%9d%9e-atcuda></a></h2><p>ä¹Ÿæ˜¯ç»“æ„åŒ–åœ°å…ˆ meta å impl çš„å®ç°æ–¹å¼ï¼Œå’Œé structured çš„ç®—å­ç”Ÿæˆæ–¹å¼ä¸åŒã€‚è¿™é‡Œåœ¨ wrapper ä¸­æ²¡æœ‰è°ƒç”¨ at::native åå­—ç©ºé—´å‡½æ•°ï¼ˆ<code>at::native::index_out</code> ï¼‰ï¼Œæ‰€ä»¥ä¸éœ€è¦ä¸ºå…¶æä¾›å®šä¹‰ï¼Œä»£ç å…¶ä»–åœ°æ–¹ä¹Ÿæ‰¾ä¸åˆ°ã€‚å›å¿†å‰é¢ï¼Œindex_out çš„å®šä¹‰å®é™…ä¸Šæ˜¯é€šè¿‡ <code>TORCH_IMPL_FUNC(index_out)</code> æä¾›çš„ï¼Œå³ <code>at::native::structured_index_out::impl</code>ï¼Œå¹¶é <code>at::native::index_out</code> å‡½æ•°ã€‚</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#719e07>namespace</span> at {
</span></span><span style=display:flex><span>at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> wrapper_CUDA_index_out_Tensor_out(<span style=color:#719e07>const</span> at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> self, <span style=color:#719e07>const</span> c10<span style=color:#719e07>::</span>List<span style=color:#719e07>&lt;::</span>std<span style=color:#719e07>::</span>optional<span style=color:#719e07>&lt;</span>at<span style=color:#719e07>::</span>Tensor<span style=color:#719e07>&gt;&gt;</span> <span style=color:#719e07>&amp;</span> indices, at<span style=color:#719e07>::</span>Tensor <span style=color:#719e07>&amp;</span> out) {
</span></span><span style=display:flex><span>  <span style=color:#586e75>// No device check
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>structured_index_out_out <span style=color:#268bd2>op</span>(out);
</span></span><span style=display:flex><span><span style=color:#719e07>auto</span> precompute <span style=color:#719e07>=</span> op.meta(self, at<span style=color:#719e07>::</span>IOptTensorListRef(indices));
</span></span><span style=display:flex><span>(<span style=color:#dc322f>void</span>)precompute;
</span></span><span style=display:flex><span>op.impl(self, precompute.sizes, precompute.strides, op.maybe_get_output(<span style=color:#2aa198>0</span>));
</span></span><span style=display:flex><span><span style=color:#719e07>if</span> (op.proxy_outputs_[<span style=color:#2aa198>0</span>].has_value()) op.outputs_[<span style=color:#2aa198>0</span>].get().copy_(<span style=color:#719e07>*</span>op.proxy_outputs_[<span style=color:#2aa198>0</span>]);
</span></span><span style=display:flex><span><span style=color:#719e07>return</span> out;
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>TORCH_LIBRARY_IMPL(aten, CUDA, m) {
</span></span><span style=display:flex><span>    m.impl(<span style=color:#2aa198>&#34;index.Tensor&#34;</span>, TORCH_FN(wrapper_CUDA_index_Tensor));
</span></span><span style=display:flex><span>m.impl(<span style=color:#2aa198>&#34;index.Tensor_out&#34;</span>, TORCH_FN(wrapper_CUDA_index_out_Tensor_out));
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>æ³¨æ„åˆ°è¿™ä¸ª <code>at::wrapper_CUDA_index_out_Tensor_out</code> åœ¨å‰é¢ä¹Ÿç”¨åˆ°äº†ï¼å®é™…ä¸Šå°±æ˜¯ç›¸åŒçš„å®ç°ï¼Œåœ¨ <code>m.impl</code> ç”¨åˆ°äº†ï¼Œåœ¨ <code>at::cuda::index_out</code> ä¸­ä¹Ÿç”¨åˆ°äº†ã€‚</p></div><footer class=post-footer><div class=post-tags><a href=/tags/torch>torch</a></div><div class=addthis_inline_share_toolbox style=text-align:center></div><hr><div class=post-nav><div class="post-nav-next post-nav-item"><a href=/posts/cli/bash/diff-%E4%B8%A4%E4%B8%AA%E6%96%87%E4%BB%B6%E5%A4%B9/ rel=next title="diff ä¸¤ä¸ªæ–‡ä»¶å¤¹"><i class="fa fa-chevron-left"></i> diff ä¸¤ä¸ªæ–‡ä»¶å¤¹</a></div><div class="post-nav-prev post-nav-item"><a href=/posts/programming/python/1.-PyTorch-C++-%E5%87%BD%E6%95%B0%E6%B4%BE%E5%8F%91/ rel=prev title="PyTorch C++ å‡½æ•°æ´¾å‘">PyTorch C++ å‡½æ•°æ´¾å‘
<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>&copy;
<span itemprop=copyrightYear>2023 - 2025
</span><span class=with-love><i class="fa fa-heart"></i>
</span><span class=author itemprop=copyrightHolder>ğŸ¤–</span></div><div class=powered-by>ç”± <a href=https://gohugo.io title=0.143.1 target=_blank>Hugo</a> & <a href=https://github.com/hugo-next/hugo-theme-next title=4.5.3 target=_blank>Hugo NexT.Gemini</a> å¼ºåŠ›é©±åŠ¨</div></div></footer><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js defer></script><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.1.0/medium-zoom.min.js defer></script><script class=next-config data-name=main type=application/json>{"bookmark":{"color":"#222","enable":false,"save":"manual"},"copybtn":true,"darkmode":false,"hostname":"https://hxhue.github.io/","i18n":{"ds_day":" å¤©å‰","ds_days":" å¤© ","ds_hour":" å°æ—¶å‰","ds_hours":" å°æ—¶ ","ds_just":"åˆšåˆš","ds_min":" åˆ†é’Ÿå‰","ds_mins":" åˆ†é’Ÿ","ds_month":" ä¸ªæœˆå‰","ds_years":" å¹´ ","empty":"æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœç´¢ç»“æœï¼š${query}","hits":"æ‰¾åˆ° ${hits} ä¸ªæœç´¢ç»“æœ","hits_time":"æ‰¾åˆ° ${hits} ä¸ªæœç´¢ç»“æœï¼ˆç”¨æ—¶ ${time} æ¯«ç§’ï¼‰","placeholder":"æœç´¢..."},"lang":"zh-CN","lazyload":false,"localSearch":{"enable":true,"limit":1e3,"path":"/searchindexes.xml","preload":false,"topnperarticle":-1,"trigger":"auto","unescape":false},"motion":{"async":true,"enable":false,"transition":{"collheader":"fadeInLeft","menu_item":"fadeInDown","postblock":"fadeIn","postbody":"fadeInDown","postheader":"fadeInDown","sidebar":"fadeInUp"}},"postmeta":{"comments":{"enable":false,"plugin":"waline"},"views":{"enable":false,"plugin":"busuanzi"}},"root":"/","scheme":"Gemini","sidebar":{"display":"post","offset":12,"padding":18,"position":"left","width":256},"vendor":{"plugins":"cdnjs","router":"https://cdnjs.cloudflare.com/ajax/libs"},"version":"4.5.3"}</script><script type=text/javascript src=/js/main.min.37ba8b54f9d4d784d08028c45eea93b5d4e13eda8ee7fb0d2edd6f3fac66cfd2.js defer></script></body></html>