<!doctype html><html lang=zh-CN data-theme=light><head><meta charset=UTF-8><meta name=viewport content="width=device-width"><meta name=theme-color content="#222" media="(prefers-color-scheme: light)"><meta name=generator content="Hugo 0.143.1"><link rel="shortcut icon" type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=/imgs/icons/favicon_16x16_next.png><link rel=icon type=image/png sizes=32x32 href=/imgs/icons/favicon_32_32_next.png><link rel=apple-touch-icon sizes=180x180 href=/imgs/icons/apple_touch_icon_next.png><meta itemprop=name content="如何对齐 PyTorch 的除法运算？"><meta itemprop=description content="个人博客，主要是零散的笔记。"><meta itemprop=datePublished zgotmplz><meta itemprop=dateModified zgotmplz><meta itemprop=image content="https://hxhue.github.io/imgs/371907.jpg"><meta itemprop=keywords content="torch,python"><link type=text/css rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.2/css/all.min.css><link type=text/css rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css><link rel=stylesheet href=/css/main.min.bea76f574a755574e17d42bea39502a74ca3ca4db65807b8c82d3e26dcec8420.css><style type=text/css>.post-footer,.flinks-list-footer hr:after{content:"~ 我可是有底线的哟 ~"}</style><link rel=stylesheet type=text/css href=/css/style.css><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/github-markdown-css@5.3.0/github-markdown-dark.css><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js></script><script>MathJax={tex:{displayMath:[["$$","$$"],["\\[","\\]"]],inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"}}</script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.esm.min.mjs';
  mermaid.initialize({ startOnLoad: false });
  mermaid.mermaidAPI.initialize();
  window.mermaid = mermaid;
</script><script type=text/javascript>(function(){localDB={set:function(e,t,n){if(n===0)return;const s=new Date,o=n*864e5,i={value:t,expiry:s.getTime()+o};localStorage.setItem(e,JSON.stringify(i))},get:function(e){const t=localStorage.getItem(e);if(!t)return void 0;const n=JSON.parse(t),s=new Date;return s.getTime()>n.expiry?(localStorage.removeItem(e),void 0):n.value}},theme={active:function(){const e=localDB.get("theme");if(e==null)return;theme.toggle(e),window.matchMedia("(prefers-color-scheme: dark)").addListener(function(e){theme.toggle(e.matches?"dark":"light")})},toggle:function(e){document.documentElement.setAttribute("data-theme",e),localDB.set("theme",e,2);const t=document.querySelector("iframe.giscus-frame");if(t){const n={setConfig:{theme:e}};t.contentWindow.postMessage({giscus:n},"https://giscus.app")}}},theme.active()})(window)</script><script class=next-config data-name=page type=application/json>{"comments":false,"isHome":false,"isPage":true,"path":"%E5%A6%82%E4%BD%95%E5%AF%B9%E9%BD%90-PyTorch-%E7%9A%84%E9%99%A4%E6%B3%95%E8%BF%90%E7%AE%97","permalink":"https://hxhue.github.io/posts/programming/python/%E5%A6%82%E4%BD%95%E5%AF%B9%E9%BD%90-PyTorch-%E7%9A%84%E9%99%A4%E6%B3%95%E8%BF%90%E7%AE%97/","title":"如何对齐 PyTorch 的除法运算？","waline":{"js":[{"alias":"waline","alias_name":"@waline/client","file":"dist/pageview.js","name":"pageview","version":"2.13.0"},{"alias":"waline","alias_name":"@waline/client","file":"dist/comment.js","name":"comment","version":"2.13.0"}]}}</script><script type=text/javascript>document.addEventListener("DOMContentLoaded",()=>{var e=document.createElement("script");e.charset="UTF-8",e.src="",e.async=!1,e.defer=!0,document.head.appendChild(e),e.onload=function(){NexT.utils.fmtBusuanzi()}})</script><title>如何对齐 PyTorch 的除法运算？ - Bluegill</title><noscript><link rel=stylesheet href=/css/noscript.css></noscript></head><body itemscope itemtype=http://schema.org/WebPage><div class=headband></div><main class=main><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div class=toggle aria-label role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div></div><div class=site-meta><a href=/ class=brand rel=start><i class=logo-line></i><h1 class=site-title>Bluegill</h1><i class=logo-line></i></a><p class=site-subtitle itemprop=description></p></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href=/ class=hvr-icon-pulse rel=section><i class="fa fa-home hvr-icon"></i>首页</a></li><li class="menu-item menu-item-about"><a href=/about/ class=hvr-icon-pulse rel=section><i class="fa fa-user hvr-icon"></i>关于</a></li><li class="menu-item menu-item-archives"><a href=/archives/ class=hvr-icon-pulse rel=section><i class="fa fa-archive hvr-icon"></i>归档</a></li><li class="menu-item menu-item-categories"><a href=/categories/ class=hvr-icon-pulse rel=section><i class="fa fa-th hvr-icon"></i>分类</a></li><li class="menu-item menu-item-tags"><a href=/tags/ class=hvr-icon-pulse rel=section><i class="fa fa-hashtag hvr-icon"></i>标签</a></li><li class="menu-item menu-item-daily"><a href=/daily/ class=hvr-icon-pulse rel=section><i class="fa fa-newspaper hvr-icon"></i>随笔</a></li><li class="menu-item menu-item-discovery"><a href=https://rift-fear-f2c.notion.site/2025-1e354a33cfb1802c841bdf29f2f3dab3 class=hvr-icon-pulse rel=section><i class="fa fa-compass hvr-icon"></i>发现</a></li><li class="menu-item menu-item-search"><a role=button class="popup-trigger hvr-icon-pulse"><i class="fa fa-search fa-fw hvr-icon"></i>搜索</a></li></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon><i class="fa fa-search"></i></span><div class=search-input-container><input autocomplete=off autocapitalize=off maxlength=80 placeholder=搜索... spellcheck=false type=search class=search-input></div><span class=popup-btn-close role=button><i class="fa fa-times-circle"></i></span></div><div class="search-result-container no-result"><div class=search-result-icon><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></div><div class="toggle sidebar-toggle" role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div><aside class=sidebar><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class=sidebar-nav><li class=sidebar-nav-toc>文章目录</li><li class=sidebar-nav-overview>站点概览</li></ul><div class=sidebar-panel-container><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><nav id=TableOfContents><ul><li><a href=#过程>过程</a></li><li><a href=#结论>结论</a></li></ul></nav></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author site-overview-item animated" itemprop=author itemscope itemtype=http://schema.org/Person><img class=site-author-image itemprop=image alt=🤖 src=/imgs/371907.jpg><p class=site-author-name itemprop=name>🤖</p><div class=site-description itemprop=description>个人博客，主要是零散的笔记。</div></div><div class="site-state-wrap site-overview-item animated"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/><span class=site-state-item-count>433</span>
<span class=site-state-item-name>日志</span></a></div><div class="site-state-item site-state-categories"><a href=/categories/><span class=site-state-item-count>12</span>
<span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/><span class=site-state-item-count>86</span>
<span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-social site-overview-item animated"><span class=links-of-social-item><a href=https://github.com/hxhue title="Github → https://github.com/hxhue" rel=noopener target=_blank><i class="fab fa-github fa-fw"></i>
Github
</a></span><span class=links-of-social-item><a href=/rss.xml title="RSS 订阅 → /rss.xml" rel=noopener target=_blank><i class="fa fa-rss fa-fw"></i>
RSS 订阅</a></span></div><div class="cc-license animated" itemprop=license><a href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh class=cc-opacity rel=noopener target=_blank title=共享知识><img src=/imgs/cc/big/by_nc_sa.svg alt=共享知识></a></div><div class="links-of-blogroll site-overview-item animated"><div class=links-of-blogroll-title><i class="fa fa-globe fa-fw"></i>
友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=https://shuai.guru/ title=https://shuai.guru/ target=_blank>shuai.guru</a></li></ul></div></div></div></div></aside><div class=sidebar-dimmer></div></header><div class=tool-buttons><div id=toggle-theme class=button title=深浅模式切换><i class="fas fa-adjust"></i></div><div class=back-to-top role=button title=返回顶部><i class="fa fa-arrow-up"></i>
<span>0%</span></div></div><div class=reading-progress-bar></div><noscript><div class=noscript-warning>Theme NexT works best with JavaScript enabled</div></noscript><div class="main-inner post posts-expand"><div class=post-block><article itemscope itemtype=http://schema.org/Article class=post-content lang><link itemprop=mainEntityOfPage href=https://hxhue.github.io/posts/programming/python/%E5%A6%82%E4%BD%95%E5%AF%B9%E9%BD%90-PyTorch-%E7%9A%84%E9%99%A4%E6%B3%95%E8%BF%90%E7%AE%97/><span hidden itemprop=author itemscope itemtype=http://schema.org/Person><meta itemprop=image content="/imgs/371907.jpg"><meta itemprop=name content="🤖"></span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization><meta itemprop=name content="🤖"><meta itemprop=description content="个人博客，主要是零散的笔记。"></span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork><meta itemprop=name content="如何对齐 PyTorch 的除法运算？"><meta itemprop=description content="过程

我遇到的情况是：a 为 64 位浮点数（FP64）标量，b 为 32 位浮点数（FP32）张量，要计算 a / b。
一种做法是：使用 1 / b * a 来代替 a / b。这样的结果看起来和 PyTorch 的计算是对齐的。"></span><header class=post-header><h1 class=post-title itemprop="name headline">如何对齐 PyTorch 的除法运算？</h1><div class=post-meta-container><div class=post-meta-items><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar"></i>
</span><span class=post-meta-item-text title=发表于>发表于：
</span><time title="创建时间：2024-11-26 00:00:00 +0800 CST" itemprop="dateCreated datePublished" datetime="2024-11-26 00:00:00 +0800 CST">2024-11-26
</time></span><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar-check"></i>
</span><span class=post-meta-item-text title=更新于>更新于：
</span><time title=修改时间：2024-12-07T00:00:00+08:00 itemprop=dateModified datetime=2024-12-07T00:00:00+08:00>2024-12-07</time></span></div><div class=post-meta-items><span class=post-meta-item title=字数><span class=post-meta-item-icon><i class="far fa-file-word"></i>
</span><span class=post-meta-item-text>字数：</span>
<span>1529</span>
</span><span class=post-meta-item title=阅读><span class=post-meta-item-icon><i class="far fa-clock"></i>
</span><span class=post-meta-item-text>阅读：&ap;</span>
<span>4分钟</span></span></div></div></header><div class=post-body itemprop=articleBody><h1 id=过程>过程
<a class=header-anchor href=#%e8%bf%87%e7%a8%8b></a></h1><p>我遇到的情况是：<code>a</code> 为 64 位浮点数（FP64）标量，<code>b</code> 为 32 位浮点数（FP32）张量，要计算 <code>a / b</code>。</p><p>一种做法是：使用 <code>1 / b * a</code> 来代替 <code>a / b</code>。这样的结果看起来和 PyTorch 的计算是对齐的。</p><p>奇怪的是，在 <code>aten/src/ATen/native/cuda/BinaryDivTrueKernel.cu</code> 这个代码的判断条件中，只有分母是 CPU 上的标量时，才会将除法转换成乘法运算，而我的遇到的情况是分子是标量，分母是张量，不符合这个条件。代码：</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#719e07>namespace</span> at<span style=color:#719e07>::</span>native {
</span></span><span style=display:flex><span><span style=color:#719e07>namespace</span> binary_internal {
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>CONSTEXPR_EXCEPT_WIN_CUDA <span style=color:#dc322f>char</span> div_name[] <span style=color:#719e07>=</span> <span style=color:#2aa198>&#34;div_kernel&#34;</span>;
</span></span><span style=display:flex><span><span style=color:#dc322f>void</span> <span style=color:#268bd2>div_true_kernel_cuda</span>(TensorIteratorBase<span style=color:#719e07>&amp;</span> iter) {
</span></span><span style=display:flex><span>  <span style=color:#719e07>auto</span> common_dtype <span style=color:#719e07>=</span> iter.common_dtype();
</span></span><span style=display:flex><span>  <span style=color:#719e07>if</span> (iter.common_dtype() <span style=color:#719e07>==</span> kComplexHalf) {
</span></span><span style=display:flex><span>    <span style=color:#586e75>// 省略
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>  }
</span></span><span style=display:flex><span>  <span style=color:#719e07>if</span> (iter.is_cpu_scalar(<span style=color:#2aa198>2</span>)) {
</span></span><span style=display:flex><span>    <span style=color:#586e75>// optimization for floating-point types: if the second operand is a CPU
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>    <span style=color:#586e75>// scalar, compute a * reciprocal(b). Note that this may lose one bit of
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>    <span style=color:#586e75>// precision compared to computing the division.
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>    AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(
</span></span><span style=display:flex><span>        kHalf, kBFloat16, common_dtype, <span style=color:#2aa198>&#34;div_true_cuda&#34;</span>, [<span style=color:#719e07>&amp;</span>]() {
</span></span><span style=display:flex><span>          <span style=color:#719e07>using</span> opmath_t <span style=color:#719e07>=</span> at<span style=color:#719e07>::</span>opmath_type<span style=color:#719e07>&lt;</span>scalar_t<span style=color:#719e07>&gt;</span>;
</span></span><span style=display:flex><span>          <span style=color:#719e07>auto</span> inv_b <span style=color:#719e07>=</span> opmath_t(<span style=color:#2aa198>1.0</span>) <span style=color:#719e07>/</span> iter.scalar_value<span style=color:#719e07>&lt;</span>opmath_t<span style=color:#719e07>&gt;</span>(<span style=color:#2aa198>2</span>);
</span></span><span style=display:flex><span>          iter.remove_operand(<span style=color:#2aa198>2</span>);
</span></span><span style=display:flex><span>          gpu_kernel(
</span></span><span style=display:flex><span>              iter,
</span></span><span style=display:flex><span>              BUnaryFunctor<span style=color:#719e07>&lt;</span>scalar_t, scalar_t, scalar_t, MulFunctor<span style=color:#719e07>&lt;</span>opmath_t<span style=color:#719e07>&gt;&gt;</span>(
</span></span><span style=display:flex><span>                  MulFunctor<span style=color:#719e07>&lt;</span>opmath_t<span style=color:#719e07>&gt;</span>(), inv_b));
</span></span><span style=display:flex><span>        });
</span></span><span style=display:flex><span>  } <span style=color:#719e07>else</span> {
</span></span><span style=display:flex><span>    AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND2(
</span></span><span style=display:flex><span>        kHalf, kBFloat16, common_dtype, <span style=color:#2aa198>&#34;div_true_cuda&#34;</span>, [<span style=color:#719e07>&amp;</span>]() {
</span></span><span style=display:flex><span>          DivFunctor<span style=color:#719e07>&lt;</span>scalar_t<span style=color:#719e07>&gt;</span> f;
</span></span><span style=display:flex><span>          gpu_kernel_with_scalars(iter, f);
</span></span><span style=display:flex><span>        });
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>} <span style=color:#586e75>// namespace binary_internal
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>
</span></span><span style=display:flex><span>REGISTER_DISPATCH(div_true_stub, <span style=color:#719e07>&amp;</span>binary_internal<span style=color:#719e07>::</span>div_true_kernel_cuda);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>} <span style=color:#586e75>// namespace at::native
</span></span></span></code></pre></div><p>接下来是做了一些实验。（注意：因为有随机数，下面的结果每次都可能不一样，但是大致表现是相同的。一开始记录结果的时候忘记了固定随机数种子。）</p><p>我尝试检查 PyTorch 是否也会对分母不是 CPU 标量的情况进行除法到乘法的优化，结果看上去是不会，因为 torch 的除法结果和直白的除法计算完全是一样的。</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#719e07>import</span> torch
</span></span><span style=display:flex><span><span style=color:#719e07>import</span> numpy <span style=color:#719e07>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#719e07>for</span> x <span style=color:#719e07>in</span> torch<span style=color:#719e07>.</span>randn([<span style=color:#2aa198>1024</span> <span style=color:#719e07>*</span> <span style=color:#2aa198>1024</span>])<span style=color:#719e07>.</span>tolist():
</span></span><span style=display:flex><span>    a <span style=color:#719e07>=</span> torch<span style=color:#719e07>.</span>tensor(<span style=color:#2aa198>1.1</span>)<span style=color:#719e07>.</span>cuda()
</span></span><span style=display:flex><span>    b <span style=color:#719e07>=</span> torch<span style=color:#719e07>.</span>tensor([x, <span style=color:#2aa198>0.1</span>])<span style=color:#719e07>.</span>cuda()
</span></span><span style=display:flex><span>    c <span style=color:#719e07>=</span> (a <span style=color:#719e07>/</span> b)[<span style=color:#2aa198>0</span>]<span style=color:#719e07>.</span>cpu()<span style=color:#719e07>.</span>numpy()<span style=color:#719e07>.</span>view(np<span style=color:#719e07>.</span>int32)
</span></span><span style=display:flex><span>    d <span style=color:#719e07>=</span> (np<span style=color:#719e07>.</span>float32(<span style=color:#2aa198>1.1</span>) <span style=color:#719e07>/</span> np<span style=color:#719e07>.</span>float32(x))<span style=color:#719e07>.</span>view(np<span style=color:#719e07>.</span>int32)
</span></span><span style=display:flex><span>    <span style=color:#719e07>assert</span> c <span style=color:#719e07>==</span> d, <span style=color:#2aa198>f</span><span style=color:#2aa198>&#39;note: </span><span style=color:#2aa198>{</span>x<span style=color:#2aa198>=}</span><span style=color:#2aa198>, </span><span style=color:#2aa198>{</span>c<span style=color:#2aa198>=}</span><span style=color:#2aa198>, </span><span style=color:#2aa198>{</span>d<span style=color:#2aa198>=}</span><span style=color:#2aa198>&#39;</span>
</span></span></code></pre></div><p>用 numpy 做实验，结果很符合直觉，32 位除法确实比 32 位乘法更贴近 64 位除法的计算结果。</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#719e07>import</span> numpy <span style=color:#719e07>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>a_is_better <span style=color:#719e07>=</span> <span style=color:#2aa198>0</span>
</span></span><span style=display:flex><span>b_is_better <span style=color:#719e07>=</span> <span style=color:#2aa198>0</span>
</span></span><span style=display:flex><span><span style=color:#719e07>for</span> x <span style=color:#719e07>in</span> np<span style=color:#719e07>.</span>random<span style=color:#719e07>.</span>random(<span style=color:#2aa198>1024</span>)<span style=color:#719e07>.</span>tolist():
</span></span><span style=display:flex><span>    x <span style=color:#719e07>=</span> np<span style=color:#719e07>.</span>float32(x)
</span></span><span style=display:flex><span>    a <span style=color:#719e07>=</span> np<span style=color:#719e07>.</span>float32(<span style=color:#2aa198>1.1</span>) <span style=color:#719e07>/</span> x
</span></span><span style=display:flex><span>    b <span style=color:#719e07>=</span> np<span style=color:#719e07>.</span>float32(<span style=color:#2aa198>1.1</span>) <span style=color:#719e07>*</span> (np<span style=color:#719e07>.</span>float32(<span style=color:#2aa198>1</span>) <span style=color:#719e07>/</span> x)
</span></span><span style=display:flex><span>    c <span style=color:#719e07>=</span> np<span style=color:#719e07>.</span>float64(<span style=color:#2aa198>1.1</span>) <span style=color:#719e07>/</span> x
</span></span><span style=display:flex><span>    d1 <span style=color:#719e07>=</span> np<span style=color:#719e07>.</span>abs(a<span style=color:#719e07>-</span>c)
</span></span><span style=display:flex><span>    d2 <span style=color:#719e07>=</span> np<span style=color:#719e07>.</span>abs(b<span style=color:#719e07>-</span>c)
</span></span><span style=display:flex><span>    <span style=color:#719e07>if</span> d1 <span style=color:#719e07>&lt;</span> d2:
</span></span><span style=display:flex><span>        a_is_better <span style=color:#719e07>+=</span> <span style=color:#2aa198>1</span>
</span></span><span style=display:flex><span>    <span style=color:#719e07>elif</span> d2 <span style=color:#719e07>&lt;</span> d1:
</span></span><span style=display:flex><span>        b_is_better <span style=color:#719e07>+=</span> <span style=color:#2aa198>1</span>
</span></span><span style=display:flex><span><span style=color:#b58900>print</span>(<span style=color:#2aa198>f</span><span style=color:#2aa198>&#39;</span><span style=color:#2aa198>{</span>a_is_better<span style=color:#2aa198>=}</span><span style=color:#2aa198>, </span><span style=color:#2aa198>{</span>b_is_better<span style=color:#2aa198>=}</span><span style=color:#2aa198>&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#586e75># a_is_better=163, b_is_better=98</span>
</span></span></code></pre></div><p>如果把 <code>c = np.float64(1.1) / x</code> 改成 <code>c = 1.1 / x</code>，那么结果则几乎完全倾向于 &ldquo;a is better&rdquo;，而不是像现在这样（a 好和 b 好的情况都有，但 a 好的情况更多）。</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-diff data-lang=diff><span style=display:flex><span> a_is_better = 0
</span></span><span style=display:flex><span> b_is_better = 0
</span></span><span style=display:flex><span> for x in np.random.random(1024).tolist():
</span></span><span style=display:flex><span>     x = np.float32(x)
</span></span><span style=display:flex><span>     a = np.float32(1.1) / x
</span></span><span style=display:flex><span>     b = np.float32(1.1) * (np.float32(1) / x)
</span></span><span style=display:flex><span><span style=color:#dc322f>-    c = np.float64(1.1) / x
</span></span></span><span style=display:flex><span><span style=color:#dc322f></span><span style=color:#719e07>+    c = 1.1 / x
</span></span></span><span style=display:flex><span><span style=color:#719e07></span>     d1 = np.abs(a-c)
</span></span><span style=display:flex><span>     d2 = np.abs(b-c)
</span></span><span style=display:flex><span>     if d1 &lt; d2:
</span></span><span style=display:flex><span>         a_is_better += 1
</span></span><span style=display:flex><span>     elif d2 &lt; d1:
</span></span><span style=display:flex><span>         b_is_better += 1
</span></span><span style=display:flex><span> print(f&#39;{a_is_better=}, {b_is_better=}&#39;)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#dc322f>-# a_is_better=163, b_is_better=98
</span></span></span><span style=display:flex><span><span style=color:#dc322f></span><span style=color:#719e07>+# a_is_better=262, b_is_better=0
</span></span></span></code></pre></div><p>用 torch 做实验，“分子 1.1 是 Python 内置浮点数还是 numpy/torch 的 64 位浮点数”对结果的影响不同。换成 Python 内置浮点数之后，结果变成了“b is better”。<strong>这证明 torch 和 numpy 都对 Python 内置浮点数这样的情况有特殊计算路径</strong>。</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-diff data-lang=diff><span style=display:flex><span> import torch
</span></span><span style=display:flex><span> import numpy as np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span> a_is_better = 0
</span></span><span style=display:flex><span> b_is_better = 0
</span></span><span style=display:flex><span> for x in np.random.random(1024).tolist():
</span></span><span style=display:flex><span>     x = torch.tensor(x).cuda()
</span></span><span style=display:flex><span>     a = torch.tensor(1.1) / x
</span></span><span style=display:flex><span>     b = torch.tensor(1.1) * (torch.tensor(1) / x)
</span></span><span style=display:flex><span><span style=color:#dc322f>-    c = torch.tensor(1.1, dtype=torch.float64) / x
</span></span></span><span style=display:flex><span><span style=color:#dc322f></span><span style=color:#719e07>+    c = 1.1 / x
</span></span></span><span style=display:flex><span><span style=color:#719e07></span>     d1 = torch.abs(a-c).item()
</span></span><span style=display:flex><span>     d2 = torch.abs(b-c).item()
</span></span><span style=display:flex><span>     if d1 &lt; d2:
</span></span><span style=display:flex><span>         a_is_better += 1
</span></span><span style=display:flex><span>     elif d2 &lt; d1:
</span></span><span style=display:flex><span>         b_is_better += 1
</span></span><span style=display:flex><span> print(f&#39;{a_is_better=}, {b_is_better=}&#39;)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#dc322f>-# a_is_better=151, b_is_better=101
</span></span></span><span style=display:flex><span><span style=color:#dc322f></span><span style=color:#719e07>+# a_is_better=0, b_is_better=278
</span></span></span></code></pre></div><div class="markdown-alert markdown-alert-note"><p class=markdown-alert-title><svg class="octicon octicon-info mr-2" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1116 0A8 8 0 010 8zm8-6.5a6.5 6.5.0 100 13 6.5 6.5.0 000-13zM6.5 7.75A.75.75.0 017.25 7h1a.75.75.0 01.75.75v2.75h.25a.75.75.0 010 1.5h-2a.75.75.0 010-1.5h.25v-2h-.25a.75.75.0 01-.75-.75zM8 6a1 1 0 110-2 1 1 0 010 2z"/></svg>Note</p><p>这里使用的分子 1.1 是在 CPU 上，分母 <code>x</code> 在 GPU 上，仍然能进行计算，说明 PyTorch 对于标量做了特殊处理，不要求和其他张量在同一个设备上。我将所有分子的末尾都加上 <code>.cuda()</code>，将其转移到 cuda:0 设备上，但是这并不影响结果。</p></div><p>经过检查，<code>1.1 / x</code> 的 <code>/</code> 操作符是写在 <code>float</code> 类型的 <code>__truediv__</code> 方法中的（<code>1 / x</code> 的 <code>/</code> 则写在 <code>int</code> 类型的 <code>__truediv__</code> 中）。<strong>这导致了被除数为 Python 内置类型时，代码执行的路径不同</strong>。如果被除数是 torch 的张量，那么使用的应该是 <code>TensorBase</code>（源代码注释：Defined in torch/csrc/autograd/python_variable.cpp）中的 <code>__truediv__</code>。</p><p>既然被除数是 Python 内置类型时，调用的方法不同，那么 torch 和 numpy 又是怎么向 Python 注册这些方法的？<code>int</code> 和 <code>float</code> 是 Python 内置类型（因此不可能知道 <code>torch.Tensor</code> 的存在） ，在遇到 <code>__truediv__</code> 时会认为其参数是未知类型，进而尝试在其参数上调用 <a href=https://docs.python.org/3.3/reference/datamodel.html#object.__rtruediv__ title=__rtruediv__ rel="noopener external nofollow noreferrer" target=_blank class=exturl><code>__rtruediv__</code><i class="fa fa-external-link-alt"></i></a> 方法（如果这个方法有实现）。</p><div class="markdown-alert markdown-alert-note"><p class=markdown-alert-title><svg class="octicon octicon-info mr-2" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1116 0A8 8 0 010 8zm8-6.5a6.5 6.5.0 100 13 6.5 6.5.0 000-13zM6.5 7.75A.75.75.0 017.25 7h1a.75.75.0 01.75.75v2.75h.25a.75.75.0 010 1.5h-2a.75.75.0 010-1.5h.25v-2h-.25a.75.75.0 01-.75-.75zM8 6a1 1 0 110-2 1 1 0 010 2z"/></svg>Note</p><p>这里 <code>__rtruediv__</code> 的 <code>r</code> 字母不是 right 的意思，而是 reflected 的意思。</p></div><p>在文件 <code>torch/_tensor.py</code> 中，有对这个方法的定义：</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#719e07>class</span> <span style=color:#268bd2>Tensor</span>(torch<span style=color:#719e07>.</span>_C<span style=color:#719e07>.</span>TensorBase):
</span></span><span style=display:flex><span>    <span style=color:#268bd2>@_handle_torch_function_and_wrap_type_error_to_not_implemented</span>
</span></span><span style=display:flex><span>    <span style=color:#719e07>def</span> <span style=color:#268bd2>__rdiv__</span>(<span style=color:#268bd2>self</span>, other):
</span></span><span style=display:flex><span>        <span style=color:#719e07>return</span> <span style=color:#268bd2>self</span><span style=color:#719e07>.</span>reciprocal() <span style=color:#719e07>*</span> other
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    __rtruediv__ <span style=color:#719e07>=</span> __rdiv__
</span></span><span style=display:flex><span>    __itruediv__ <span style=color:#719e07>=</span> _C<span style=color:#719e07>.</span>TensorBase<span style=color:#719e07>.</span>__idiv__
</span></span></code></pre></div><p>实际上就是把除法转换为了乘法来算！</p><p>再看 numpy：</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#586e75># numpy/lib/mixins.py</span>
</span></span><span style=display:flex><span><span style=color:#719e07>def</span> <span style=color:#268bd2>_reflected_binary_method</span>(ufunc, name):
</span></span><span style=display:flex><span>    <span style=color:#2aa198>&#34;&#34;&#34;Implement a reflected binary method with a ufunc, e.g., __radd__.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#719e07>def</span> <span style=color:#268bd2>func</span>(<span style=color:#268bd2>self</span>, other):
</span></span><span style=display:flex><span>        <span style=color:#719e07>if</span> _disables_array_ufunc(other):
</span></span><span style=display:flex><span>            <span style=color:#719e07>return</span> <span style=color:#268bd2>NotImplemented</span>
</span></span><span style=display:flex><span>        <span style=color:#719e07>return</span> ufunc(other, <span style=color:#268bd2>self</span>)
</span></span><span style=display:flex><span>    func<span style=color:#719e07>.</span>__name__ <span style=color:#719e07>=</span> <span style=color:#2aa198>&#39;__r</span><span style=color:#2aa198>{}</span><span style=color:#2aa198>__&#39;</span><span style=color:#719e07>.</span>format(name)
</span></span><span style=display:flex><span>    <span style=color:#719e07>return</span> func
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#719e07>def</span> <span style=color:#268bd2>_numeric_methods</span>(ufunc, name):
</span></span><span style=display:flex><span>    <span style=color:#2aa198>&#34;&#34;&#34;Implement forward, reflected and inplace binary methods with a ufunc.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#719e07>return</span> (_binary_method(ufunc, name),
</span></span><span style=display:flex><span>            _reflected_binary_method(ufunc, name),
</span></span><span style=display:flex><span>            _inplace_binary_method(ufunc, name))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#719e07>class</span> <span style=color:#268bd2>NDArrayOperatorsMixin</span>:
</span></span><span style=display:flex><span>    <span style=color:#586e75># ...</span>
</span></span><span style=display:flex><span>    __truediv__, __rtruediv__, __itruediv__ <span style=color:#719e07>=</span> _numeric_methods(
</span></span><span style=display:flex><span>        um<span style=color:#719e07>.</span>true_divide, <span style=color:#2aa198>&#39;truediv&#39;</span>)
</span></span><span style=display:flex><span>    <span style=color:#586e75># ...</span>
</span></span></code></pre></div><p>其实就是把 <code>um.true_divide</code> 这个 ufunc 的两个参数交换顺序。也就是说 <code>1.1 / x</code> 在 x 为 <code>numpy.ndarray</code> 时，会先匹配到 <code>float.__truediv__(self, numpy.ndarray)</code>，再匹配到 <code>numpy.ndarray.__rtruediv__(self, f)</code>，然后匹配到 <code>um.true_divide(f, self)</code>。剩下的细节都是 <code>true_divide</code> 函数在处理。</p><p>从直觉上来讲，numpy 的处理方式更容易理解一点。PyTorch 这种转除法为乘法的计算方式应该有其用意。</p><h1 id=结论>结论
<a class=header-anchor href=#%e7%bb%93%e8%ae%ba></a></h1><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#586e75># x: torch.Tensor</span>
</span></span><span style=display:flex><span><span style=color:#2aa198>1.1</span> <span style=color:#719e07>/</span> x <span style=color:#719e07>=&gt;</span> x<span style=color:#719e07>.</span>__rtruediv__(<span style=color:#2aa198>1.1</span>) <span style=color:#719e07>=&gt;</span> x<span style=color:#719e07>.</span>reciprocal() <span style=color:#719e07>*</span> <span style=color:#2aa198>1.1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#586e75># y: numpy.ndarray</span>
</span></span><span style=display:flex><span><span style=color:#2aa198>1.1</span> <span style=color:#719e07>/</span> y <span style=color:#719e07>=&gt;</span> y<span style=color:#719e07>.</span>__rtruediv__(<span style=color:#2aa198>1.1</span>) <span style=color:#719e07>=&gt;</span> np<span style=color:#719e07>.</span>_core<span style=color:#719e07>.</span>umath<span style=color:#719e07>.</span>true_divide(<span style=color:#2aa198>1.1</span>, y)
</span></span></code></pre></div></div><footer class=post-footer><div class=post-tags><a href=/tags/torch>torch
</a><a href=/tags/python>python</a></div><div class=addthis_inline_share_toolbox style=text-align:center></div><hr><div class=post-nav><div class="post-nav-next post-nav-item"><a href=/posts/systems/Linux/%E8%8E%B7%E5%8F%96-ssh-%E4%B8%AD%E6%AF%8F%E4%B8%AA%E5%85%AC%E9%92%A5%E7%9A%84%E6%8C%87%E7%BA%B9/ rel=next title="获取 ssh 中已知公钥的指纹"><i class="fa fa-chevron-left"></i> 获取 ssh 中已知公钥的指纹</a></div><div class="post-nav-prev post-nav-item"><a href=/posts/programming/python/%E7%94%A8-cv2-%E6%88%96-Pillow-%E4%BF%9D%E5%AD%98-numpy-%E6%A0%BC%E5%BC%8F%E5%9B%BE%E7%89%87/ rel=prev title="用 cv2 或 Pillow 保存 numpy 格式图片">用 cv2 或 Pillow 保存 numpy 格式图片
<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>&copy;
<span itemprop=copyrightYear>2023 - 2025
</span><span class=with-love><i class="fa fa-heart"></i>
</span><span class=author itemprop=copyrightHolder>🤖</span></div><div class=powered-by>由 <a href=https://gohugo.io title=0.143.1 target=_blank>Hugo</a> & <a href=https://github.com/hugo-next/hugo-theme-next title=4.5.3 target=_blank>Hugo NexT.Gemini</a> 强力驱动</div></div></footer><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js defer></script><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.1.0/medium-zoom.min.js defer></script><script class=next-config data-name=main type=application/json>{"bookmark":{"color":"#222","enable":false,"save":"manual"},"copybtn":true,"darkmode":false,"hostname":"https://hxhue.github.io/","i18n":{"ds_day":" 天前","ds_days":" 天 ","ds_hour":" 小时前","ds_hours":" 小时 ","ds_just":"刚刚","ds_min":" 分钟前","ds_mins":" 分钟","ds_month":" 个月前","ds_years":" 年 ","empty":"没有找到任何搜索结果：${query}","hits":"找到 ${hits} 个搜索结果","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","placeholder":"搜索..."},"lang":"zh-CN","lazyload":false,"localSearch":{"enable":true,"limit":1e3,"path":"/searchindexes.xml","preload":false,"topnperarticle":-1,"trigger":"auto","unescape":false},"motion":{"async":true,"enable":false,"transition":{"collheader":"fadeInLeft","menu_item":"fadeInDown","postblock":"fadeIn","postbody":"fadeInDown","postheader":"fadeInDown","sidebar":"fadeInUp"}},"postmeta":{"comments":{"enable":false,"plugin":"waline"},"views":{"enable":false,"plugin":"busuanzi"}},"root":"/","scheme":"Gemini","sidebar":{"display":"post","offset":12,"padding":18,"position":"left","width":256},"vendor":{"plugins":"cdnjs","router":"https://cdnjs.cloudflare.com/ajax/libs"},"version":"4.5.3"}</script><script type=text/javascript src=/js/main.min.37ba8b54f9d4d784d08028c45eea93b5d4e13eda8ee7fb0d2edd6f3fac66cfd2.js defer></script></body></html>