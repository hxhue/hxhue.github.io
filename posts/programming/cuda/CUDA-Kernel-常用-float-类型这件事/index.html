<!doctype html><html lang=zh-CN data-theme=light><head><meta charset=UTF-8><meta name=viewport content="width=device-width"><meta name=theme-color content="#222" media="(prefers-color-scheme: light)"><meta name=generator content="Hugo 0.143.1"><link rel="shortcut icon" type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=/imgs/icons/favicon_16x16_next.png><link rel=icon type=image/png sizes=32x32 href=/imgs/icons/favicon_32_32_next.png><link rel=apple-touch-icon sizes=180x180 href=/imgs/icons/apple_touch_icon_next.png><meta itemprop=name content="CUDA Kernel 常用 float 类型这件事"><meta itemprop=description content="个人博客，主要是零散的笔记。"><meta itemprop=datePublished zgotmplz><meta itemprop=dateModified zgotmplz><meta itemprop=image content="https://hxhue.github.io/imgs/371907.jpg"><meta itemprop=keywords content="cuda"><link type=text/css rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.2/css/all.min.css><link type=text/css rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css><link rel=stylesheet href=/css/main.min.bea76f574a755574e17d42bea39502a74ca3ca4db65807b8c82d3e26dcec8420.css><style type=text/css>.post-footer,.flinks-list-footer hr:after{content:"~ 我可是有底线的哟 ~"}</style><link rel=stylesheet type=text/css href=/css/style.css><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/github-markdown-css@5.3.0/github-markdown-dark.css><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js></script><script>MathJax={tex:{displayMath:[["$$","$$"],["\\[","\\]"]],inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"}}</script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.esm.min.mjs';
  mermaid.initialize({ startOnLoad: false });
  mermaid.mermaidAPI.initialize();
  window.mermaid = mermaid;
</script><script type=text/javascript>(function(){localDB={set:function(e,t,n){if(n===0)return;const s=new Date,o=n*864e5,i={value:t,expiry:s.getTime()+o};localStorage.setItem(e,JSON.stringify(i))},get:function(e){const t=localStorage.getItem(e);if(!t)return void 0;const n=JSON.parse(t),s=new Date;return s.getTime()>n.expiry?(localStorage.removeItem(e),void 0):n.value}},theme={active:function(){const e=localDB.get("theme");if(e==null)return;theme.toggle(e),window.matchMedia("(prefers-color-scheme: dark)").addListener(function(e){theme.toggle(e.matches?"dark":"light")})},toggle:function(e){document.documentElement.setAttribute("data-theme",e),localDB.set("theme",e,2);const t=document.querySelector("iframe.giscus-frame");if(t){const n={setConfig:{theme:e}};t.contentWindow.postMessage({giscus:n},"https://giscus.app")}}},theme.active()})(window)</script><script class=next-config data-name=page type=application/json>{"comments":false,"isHome":false,"isPage":true,"path":"CUDA-Kernel-%E5%B8%B8%E7%94%A8-float-%E7%B1%BB%E5%9E%8B%E8%BF%99%E4%BB%B6%E4%BA%8B","permalink":"https://hxhue.github.io/posts/programming/cuda/CUDA-Kernel-%E5%B8%B8%E7%94%A8-float-%E7%B1%BB%E5%9E%8B%E8%BF%99%E4%BB%B6%E4%BA%8B/","title":"CUDA Kernel 常用 float 类型这件事","waline":{"js":[{"alias":"waline","alias_name":"@waline/client","file":"dist/pageview.js","name":"pageview","version":"2.13.0"},{"alias":"waline","alias_name":"@waline/client","file":"dist/comment.js","name":"comment","version":"2.13.0"}]}}</script><script type=text/javascript>document.addEventListener("DOMContentLoaded",()=>{var e=document.createElement("script");e.charset="UTF-8",e.src="",e.async=!1,e.defer=!0,document.head.appendChild(e),e.onload=function(){NexT.utils.fmtBusuanzi()}})</script><title>CUDA Kernel 常用 float 类型这件事 - Bluegill</title><noscript><link rel=stylesheet href=/css/noscript.css></noscript></head><body itemscope itemtype=http://schema.org/WebPage><div class=headband></div><main class=main><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div class=toggle aria-label role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div></div><div class=site-meta><a href=/ class=brand rel=start><i class=logo-line></i><h1 class=site-title>Bluegill</h1><i class=logo-line></i></a><p class=site-subtitle itemprop=description></p></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href=/ class=hvr-icon-pulse rel=section><i class="fa fa-home hvr-icon"></i>首页</a></li><li class="menu-item menu-item-about"><a href=/about/ class=hvr-icon-pulse rel=section><i class="fa fa-user hvr-icon"></i>关于</a></li><li class="menu-item menu-item-archives"><a href=/archives/ class=hvr-icon-pulse rel=section><i class="fa fa-archive hvr-icon"></i>归档</a></li><li class="menu-item menu-item-categories"><a href=/categories/ class=hvr-icon-pulse rel=section><i class="fa fa-th hvr-icon"></i>分类</a></li><li class="menu-item menu-item-tags"><a href=/tags/ class=hvr-icon-pulse rel=section><i class="fa fa-hashtag hvr-icon"></i>标签</a></li><li class="menu-item menu-item-daily"><a href=/daily/ class=hvr-icon-pulse rel=section><i class="fa fa-newspaper hvr-icon"></i>随笔</a></li><li class="menu-item menu-item-discovery"><a href=https://rift-fear-f2c.notion.site/2025-1e354a33cfb1802c841bdf29f2f3dab3 class=hvr-icon-pulse rel=section><i class="fa fa-compass hvr-icon"></i>发现</a></li><li class="menu-item menu-item-search"><a role=button class="popup-trigger hvr-icon-pulse"><i class="fa fa-search fa-fw hvr-icon"></i>搜索</a></li></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon><i class="fa fa-search"></i></span><div class=search-input-container><input autocomplete=off autocapitalize=off maxlength=80 placeholder=搜索... spellcheck=false type=search class=search-input></div><span class=popup-btn-close role=button><i class="fa fa-times-circle"></i></span></div><div class="search-result-container no-result"><div class=search-result-icon><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></div><div class="toggle sidebar-toggle" role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div><aside class=sidebar><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class=sidebar-nav><li class=sidebar-nav-toc>文章目录</li><li class=sidebar-nav-overview>站点概览</li></ul><div class=sidebar-panel-container><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><nav id=TableOfContents><ul><li><a href=#gpu-上双精度计算慢在哪里>GPU 上双精度计算慢在哪里？</a></li><li><a href=#怎么正确使用单精度类型谨防隐式转换成双精度>怎么正确使用单精度类型？——谨防隐式转换成双精度</a><ul><li><a href=#实验看看基础类型的乘法汇编是什么>实验：看看基础类型的乘法汇编是什么？</a></li><li><a href=#实验验证中间表示类型会影响计算精度>实验：验证中间表示类型会影响计算精度</a></li><li><a href=#pytorch-是怎么做的>PyTorch 是怎么做的？</a></li><li><a href=#fp32-计算精度低并不是-gpu-的原罪>FP32 计算精度低并不是 GPU 的原罪</a></li></ul></li><li><a href=#cpu-和-gpu-上的半精度计算>CPU 和 GPU 上的半精度计算</a></li><li><a href=#tensor-core-和混合精度计算>Tensor Core 和混合精度计算</a><ul><li><a href=#v100-的第一代-tensor-core-支持了-fp16fp32-的混合精度计算>V100 的第一代 Tensor Core 支持了 FP16+FP32 的混合精度计算</a></li><li><a href=#a100-的第三代-tensor-core-新增了-tf32-类型>A100 的第三代 Tensor Core 新增了 TF32 类型</a></li><li><a href=#tf32-会带来什么样的改变>TF32 会带来什么样的改变？</a></li><li><a href=#我们如何使用混合精度计算>我们如何使用混合精度计算？</a></li></ul></li></ul></nav></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author site-overview-item animated" itemprop=author itemscope itemtype=http://schema.org/Person><img class=site-author-image itemprop=image alt=🤖 src=/imgs/371907.jpg><p class=site-author-name itemprop=name>🤖</p><div class=site-description itemprop=description>个人博客，主要是零散的笔记。</div></div><div class="site-state-wrap site-overview-item animated"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/><span class=site-state-item-count>433</span>
<span class=site-state-item-name>日志</span></a></div><div class="site-state-item site-state-categories"><a href=/categories/><span class=site-state-item-count>12</span>
<span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/><span class=site-state-item-count>86</span>
<span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-social site-overview-item animated"><span class=links-of-social-item><a href=https://github.com/hxhue title="Github → https://github.com/hxhue" rel=noopener target=_blank><i class="fab fa-github fa-fw"></i>
Github
</a></span><span class=links-of-social-item><a href=/rss.xml title="RSS 订阅 → /rss.xml" rel=noopener target=_blank><i class="fa fa-rss fa-fw"></i>
RSS 订阅</a></span></div><div class="cc-license animated" itemprop=license><a href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh class=cc-opacity rel=noopener target=_blank title=共享知识><img src=/imgs/cc/big/by_nc_sa.svg alt=共享知识></a></div><div class="links-of-blogroll site-overview-item animated"><div class=links-of-blogroll-title><i class="fa fa-globe fa-fw"></i>
友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=https://shuai.guru/ title=https://shuai.guru/ target=_blank>shuai.guru</a></li></ul></div></div></div></div></aside><div class=sidebar-dimmer></div></header><div class=tool-buttons><div id=toggle-theme class=button title=深浅模式切换><i class="fas fa-adjust"></i></div><div class=back-to-top role=button title=返回顶部><i class="fa fa-arrow-up"></i>
<span>0%</span></div></div><div class=reading-progress-bar></div><noscript><div class=noscript-warning>Theme NexT works best with JavaScript enabled</div></noscript><div class="main-inner post posts-expand"><div class=post-block><article itemscope itemtype=http://schema.org/Article class=post-content lang><link itemprop=mainEntityOfPage href=https://hxhue.github.io/posts/programming/cuda/CUDA-Kernel-%E5%B8%B8%E7%94%A8-float-%E7%B1%BB%E5%9E%8B%E8%BF%99%E4%BB%B6%E4%BA%8B/><span hidden itemprop=author itemscope itemtype=http://schema.org/Person><meta itemprop=image content="/imgs/371907.jpg"><meta itemprop=name content="🤖"></span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization><meta itemprop=name content="🤖"><meta itemprop=description content="个人博客，主要是零散的笔记。"></span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork><meta itemprop=name content="CUDA Kernel 常用 float 类型这件事"><meta itemprop=description content="本文分别讨论双精度、单精度、半精度的浮点数计算，最后提及混合精度。在 CPU 方面，仅考虑 x86-64 CPU 和 GNU/Linux 上的 GCC 编译器；GPU 方面仅考虑 NVIDIA GPU。
GPU 上双精度计算慢在哪里？

https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#arithmetic-instructions 以上链接说明：GPU 双精度浮点数运算比单精度浮点数慢，在有些架构（很多 $x.y~(y \ne 0)$ 运算能力的 GPU 都是游戏卡）上甚至慢得多。除了指令慢之外，double 类型也不利于 cache 和全局内存带宽。"></span><header class=post-header><h1 class=post-title itemprop="name headline">CUDA Kernel 常用 float 类型这件事</h1><div class=post-meta-container><div class=post-meta-items><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar"></i>
</span><span class=post-meta-item-text title=发表于>发表于：
</span><time title="创建时间：2024-08-09 00:00:00 +0800 CST" itemprop="dateCreated datePublished" datetime="2024-08-09 00:00:00 +0800 CST">2024-08-09
</time></span><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar-check"></i>
</span><span class=post-meta-item-text title=更新于>更新于：
</span><time title=修改时间：2025-03-28T00:00:00+08:00 itemprop=dateModified datetime=2025-03-28T00:00:00+08:00>2025-03-28</time></span></div><div class=post-meta-items><span class=post-meta-item title=字数><span class=post-meta-item-icon><i class="far fa-file-word"></i>
</span><span class=post-meta-item-text>字数：</span>
<span>5338</span>
</span><span class=post-meta-item title=阅读><span class=post-meta-item-icon><i class="far fa-clock"></i>
</span><span class=post-meta-item-text>阅读：&ap;</span>
<span>11分钟</span></span></div></div></header><div class=post-body itemprop=articleBody><p>本文分别讨论双精度、单精度、半精度的浮点数计算，最后提及混合精度。在 CPU 方面，仅考虑 x86-64 CPU 和 GNU/Linux 上的 GCC 编译器；GPU 方面仅考虑 NVIDIA GPU。</p><h1 id=gpu-上双精度计算慢在哪里>GPU 上双精度计算慢在哪里？
<a class=header-anchor href=#gpu-%e4%b8%8a%e5%8f%8c%e7%b2%be%e5%ba%a6%e8%ae%a1%e7%ae%97%e6%85%a2%e5%9c%a8%e5%93%aa%e9%87%8c></a></h1><p><a href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#arithmetic-instructions title=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#arithmetic-instructions rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#arithmetic-instructions<i class="fa fa-external-link-alt"></i></a> 以上链接说明：GPU 双精度浮点数运算比单精度浮点数慢，在有些架构（很多 $x.y~(y \ne 0)$ 运算能力的 GPU 都是游戏卡）上甚至慢得多。除了指令慢之外，double 类型也不利于 cache 和全局内存带宽。</p><p><a href=https://forums.developer.nvidia.com/t/use-float-rather-than-double-in-a-kernel/107363 title=https://forums.developer.nvidia.com/t/use-float-rather-than-double-in-a-kernel/107363 rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://forums.developer.nvidia.com/t/use-float-rather-than-double-in-a-kernel/107363<i class="fa fa-external-link-alt"></i></a> <em>A 64 bit double variable takes 2 registers. A 32 bit float can be stored in 1 register.</em> 双精度浮点数的使用会增加单个 CUDA 线程对寄存器数量的需求，从而减少实际上同时可以运行的线程数。</p><h1 id=怎么正确使用单精度类型谨防隐式转换成双精度>怎么正确使用单精度类型？——谨防隐式转换成双精度
<a class=header-anchor href=#%e6%80%8e%e4%b9%88%e6%ad%a3%e7%a1%ae%e4%bd%bf%e7%94%a8%e5%8d%95%e7%b2%be%e5%ba%a6%e7%b1%bb%e5%9e%8b%e8%b0%a8%e9%98%b2%e9%9a%90%e5%bc%8f%e8%bd%ac%e6%8d%a2%e6%88%90%e5%8f%8c%e7%b2%be%e5%ba%a6></a></h1><p><em>CUDA 和 C/C++ 都不会先将 float 转 double 再计算，除非……</em></p><p><a href=https://godbolt.org/z/Me66bEeaa title=https://godbolt.org/z/Me66bEeaa rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://godbolt.org/z/Me66bEeaa<i class="fa fa-external-link-alt"></i></a> 我在本地尝试构造浮点数精度损失，但是无法构造出来，去 compiler explorer 一看发现实际上单精度浮点数被转换成了双精度浮点数计算。为什么呢？</p><p><a href=https://godbolt.org/z/jM6sGn5je title=https://godbolt.org/z/jM6sGn5je rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://godbolt.org/z/jM6sGn5je<i class="fa fa-external-link-alt"></i></a> 罪魁祸首其实是表达式中的那些常量，他们都是 double 类型的，会导致其他 float 参数被转换成 double。</p><p><a href=https://godbolt.org/z/4hevdr7Mo title=https://godbolt.org/z/4hevdr7Mo rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://godbolt.org/z/4hevdr7Mo<i class="fa fa-external-link-alt"></i></a> 写模板函数时如果需要使用浮点数字面量，最好是先写成 double 类型（比如 3.14 而不是 3.14f）然后在前面做类型强制转换，字面量的类型转换可以由编译器完成，而无需运行时使用类型转换指令。</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-c data-lang=c><span style=display:flex><span>template <span style=color:#719e07>&lt;</span><span style=color:#268bd2>typename</span> T<span style=color:#719e07>&gt;</span>
</span></span><span style=display:flex><span>__device__ __host__ T <span style=color:#268bd2>helper</span>(T a, T b) {
</span></span><span style=display:flex><span>  <span style=color:#719e07>return</span> ((a <span style=color:#719e07>*</span> (T)<span style=color:#2aa198>3.4</span>) <span style=color:#719e07>*</span> b <span style=color:#719e07>*</span> (T)<span style=color:#2aa198>1.2</span>) <span style=color:#719e07>*</span> a <span style=color:#719e07>*</span> b <span style=color:#719e07>/</span> (T)<span style=color:#2aa198>0.7</span>;
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>这个类型转换规则是编程语言的规定，不分 CPU 还是 GPU。那 float 什么时候会没遇到 double 还自动转换成 double 呢？在可变长参数函数中会这样。比如 <a href=https://godbolt.org/z/4PeK6coMn title=https://godbolt.org/z/4PeK6coMn rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://godbolt.org/z/4PeK6coMn<i class="fa fa-external-link-alt"></i></a> 中对 <code>printf()</code> 的传参导致 cvtss2sd 指令的出现。</p><p><strong>Integer/floating-point promotion</strong>: <del>难道 float 在计算时不会先转成 double 吗</del>？这是记混知识了。整数参与计算时是会至少 promote 到 32 位的（小于 int 表示范围的会 promote 到 int），而浮点数只有其他参数表数范围更大时才会发生 promotion。可以参考 <a href=https://en.cppreference.com/w/c/language/conversion title=https://en.cppreference.com/w/c/language/conversion rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://en.cppreference.com/w/c/language/conversion<i class="fa fa-external-link-alt"></i></a> ，里面有个值得注意的是 <code>0UL - 1LL</code> 计算时两个参数都会被转换成 <code>unsigned long long</code> 类型。</p><h2 id=实验看看基础类型的乘法汇编是什么>实验：看看基础类型的乘法汇编是什么？
<a class=header-anchor href=#%e5%ae%9e%e9%aa%8c%e7%9c%8b%e7%9c%8b%e5%9f%ba%e7%a1%80%e7%b1%bb%e5%9e%8b%e7%9a%84%e4%b9%98%e6%b3%95%e6%b1%87%e7%bc%96%e6%98%af%e4%bb%80%e4%b9%88></a></h2><p><a href=https://godbolt.org/z/T8nsKWxGs title=https://godbolt.org/z/T8nsKWxGs rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://godbolt.org/z/T8nsKWxGs<i class="fa fa-external-link-alt"></i></a> CPU 上 float 和 double 使用相同的寄存器，却使用不同的指令去计算（编译器会追踪一个寄存器存储的真实类型，比如 eax 既能存 int 又能存 unsigned，xmm0 也是既能存 float 又能存 double）；CPU 上 int 和 long long 使用不同的寄存器，指令却是一样的（只区分有无符号）。</p><p><a href=https://godbolt.org/z/4P65dK8nq title=https://godbolt.org/z/4P65dK8nq rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://godbolt.org/z/4P65dK8nq<i class="fa fa-external-link-alt"></i></a> GPU 上 float 和 double 计算使用不同的寄存器和不同的指令，int 和 long long 也是用不同的寄存器和不同的指令。（double 类型实际上是用了两个 32 位寄存器？只是 PTX 虚拟汇编中用 f64 表示双精度浮点数。）</p><p>我的猜测：</p><ol><li><del>进行数值计算时，只要还没有发生存储步骤（寄存器存不下了，需要写回内存），CPU 就能用高精度的寄存器来暂存计算结果，因而精度更高</del>。这显然不对，因为单精度计算的指令和双精度计算的指令不同，即便都是在 xmm0（举例）中<strong>位表示</strong>也不同。</li><li>因为知道“GPU 上双精度计算慢”，大家写 CUDA kernel 时都刻意采用 3.14f 这样的字面量，存储结构也是用 float 类型，自然比不上 CPU 上双精度浮点数的计算精度。在 CPU 上，由于 double 和 float 的性能差异没有那么大，往往直接使用 double。</li></ol><h2 id=实验验证中间表示类型会影响计算精度>实验：验证中间表示类型会影响计算精度
<a class=header-anchor href=#%e5%ae%9e%e9%aa%8c%e9%aa%8c%e8%af%81%e4%b8%ad%e9%97%b4%e8%a1%a8%e7%a4%ba%e7%b1%bb%e5%9e%8b%e4%bc%9a%e5%bd%b1%e5%93%8d%e8%ae%a1%e7%ae%97%e7%b2%be%e5%ba%a6></a></h2><p>假定所用到的所有输入，还有返回值都是 float 类型，保证输入相同。</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#719e07>#include</span> <span style=color:#719e07>&lt;cstdio&gt;</span><span style=color:#719e07>
</span></span></span><span style=display:flex><span><span style=color:#719e07>#include</span> <span style=color:#719e07>&lt;thrust/device_vector.h&gt;</span><span style=color:#719e07>
</span></span></span><span style=display:flex><span><span style=color:#719e07></span>
</span></span><span style=display:flex><span><span style=color:#586e75>// T 是返回值/参数/字面量的类型，U 控制单精度计算还是双精度计算。
</span></span></span><span style=display:flex><span><span style=color:#586e75>// (float) 强制转换保证字面量总能被单精度浮点数表示，排除输入不同造成的差异。
</span></span></span><span style=display:flex><span><span style=color:#586e75></span><span style=color:#719e07>template</span> <span style=color:#719e07>&lt;</span><span style=color:#719e07>typename</span> U, <span style=color:#719e07>typename</span> T<span style=color:#719e07>&gt;</span> __device__ __host__ T calc(T a, T b) {
</span></span><span style=display:flex><span>  <span style=color:#586e75>// return (U)a * (float)3.4543522434343 * (float)-1.244445135 * b * a /
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>  <span style=color:#586e75>//        (float)0.7434333332 + (float)0.453535433338;
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>  <span style=color:#586e75>// 写成一行和分开写，效果一样
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>  U x <span style=color:#719e07>=</span> a;
</span></span><span style=display:flex><span>  x <span style=color:#719e07>*=</span> (<span style=color:#dc322f>float</span>)<span style=color:#2aa198>3.4543522434343</span>;
</span></span><span style=display:flex><span>  x <span style=color:#719e07>*=</span> (<span style=color:#dc322f>float</span>)<span style=color:#719e07>-</span><span style=color:#2aa198>1.244445135</span>;
</span></span><span style=display:flex><span>  x <span style=color:#719e07>*=</span> b;
</span></span><span style=display:flex><span>  x <span style=color:#719e07>*=</span> a;
</span></span><span style=display:flex><span>  x <span style=color:#719e07>/=</span> (<span style=color:#dc322f>float</span>)<span style=color:#2aa198>0.7434333332</span>;
</span></span><span style=display:flex><span>  x <span style=color:#719e07>+=</span> (<span style=color:#dc322f>float</span>)<span style=color:#2aa198>0.453535433338</span>;
</span></span><span style=display:flex><span>  <span style=color:#719e07>return</span> x;
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#719e07>template</span> <span style=color:#719e07>&lt;</span><span style=color:#719e07>typename</span> U, <span style=color:#719e07>typename</span> T<span style=color:#719e07>&gt;</span>
</span></span><span style=display:flex><span>__global__ <span style=color:#dc322f>void</span> calc_kernel(T <span style=color:#719e07>*</span>a, T <span style=color:#719e07>*</span>b, T <span style=color:#719e07>*</span>result) {
</span></span><span style=display:flex><span>  <span style=color:#719e07>*</span>result <span style=color:#719e07>=</span> calc<span style=color:#719e07>&lt;</span>U, T<span style=color:#719e07>&gt;</span>(<span style=color:#719e07>*</span>a, <span style=color:#719e07>*</span>b);
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#719e07>template</span> <span style=color:#719e07>&lt;</span><span style=color:#719e07>typename</span> U, <span style=color:#719e07>typename</span> T<span style=color:#719e07>&gt;</span> T cuda_calc(T a, T b) {
</span></span><span style=display:flex><span>  thrust<span style=color:#719e07>::</span>device_vector<span style=color:#719e07>&lt;</span>T<span style=color:#719e07>&gt;</span> v(<span style=color:#2aa198>3</span>);
</span></span><span style=display:flex><span>  v[<span style=color:#2aa198>0</span>] <span style=color:#719e07>=</span> a;
</span></span><span style=display:flex><span>  v[<span style=color:#2aa198>1</span>] <span style=color:#719e07>=</span> b;
</span></span><span style=display:flex><span>  T <span style=color:#719e07>*</span>p <span style=color:#719e07>=</span> v.data().get();
</span></span><span style=display:flex><span>  calc_kernel<span style=color:#719e07>&lt;</span>U<span style=color:#719e07>&gt;&lt;&lt;&lt;</span><span style=color:#2aa198>1</span>, <span style=color:#2aa198>1</span><span style=color:#719e07>&gt;&gt;&gt;</span>(p, p <span style=color:#719e07>+</span> <span style=color:#2aa198>1</span>, p <span style=color:#719e07>+</span> <span style=color:#2aa198>2</span>);
</span></span><span style=display:flex><span>  <span style=color:#719e07>return</span> v[<span style=color:#2aa198>2</span>];
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#719e07>template</span> <span style=color:#719e07>&lt;</span><span style=color:#719e07>typename</span> U, <span style=color:#719e07>typename</span> T<span style=color:#719e07>&gt;</span> T cpu_calc(T a, T b) {
</span></span><span style=display:flex><span>  <span style=color:#719e07>return</span> calc<span style=color:#719e07>&lt;</span>U, T<span style=color:#719e07>&gt;</span>(a, b);
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#dc322f>int</span> <span style=color:#268bd2>main</span>() {
</span></span><span style=display:flex><span>  <span style=color:#dc322f>float</span> a <span style=color:#719e07>=</span> <span style=color:#2aa198>13.42433332</span>;
</span></span><span style=display:flex><span>  <span style=color:#dc322f>float</span> b <span style=color:#719e07>=</span> <span style=color:#2aa198>3.1487223258</span>;
</span></span><span style=display:flex><span>  {
</span></span><span style=display:flex><span>    printf(<span style=color:#2aa198>&#34;cuda_calc: &#34;</span>);
</span></span><span style=display:flex><span>    <span style=color:#dc322f>float</span> x <span style=color:#719e07>=</span> cuda_calc<span style=color:#719e07>&lt;</span><span style=color:#dc322f>float</span><span style=color:#719e07>&gt;</span>(a, b);
</span></span><span style=display:flex><span>    <span style=color:#dc322f>float</span> y <span style=color:#719e07>=</span> cuda_calc<span style=color:#719e07>&lt;</span><span style=color:#dc322f>double</span><span style=color:#719e07>&gt;</span>(a, b);
</span></span><span style=display:flex><span>    <span style=color:#719e07>if</span> (x <span style=color:#719e07>==</span> y) {
</span></span><span style=display:flex><span>      printf(<span style=color:#2aa198>&#34;x == y</span><span style=color:#cb4b16>\n</span><span style=color:#2aa198>&#34;</span>);
</span></span><span style=display:flex><span>    } <span style=color:#719e07>else</span> {
</span></span><span style=display:flex><span>      printf(<span style=color:#2aa198>&#34;x - y = %.8f</span><span style=color:#cb4b16>\n</span><span style=color:#2aa198>&#34;</span>, x <span style=color:#719e07>-</span> y);
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>  {
</span></span><span style=display:flex><span>    printf(<span style=color:#2aa198>&#34;cpu_calc : &#34;</span>);
</span></span><span style=display:flex><span>    <span style=color:#dc322f>float</span> x <span style=color:#719e07>=</span> cpu_calc<span style=color:#719e07>&lt;</span><span style=color:#dc322f>float</span><span style=color:#719e07>&gt;</span>(a, b);
</span></span><span style=display:flex><span>    <span style=color:#dc322f>float</span> y <span style=color:#719e07>=</span> cpu_calc<span style=color:#719e07>&lt;</span><span style=color:#dc322f>double</span><span style=color:#719e07>&gt;</span>(a, b);
</span></span><span style=display:flex><span>    <span style=color:#719e07>if</span> (x <span style=color:#719e07>==</span> y) {
</span></span><span style=display:flex><span>      printf(<span style=color:#2aa198>&#34;x == y</span><span style=color:#cb4b16>\n</span><span style=color:#2aa198>&#34;</span>);
</span></span><span style=display:flex><span>    } <span style=color:#719e07>else</span> {
</span></span><span style=display:flex><span>      printf(<span style=color:#2aa198>&#34;x - y = %.8f</span><span style=color:#cb4b16>\n</span><span style=color:#2aa198>&#34;</span>, x <span style=color:#719e07>-</span> y);
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>代码中的 U 为 float 时使用的是单精度计算，U 为 double 时使用双精度计算。结果为：</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-c data-lang=c><span style=display:flex><span>cuda_calc: x <span style=color:#719e07>-</span> y <span style=color:#719e07>=</span> <span style=color:#2aa198>0.00024414</span>
</span></span><span style=display:flex><span>cpu_calc : x <span style=color:#719e07>-</span> y <span style=color:#719e07>=</span> <span style=color:#2aa198>0.00024414</span>
</span></span></code></pre></div><h2 id=pytorch-是怎么做的>PyTorch 是怎么做的？
<a class=header-anchor href=#pytorch-%e6%98%af%e6%80%8e%e4%b9%88%e5%81%9a%e7%9a%84></a></h2><p><a href=https://github.com/pytorch/pytorch/blob/e890d888d916b4f38b383a59e0e9445513c67313/aten/src/ATen/AccumulateType.h#L136 title=https://github.com/pytorch/pytorch/blob/e890d888d916b4f38b383a59e0e9445513c67313/aten/src/ATen/AccumulateType.h#L136 rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://github.com/pytorch/pytorch/blob/e890d888d916b4f38b383a59e0e9445513c67313/aten/src/ATen/AccumulateType.h#L136<i class="fa fa-external-link-alt"></i></a> <strong>PyTorch 在 CUDA 上计算时，如果输入是 float，中间的存储类型（<code>acc_type</code>）也用的是 float</strong>。</p><p><a href=https://github.com/pytorch/pytorch/blob/e890d888d916b4f38b383a59e0e9445513c67313/aten/src/ATen/AccumulateType.h#L155 title=https://github.com/pytorch/pytorch/blob/e890d888d916b4f38b383a59e0e9445513c67313/aten/src/ATen/AccumulateType.h#L155 rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://github.com/pytorch/pytorch/blob/e890d888d916b4f38b383a59e0e9445513c67313/aten/src/ATen/AccumulateType.h#L155<i class="fa fa-external-link-alt"></i></a> PyTorch 在 CPU 上计算时，如果输入是 float，中间的存储类型用的是 double。</p><p><a href=https://github.com/pytorch/pytorch/issues/113414 title=https://github.com/pytorch/pytorch/issues/113414 rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://github.com/pytorch/pytorch/issues/113414<i class="fa fa-external-link-alt"></i></a> （该 issue 来自 2023 年 11 月）这里说如果使用 float 作为单精度计算的累加类型，会导致 batchnorm 的单元测试失败。GPU 能过测试但 CPU 不能过测试的原因是什么呢？我写了两外一篇文章来看 torch 的代码（但也是瞎猜），见
<a href=/posts/programming/python/PyTorch-%E7%9A%84-CPU-%E8%AE%A1%E7%AE%97%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8-double-%E4%BD%9C%E4%B8%BA-32-%E4%BD%8D%E6%B5%AE%E7%82%B9%E6%95%B0%E7%9A%84%E7%B4%AF%E5%8A%A0%E7%B1%BB%E5%9E%8B/ title="PyTorch 的 CPU 计算为什么使用 double 作为 32 位浮点数的累加类型？">PyTorch 的 CPU 计算为什么使用 double 作为 32 位浮点数的累加类型？</a> 。</p><p><a href=https://github.com/pytorch/pytorch/blob/2ad011ca73f68185783ec9afeb730615769a3fca/aten/src/ATen/native/cpu/batch_norm_kernel.cpp#L226-L228 title=https://github.com/pytorch/pytorch/blob/2ad011ca73f68185783ec9afeb730615769a3fca/aten/src/ATen/native/cpu/batch_norm_kernel.cpp#L226-L228 rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://github.com/pytorch/pytorch/blob/2ad011ca73f68185783ec9afeb730615769a3fca/aten/src/ATen/native/cpu/batch_norm_kernel.cpp#L226-L228<i class="fa fa-external-link-alt"></i></a> 这里的代码注释说明了 CPU 上 batch_norm 的累加计算使用 <code>at::acc_type&lt;scalar_t, /* is_cuda */ false></code> 作为中间类型，相同的注释也多次在此文件的其他函数中出现。</p><div class="markdown-alert markdown-alert-note"><p class=markdown-alert-title><svg class="octicon octicon-info mr-2" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1116 0A8 8 0 010 8zm8-6.5a6.5 6.5.0 100 13 6.5 6.5.0 000-13zM6.5 7.75A.75.75.0 017.25 7h1a.75.75.0 01.75.75v2.75h.25a.75.75.0 010 1.5h-2a.75.75.0 010-1.5h.25v-2h-.25a.75.75.0 01-.75-.75zM8 6a1 1 0 110-2 1 1 0 010 2z"/></svg>Note</p><p>我不觉得把单精度浮点数转双精度浮点数就是误差（反而精度更高呢），但是我们老师认为 PyTorch 是深度学习的金标准，计算结果和它不一致就是误差。为了“减少误差”，写单精度的 kernel 时就要避免隐式转换到双精度。我倒觉得始终使用单精度计算只是为了速度，拿精度换速度也是合理的。</p></div><h2 id=fp32-计算精度低并不是-gpu-的原罪>FP32 计算精度低并不是 GPU 的原罪
<a class=header-anchor href=#fp32-%e8%ae%a1%e7%ae%97%e7%b2%be%e5%ba%a6%e4%bd%8e%e5%b9%b6%e4%b8%8d%e6%98%af-gpu-%e7%9a%84%e5%8e%9f%e7%bd%aa></a></h2><p>本实验中 CPU 和 GPU 的表现一致。这说明写 kernel 时将“FP32 四则运算的简单组合在 GPU 和 CPU 计算的结果不同”归咎于“GPU 就是有误差”是不对的，很可能是写法有问题。</p><div class="markdown-alert markdown-alert-note"><p class=markdown-alert-title><svg class="octicon octicon-info mr-2" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1116 0A8 8 0 010 8zm8-6.5a6.5 6.5.0 100 13 6.5 6.5.0 000-13zM6.5 7.75A.75.75.0 017.25 7h1a.75.75.0 01.75.75v2.75h.25a.75.75.0 010 1.5h-2a.75.75.0 010-1.5h.25v-2h-.25a.75.75.0 01-.75-.75zM8 6a1 1 0 110-2 1 1 0 010 2z"/></svg>Note</p><p>2024 年 12 月 10 日：我最近看到有个说法：在 TF32 出现之前，很多 GPU 制造商已经悄悄通过降低浮点数舍入前的最大精度来提高计算速度。</p><p>IEEE754 只是规定了浮点数扩展格式的最低要求，并没有对编码方式做严格规定，x86 就有一个 80 位的浮点数扩展格式（下面这个 integer part 有点特殊，详见维基百科）。这样的扩展格式一般是在中间计算过程中使用的，尤其是在 x86 指数运算时对精度的作用显得非常重要。降低中间表示的精度来提速在理论上是可行的。</p><p><img src=/assets/Pasted%20image%2020241210110959.webp></p></div><h1 id=cpu-和-gpu-上的半精度计算>CPU 和 GPU 上的半精度计算
<a class=header-anchor href=#cpu-%e5%92%8c-gpu-%e4%b8%8a%e7%9a%84%e5%8d%8a%e7%b2%be%e5%ba%a6%e8%ae%a1%e7%ae%97></a></h1><p>先看 CPU：GCC 提供了 <code>_Float16</code> 扩展。从 <a href=https://godbolt.org/z/oe154xKbc title=汇编代码 rel="noopener external nofollow noreferrer" target=_blank class=exturl>汇编代码<i class="fa fa-external-link-alt"></i></a> 可以看出该类型借助了软件支持，而且在计算时也是先转换成单精度的。所以 CPU 上刻意去用半精度会很慢。（在 C++23 还有 <code>std::float16_t</code>，不过我想也差不多。）</p><p>再看 GPU 上的半精度类型 <code>__half</code>，使用该类型需要包含头文件 cuda_fp16.h。 从 <a href=https://godbolt.org/z/8K7cME5jx title=汇编代码 rel="noopener external nofollow noreferrer" target=_blank class=exturl>汇编代码<i class="fa fa-external-link-alt"></i></a> 可以看出，半精度类型是一种用户定义类型（一个类），PTX 代码也比 float 和 double 复杂太多。这样就会得到 CUDA kernel 上半精度计算很慢的结论，和我们平时训练模型的实际感受不一致啊？</p><p>为此我找到了论坛的一个回答：<a href=https://forums.developer.nvidia.com/t/poor-half-performance/111626 title=https://forums.developer.nvidia.com/t/poor-half-performance/111626 rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://forums.developer.nvidia.com/t/poor-half-performance/111626<i class="fa fa-external-link-alt"></i></a> <em>There is no 16-bit register on the device. They are all 32-bits. To get double performance of a float, you need to use half2. Where you package two half precision variables in one register. And then use the appropriate intrinsic</em>. Moderator 表示 <code>__half</code> 类型没有对应的寄存器，所以很慢，要快的话得用 <code>__half2</code> 类型。该类型同样需要 cuda_fp16.h 头文件。根据 mod 的说法，使用 <code>__half2</code> 类型和相应的内置计算函数可以得到近似于单精度计算两倍的速度。</p><p>在 CPU 的 SIMD 计算中，是不是也可以像 GPU 一样实现把多个半精度数据打包在大的寄存器里，从而让半精度计算变快呢？我看了一下只有较新的 CPU 才在 AVX512 中支持 16 位浮点数，而以前的老 CPU 计算 16 位浮点数要转成 32 位浮点数，转换开销让向量计算得不偿失。</p><p>总结：CUDA kernel 上双精度换单精度，代码逻辑不需要改，速度就能提升。单精度换半精度，就要使用 <code>__half2</code> 类型和一些内置函数了，写起来会稍微麻烦一点。</p><h1 id=tensor-core-和混合精度计算>Tensor Core 和混合精度计算
<a class=header-anchor href=#tensor-core-%e5%92%8c%e6%b7%b7%e5%90%88%e7%b2%be%e5%ba%a6%e8%ae%a1%e7%ae%97></a></h1><p>从前面的实验我们可以看出：中间表示类型的不同会影响计算的类型，从而影响计算精度和计算速度。混合精度计算就是为了提高计算速度，使用了精度更小的中间表示类型。</p><p><a href=https://pytorch.org/docs/stable/notes/numerical_accuracy.html title="PyTorch 的一篇文档" rel="noopener external nofollow noreferrer" target=_blank class=exturl>PyTorch 的一篇文档<i class="fa fa-external-link-alt"></i></a> 描述了 PyTorch 在什么场合下会有计算差异。其中提到 <em>TensorFloat-32 (TF32) on Nvidia Ampere (and later) devices</em> 和 <em>Reduced Precision Reduction for FP16 and BF16 GEMM</em>。后者是 FP16 计算，在很多 GPU 上都有支持；前者是 TF32 计算，最早出现在 A100 的第三代 Tensor Core 中。</p><h2 id=v100-的第一代-tensor-core-支持了-fp16fp32-的混合精度计算>V100 的第一代 Tensor Core 支持了 FP16+FP32 的混合精度计算
<a class=header-anchor href=#v100-%e7%9a%84%e7%ac%ac%e4%b8%80%e4%bb%a3-tensor-core-%e6%94%af%e6%8c%81%e4%ba%86-fp16fp32-%e7%9a%84%e6%b7%b7%e5%90%88%e7%b2%be%e5%ba%a6%e8%ae%a1%e7%ae%97></a></h2><p>Tensor Core 在 <a href=https://en.wikipedia.org/wiki/Volta_%28microarchitecture%29 title="NVIDIA Volta" rel="noopener external nofollow noreferrer" target=_blank class=exturl>NVIDIA Volta<i class="fa fa-external-link-alt"></i></a> 上就已经出现了，本质就是以混合精度计算提高矩阵乘法速度。第一代 Tensor Core 使用 FP16+FP32 加速计算。附上演化史：</p><p>这张图信息非常多，不仅说明了很多数据类型的计算是在什么时候被支持的，还说明了 Tensor Core 是在 Volta 架构开始支持的。<strong>Tensor Core 一直都不支持 FP32 计算</strong>！？</p><p><img src=/assets/Pasted%20image%2020250123132915.webp></p><p>更详细的计算支持可以参见 <a href=https://en.wikipedia.org/wiki/CUDA#Data_types title=CUDA#Data_types rel="noopener external nofollow noreferrer" target=_blank class=exturl>CUDA#Data_types<i class="fa fa-external-link-alt"></i></a> 和 <a href=https://en.wikipedia.org/wiki/CUDA#Tensor_cores title=CUDA#Tensor_cores rel="noopener external nofollow noreferrer" target=_blank class=exturl>CUDA#Tensor_cores<i class="fa fa-external-link-alt"></i></a> 。H100 属于 Hopper 架构。RTX 3090 和 A100 属于 Ampere 架构。RTX 4090 属于 Ada 架构，表上没有。RTX 4090 计算能力是 8.9，从维基百科见其相比 Hopper 的 Tensor Core 多了 INT1 和 INT4 支持。FP64 表上写的是 <code>speed tbd</code>。<a href=https://images.nvidia.com/aem-dam/Solutions/Data-Center/l4/nvidia-ada-gpu-architecture-whitepaper-v2.1.pdf title="NVIDIA Ada 架构白皮书" rel="noopener external nofollow noreferrer" target=_blank class=exturl>NVIDIA Ada 架构白皮书<i class="fa fa-external-link-alt"></i></a> 第 24 页提到 4090 支持了 FP16 / BF16 / INT4 / INT8 / TF32，没有提到 INT1。第 7 页提到 Ada AD102 GPU 含有少量的 FP64 Core，计算性能低，但是为程序提供了兼容支持。AD102 就是 RTX 4090。</p><blockquote><p>Note: The AD102 GPU also includes 288 FP64 Cores (2 per SM) which are not depicted in the above diagram. The FP64 TFLOP rate is 1/64th the TFLOP rate of FP32 operations. The small number of FP64 Cores are included to ensure any programs with FP64 code operate correctly, <strong>including FP64 Tensor Core code</strong>.</p></blockquote><h2 id=a100-的第三代-tensor-core-新增了-tf32-类型>A100 的第三代 Tensor Core 新增了 TF32 类型
<a class=header-anchor href=#a100-%e7%9a%84%e7%ac%ac%e4%b8%89%e4%bb%a3-tensor-core-%e6%96%b0%e5%a2%9e%e4%ba%86-tf32-%e7%b1%bb%e5%9e%8b></a></h2><p>TF32 类型是什么呢？<a href=https://blogs.nvidia.com/blog/tensorfloat-32-precision-format/ title=英伟达的博客 rel="noopener external nofollow noreferrer" target=_blank class=exturl>英伟达的博客<i class="fa fa-external-link-alt"></i></a> 提到 TF32 类型是 8 位指数和 10 位尾数，它的指数位数和 FP32 一致，而尾数位数和 FP16 一致。</p><blockquote><p>TF32 Tensor Cores operate on FP32 inputs and produce results in FP32. Non-matrix operations continue to use FP32.</p></blockquote><p>也就是说，第三代 Tensor Core 可以用 TF32+FP32 混合精度计算矩阵乘法，即中间表示会从单精度浮点数缩小为 TF32，在提升速度的同时还能保持和 FP32 几乎一样大的表数范围。</p><p><img alt="图片，来自 https://blogs.nvidia.com/blog/tensorfloat-32-precision-format/" src=/assets/Pasted%20image%2020240810114104.webp></p><p>根据英伟达的文档 <a href=https://developer.nvidia.com/automatic-mixed-precision title=https://developer.nvidia.com/automatic-mixed-precision rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://developer.nvidia.com/automatic-mixed-precision<i class="fa fa-external-link-alt"></i></a> ，A100 还是能够使用 FP16 来进行混合精度计算，只是提供了 TF32 这种表数范围更大的新选择（比 FP16 慢）：</p><blockquote><p>On Ampere GPUs, automatic mixed precision uses FP16 to deliver a performance boost of 3X versus TF32, the new format which is already ~6x faster than FP32.</p></blockquote><h2 id=tf32-会带来什么样的改变>TF32 会带来什么样的改变？
<a class=header-anchor href=#tf32-%e4%bc%9a%e5%b8%a6%e6%9d%a5%e4%bb%80%e4%b9%88%e6%a0%b7%e7%9a%84%e6%94%b9%e5%8f%98></a></h2><p><strong>TF32 有和 FP32 相同数量的指数位，是被设计来平替 FP32 的</strong>。因此，一些 NVIDIA 计算库的默认计算方式有变化。在没有 TF32 的时候，想要使用 FP16 混合精度计算需要显式设置计算模式，因此不会无缘无故发生精度损失；TF32 出现之后就不一样了！</p><div class="markdown-alert markdown-alert-note"><p class=markdown-alert-title><svg class="octicon octicon-info mr-2" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1116 0A8 8 0 010 8zm8-6.5a6.5 6.5.0 100 13 6.5 6.5.0 000-13zM6.5 7.75A.75.75.0 017.25 7h1a.75.75.0 01.75.75v2.75h.25a.75.75.0 010 1.5h-2a.75.75.0 010-1.5h.25v-2h-.25a.75.75.0 01-.75-.75zM8 6a1 1 0 110-2 1 1 0 010 2z"/></svg>Note</p><p>TF32 只能平替 FP32（不会替换 FP16 或者 FP64），场合包括卷积、矩阵乘法，不包括优化器和求解器。</p></div><p>根据 <a href=https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/ title=https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/ rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/<i class="fa fa-external-link-alt"></i></a> ：</p><ol><li><strong>cuDNN</strong>：在进行 FP32 计算时，如果有 TF32 卷积 kernel 可用则使用 TF32，否则使用 FP32。</li><li><strong>cuBLAS</strong>：cuBLAS 被广泛用于需要 FP32 精度的场合（因为 cuBLAS 是线性代数库，并不是深度学习计算库），所以默认值还是 FP32，但是可以通过一些设置来启用 TF32 计算模式。为英伟达进行优化过的深度神经网络计算框架（原文是 <em>NVIDIA optimized deep learning frameworks</em>，我的理解是 PyTorch、Tensorflow 这类框架）会根据当前的计算任务调用 <code>cublasSetMathMode()</code>，以在一部分场合优选 TF32 计算、在其他场合仍然使用 FP32 计算。</li><li>cuSOLVER：默认值没有变化，总是会使用 API 参数中指定的精度。</li><li>cuTENSOR：默认值没有变化，总是会使用 API 参数中指定的精度。</li></ol><p>该文章的结论是：尽管 FP16/BF16 混合精度仍然是最快的选择，TF32 使得单精度计算可以得到隐式的加速。</p><h2 id=我们如何使用混合精度计算>我们如何使用混合精度计算？
<a class=header-anchor href=#%e6%88%91%e4%bb%ac%e5%a6%82%e4%bd%95%e4%bd%bf%e7%94%a8%e6%b7%b7%e5%90%88%e7%b2%be%e5%ba%a6%e8%ae%a1%e7%ae%97></a></h2><p>要在 PyTorch 中使用混合精度，可以用 <code>torch.amp</code>，其中 amp 是自动混合精度的意思。</p><p>要在 CUDA C++ 中利用混合精度计算，可以选择 cuBLAS、cuDNN 等 NVIDIA 库。它们有混合精度计算的支持。</p><p>要在 CUDA kernel 中利用 Tensor Core 进行混合精度计算，需要显式调用一些基础 API，包括引入头文件 mma.h 和使用 <code>nvcuda::wmma</code> 名字空间下的一些函数。可以参考 <a href=https://leimao.github.io/blog/NVIDIA-Tensor-Core-Programming/ title=https://leimao.github.io/blog/NVIDIA-Tensor-Core-Programming/ rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://leimao.github.io/blog/NVIDIA-Tensor-Core-Programming/<i class="fa fa-external-link-alt"></i></a> 。CUDA C++ WMMA API 的调用级别是 warp，而不是每线程。</p><p><em>GPU 如何协调 CUDA Core 和 Tensor Core 之间的任务呢？</em> 我的理解是：每个 block 是整体派发给 SM 的（当然一个 SM 有可能被分配多个 blocks），然后 SM 分成多个 SMSPs（<strong>SM Sub-Partitions</strong>，SP 不是流处理器），而 CUDA Core（在 AMD GPU 上叫做 SP，Streaming Processor）和 Tensor Core 的地位差不多是算术运算单元，都在 SMSP 的内部。<strong>由 SMSP 内部的 warp scheduler 来协调一个 warp 要么使用 CUDA Core 来进行 SIMT 计算，要么使用 Tensor Core 来进行矩阵计算</strong>。CUDA Core 的数量对应于一次能进行的浮点数 / 整数运算最大数量，它不是完整的线程执行器，而只是计算单元而已。A100 白皮书 22 页显示 A100 每个 SM 有 4 个 SMSP，但是一个 SM 最大支持 64 个 warps，所以 A100 的一个 SMSP 上可以最多有 16 个 warps，这说明 1 个 warp scheduler 可以同时调度多个 warps。参考：</p><ol><li><a href=https://stackoverflow.com/questions/16986770/cuda-cores-vs-thread-count title=https://stackoverflow.com/questions/16986770/cuda-cores-vs-thread-count rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://stackoverflow.com/questions/16986770/cuda-cores-vs-thread-count<i class="fa fa-external-link-alt"></i></a></li><li><a href="https://www.bilibili.com/video/BV1pL41187FH?t=288.0" title="B 站视频：NVIDIA英伟达Tensor Core架构发展(中)【AI芯片】GPU架构05" rel="noopener external nofollow noreferrer" target=_blank class=exturl>B 站视频：NVIDIA英伟达Tensor Core架构发展(中)【AI芯片】GPU架构05<i class="fa fa-external-link-alt"></i></a></li><li><a href="https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf#page=43" title="Ampere 白皮书：Table 5. Compute Capability: GP100 vs GV100 vs GA100" rel="noopener external nofollow noreferrer" target=_blank class=exturl>Ampere 白皮书：Table 5. Compute Capability: GP100 vs GV100 vs GA100<i class="fa fa-external-link-alt"></i></a></li><li><a href="https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf#page=22" title="Ampere 白皮书：SM 架构图" rel="noopener external nofollow noreferrer" target=_blank class=exturl>Ampere 白皮书：SM 架构图<i class="fa fa-external-link-alt"></i></a></li><li><a href="https://www.advancedclustering.com/wp-content/uploads/2022/03/gtc22-whitepaper-hopper.pdf#page=21" title="Hopper 白皮书：SM 架构图" rel="noopener external nofollow noreferrer" target=_blank class=exturl>Hopper 白皮书：SM 架构图<i class="fa fa-external-link-alt"></i></a></li></ol><p><a href=https://www.reddit.com/r/CUDA/comments/qk9rbs/i_dont_understand_how_cuda_kernel_works_within/ title=https://www.reddit.com/r/CUDA/comments/qk9rbs/i_dont_understand_how_cuda_kernel_works_within/ rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://www.reddit.com/r/CUDA/comments/qk9rbs/i_dont_understand_how_cuda_kernel_works_within/<i class="fa fa-external-link-alt"></i></a> <em>The TensorCores are specialized execution units within the SM.</em></p><p><a href=https://forums.developer.nvidia.com/t/concurrent-execution-of-cuda-and-tensor-cores/222985/8 title=https://forums.developer.nvidia.com/t/concurrent-execution-of-cuda-and-tensor-cores/222985/8 rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://forums.developer.nvidia.com/t/concurrent-execution-of-cuda-and-tensor-cores/222985/8<i class="fa fa-external-link-alt"></i></a> 该回复建议参阅 <a href=https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf title="A100 白皮书" rel="noopener external nofollow noreferrer" target=_blank class=exturl>A100 白皮书<i class="fa fa-external-link-alt"></i></a>，可以看到 Tensor Core 位于 SM 里面。该文档还指出 A100 有 128 个 SMs，每个 SM 有 4 个 Tensor Cores 和 64 个 FP32 CUDA Cores。下面几张图放的是 A100 的技术规格。（2025/3/19 V100 的老一代 Tensor Core 在一个 SM 中有 8 个，可查白皮书，A100 减少了单 SM 中 Tensor Core 的数量。）</p><p><img src=/assets/Pasted%20image%2020240810132927.webp width=700></p><p><img src=/assets/Pasted%20image%2020240810131538.webp width=800></p><ul><li>共享存储部分：纹理内存、L1 指令缓存、L1 数据缓存。<ul><li>L1 数据缓存和共享内存是共用一个存储器件的，此多彼少，<a href="https://developer.nvidia.com/blog/using-shared-memory-cuda-cc/#:~:text=Configuring%20the%20amount%20of%20shared%20memory" title="共享内存的占用大小可以通过 CUDA API 配置" rel="noopener external nofollow noreferrer" target=_blank class=exturl>共享内存的占用大小可以通过 CUDA API 配置<i class="fa fa-external-link-alt"></i></a>。</li></ul></li><li>子单元：SMSP<ul><li>控制部分：<ul><li>Warp 调度器</li><li>指令派发器。warp 调度器选择 warp（任务）来执行，指令派发器负责具体任务的指令发射。</li></ul></li><li>存储部分：<ul><li>L0 <strong>指令</strong> cache（注意是指令的 cache）</li><li>LD/ST：存储加载单元</li><li>寄存器文件</li></ul></li><li>计算部分：<ul><li>整型和浮点运算单元（CUDA Core 数量反映于 FP32 计算单元的数量，INT32 计算单元数量与其相同；FP64 少于，甚至可能显著少于 FP32 单元的数量）<blockquote><p>🤔 我觉得可以理解为线程 = CUDA Core + 每线程的维护数据。然后 <del>CUDA Core</del> 线程以 warp 形式组织。</p></blockquote></li><li>Tensor Core</li><li>SFU（特殊函数计算单元，比如 sin/tan/exp/log/sqrt 等数学函数）</li></ul></li></ul></li></ul><p><a href="https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf#page=20" title="A100 白皮书第 20 页" rel="noopener external nofollow noreferrer" target=_blank class=exturl>A100 白皮书第 20 页<i class="fa fa-external-link-alt"></i></a> 画了 A100 更完整的图，可以看到 SM 一下子变得很小、很多。图片有点大放在这里也看不清楚，最好还是去链接里面看原图。</p><p><img src=/posts/programming/cuda/assets/Pasted%20image%2020250308221429.webp></p><p>第 19 页提到：</p><blockquote><p>The NVIDIA GA100 GPU is composed of multiple GPU Processing Clusters (GPCs), Texture Processing Clusters (TPCs), Streaming Multiprocessors (SMs), and HBM2 memory controllers.</p><p>The full implementation of the GA100 GPU includes the following units:</p><ul><li>8 GPCs, 8 TPCs/GPC, 2 SMs/TPC, 16 SMs/GPC, 128 SMs per full GPU</li><li>64 FP32 CUDA Cores/SM, 8192 FP32 CUDA Cores per full GPU</li><li>4 Third-generation Tensor Cores/SM, 512 Third-generation Tensor Cores per full GPU</li><li>6 HBM2 stacks, 12 512-bit Memory Controllers</li></ul></blockquote><p>后面还提到了另外一个版本（The NVIDIA A100 Tensor Core GPU implementation of the GA100 GPU）的配置，看起来像是缩水版。</p></div><footer class=post-footer><div class=post-tags><a href=/tags/cuda>cuda</a></div><div class=addthis_inline_share_toolbox style=text-align:center></div><hr><div class=post-nav><div class="post-nav-next post-nav-item"><a href=/posts/programming/python/PyTorch-%E7%9A%84-CPU-%E8%AE%A1%E7%AE%97%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8-double-%E4%BD%9C%E4%B8%BA-32-%E4%BD%8D%E6%B5%AE%E7%82%B9%E6%95%B0%E7%9A%84%E7%B4%AF%E5%8A%A0%E7%B1%BB%E5%9E%8B/ rel=next title="PyTorch 的 CPU 计算为什么使用 double 作为 32 位浮点数的累加类型？"><i class="fa fa-chevron-left"></i> PyTorch 的 CPU 计算为什么使用 double 作为 32 位浮点数的累加类型？</a></div><div class="post-nav-prev post-nav-item"><a href=/posts/programming/cpp/%E8%A6%81%E5%B0%8F%E5%BF%83-C++-%E9%9D%99%E6%80%81%E5%88%9D%E5%A7%8B%E5%8C%96%E9%A1%BA%E5%BA%8F/ rel=prev title="要小心 C++ 静态初始化顺序">要小心 C++ 静态初始化顺序
<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>&copy;
<span itemprop=copyrightYear>2023 - 2025
</span><span class=with-love><i class="fa fa-heart"></i>
</span><span class=author itemprop=copyrightHolder>🤖</span></div><div class=powered-by>由 <a href=https://gohugo.io title=0.143.1 target=_blank>Hugo</a> & <a href=https://github.com/hugo-next/hugo-theme-next title=4.5.3 target=_blank>Hugo NexT.Gemini</a> 强力驱动</div></div></footer><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js defer></script><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.1.0/medium-zoom.min.js defer></script><script class=next-config data-name=main type=application/json>{"bookmark":{"color":"#222","enable":false,"save":"manual"},"copybtn":true,"darkmode":false,"hostname":"https://hxhue.github.io/","i18n":{"ds_day":" 天前","ds_days":" 天 ","ds_hour":" 小时前","ds_hours":" 小时 ","ds_just":"刚刚","ds_min":" 分钟前","ds_mins":" 分钟","ds_month":" 个月前","ds_years":" 年 ","empty":"没有找到任何搜索结果：${query}","hits":"找到 ${hits} 个搜索结果","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","placeholder":"搜索..."},"lang":"zh-CN","lazyload":false,"localSearch":{"enable":true,"limit":1e3,"path":"/searchindexes.xml","preload":false,"topnperarticle":-1,"trigger":"auto","unescape":false},"motion":{"async":true,"enable":false,"transition":{"collheader":"fadeInLeft","menu_item":"fadeInDown","postblock":"fadeIn","postbody":"fadeInDown","postheader":"fadeInDown","sidebar":"fadeInUp"}},"postmeta":{"comments":{"enable":false,"plugin":"waline"},"views":{"enable":false,"plugin":"busuanzi"}},"root":"/","scheme":"Gemini","sidebar":{"display":"post","offset":12,"padding":18,"position":"left","width":256},"vendor":{"plugins":"cdnjs","router":"https://cdnjs.cloudflare.com/ajax/libs"},"version":"4.5.3"}</script><script type=text/javascript src=/js/main.min.37ba8b54f9d4d784d08028c45eea93b5d4e13eda8ee7fb0d2edd6f3fac66cfd2.js defer></script></body></html>