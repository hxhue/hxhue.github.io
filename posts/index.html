<!doctype html><html lang=zh-CN data-theme=light><head><meta charset=UTF-8><meta name=viewport content="width=device-width"><meta name=theme-color content="#222" media="(prefers-color-scheme: light)"><meta name=generator content="Hugo 0.143.1"><link rel="shortcut icon" type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=/imgs/icons/favicon_16x16_next.png><link rel=icon type=image/png sizes=32x32 href=/imgs/icons/favicon_32_32_next.png><link rel=apple-touch-icon sizes=180x180 href=/imgs/icons/apple_touch_icon_next.png><meta itemprop=name content="Posts"><meta itemprop=description content="个人博客，主要是零散的笔记。"><meta itemprop=image content="https://hxhue.github.io/imgs/371907.jpg"><meta itemprop=keywords content><link type=text/css rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.2/css/all.min.css><link type=text/css rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css><link rel=stylesheet href=/css/main.min.bea76f574a755574e17d42bea39502a74ca3ca4db65807b8c82d3e26dcec8420.css><style type=text/css>.post-footer,.flinks-list-footer hr:after{content:"~ 我可是有底线的哟 ~"}</style><link rel=stylesheet type=text/css href=/css/style.css><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/github-markdown-css@5.3.0/github-markdown-dark.css><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js></script><script>MathJax={tex:{displayMath:[["$$","$$"],["\\[","\\]"]],inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"}}</script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.esm.min.mjs';
  mermaid.initialize({ startOnLoad: false });
  mermaid.mermaidAPI.initialize();
  window.mermaid = mermaid;
</script><script type=text/javascript>(function(){localDB={set:function(e,t,n){if(n===0)return;const s=new Date,o=n*864e5,i={value:t,expiry:s.getTime()+o};localStorage.setItem(e,JSON.stringify(i))},get:function(e){const t=localStorage.getItem(e);if(!t)return void 0;const n=JSON.parse(t),s=new Date;return s.getTime()>n.expiry?(localStorage.removeItem(e),void 0):n.value}},theme={active:function(){const e=localDB.get("theme");if(e==null)return;theme.toggle(e),window.matchMedia("(prefers-color-scheme: dark)").addListener(function(e){theme.toggle(e.matches?"dark":"light")})},toggle:function(e){document.documentElement.setAttribute("data-theme",e),localDB.set("theme",e,2);const t=document.querySelector("iframe.giscus-frame");if(t){const n={setConfig:{theme:e}};t.contentWindow.postMessage({giscus:n},"https://giscus.app")}}},theme.active()})(window)</script><script class=next-config data-name=page type=application/json>{"comments":false,"isHome":false,"isPage":false,"path":"posts","permalink":"https://hxhue.github.io/posts/","title":"Posts","waline":{"js":[{"alias":"waline","alias_name":"@waline/client","file":"dist/pageview.js","name":"pageview","version":"2.13.0"},{"alias":"waline","alias_name":"@waline/client","file":"dist/comment.js","name":"comment","version":"2.13.0"}]}}</script><script type=text/javascript>document.addEventListener("DOMContentLoaded",()=>{var e=document.createElement("script");e.charset="UTF-8",e.src="",e.async=!1,e.defer=!0,document.head.appendChild(e),e.onload=function(){NexT.utils.fmtBusuanzi()}})</script><title>posts - Bluegill</title><noscript><link rel=stylesheet href=/css/noscript.css></noscript></head><body itemscope itemtype=http://schema.org/WebPage><div class=headband></div><main class=main><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div class=toggle aria-label role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div></div><div class=site-meta><a href=/ class=brand rel=start><i class=logo-line></i><h1 class=site-title>Bluegill</h1><i class=logo-line></i></a><p class=site-subtitle itemprop=description></p></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href=/ class=hvr-icon-pulse rel=section><i class="fa fa-home hvr-icon"></i>首页</a></li><li class="menu-item menu-item-about"><a href=/about/ class=hvr-icon-pulse rel=section><i class="fa fa-user hvr-icon"></i>关于</a></li><li class="menu-item menu-item-archives"><a href=/archives/ class=hvr-icon-pulse rel=section><i class="fa fa-archive hvr-icon"></i>归档</a></li><li class="menu-item menu-item-categories"><a href=/categories/ class=hvr-icon-pulse rel=section><i class="fa fa-th hvr-icon"></i>分类</a></li><li class="menu-item menu-item-tags"><a href=/tags/ class=hvr-icon-pulse rel=section><i class="fa fa-hashtag hvr-icon"></i>标签</a></li><li class="menu-item menu-item-daily"><a href=/daily/ class=hvr-icon-pulse rel=section><i class="fa fa-newspaper hvr-icon"></i>随笔</a></li><li class="menu-item menu-item-discovery"><a href=https://rift-fear-f2c.notion.site/2025-1e354a33cfb1802c841bdf29f2f3dab3 class=hvr-icon-pulse rel=section><i class="fa fa-compass hvr-icon"></i>发现</a></li><li class="menu-item menu-item-search"><a role=button class="popup-trigger hvr-icon-pulse"><i class="fa fa-search fa-fw hvr-icon"></i>搜索</a></li></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon><i class="fa fa-search"></i></span><div class=search-input-container><input autocomplete=off autocapitalize=off maxlength=80 placeholder=搜索... spellcheck=false type=search class=search-input></div><span class=popup-btn-close role=button><i class="fa fa-times-circle"></i></span></div><div class="search-result-container no-result"><div class=search-result-icon><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></div><div class="toggle sidebar-toggle" role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div><aside class=sidebar><div class="sidebar-inner sidebar-overview-active"><ul class=sidebar-nav><li class=sidebar-nav-toc>文章目录</li><li class=sidebar-nav-overview>站点概览</li></ul><div class=sidebar-panel-container><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><nav id=TableOfContents></nav></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author site-overview-item animated" itemprop=author itemscope itemtype=http://schema.org/Person><img class=site-author-image itemprop=image alt=🤖 src=/imgs/371907.jpg><p class=site-author-name itemprop=name>🤖</p><div class=site-description itemprop=description>个人博客，主要是零散的笔记。</div></div><div class="site-state-wrap site-overview-item animated"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/><span class=site-state-item-count>433</span>
<span class=site-state-item-name>日志</span></a></div><div class="site-state-item site-state-categories"><a href=/categories/><span class=site-state-item-count>12</span>
<span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/><span class=site-state-item-count>86</span>
<span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-social site-overview-item animated"><span class=links-of-social-item><a href=https://github.com/hxhue title="Github → https://github.com/hxhue" rel=noopener target=_blank><i class="fab fa-github fa-fw"></i>
Github
</a></span><span class=links-of-social-item><a href=/rss.xml title="RSS 订阅 → /rss.xml" rel=noopener target=_blank><i class="fa fa-rss fa-fw"></i>
RSS 订阅</a></span></div><div class="cc-license animated" itemprop=license><a href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh class=cc-opacity rel=noopener target=_blank title=共享知识><img src=/imgs/cc/big/by_nc_sa.svg alt=共享知识></a></div><div class="links-of-blogroll site-overview-item animated"><div class=links-of-blogroll-title><i class="fa fa-globe fa-fw"></i>
友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=https://shuai.guru/ title=https://shuai.guru/ target=_blank>shuai.guru</a></li></ul></div></div></div></div></aside><div class=sidebar-dimmer></div></header><div class=tool-buttons><div id=toggle-theme class=button title=深浅模式切换><i class="fas fa-adjust"></i></div><div class=back-to-top role=button title=返回顶部><i class="fa fa-arrow-up"></i>
<span>0%</span></div></div><div class=reading-progress-bar></div><noscript><div class=noscript-warning>Theme NexT works best with JavaScript enabled</div></noscript><div class="main-inner index posts-expand"><div class=post-block><article itemscope itemtype=http://schema.org/Article class=post-content lang><link itemprop=mainEntityOfPage href=https://hxhue.github.io/posts/ai-infra/CUDA_DEVICE_MAX_CONNECTIONS/><span hidden itemprop=author itemscope itemtype=http://schema.org/Person><meta itemprop=image content="/imgs/371907.jpg"><meta itemprop=name content="🤖"></span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization><meta itemprop=name content="🤖"><meta itemprop=description content="个人博客，主要是零散的笔记。"></span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork><meta itemprop=name content="CUDA_DEVICE_MAX_CONNECTIONS"><meta itemprop=description content="环境变量解释

CUDA_DEVICE_MAX_CONNECTIONS 这个环境变量在 Megatron-LM 源码中经常见到，主要是用来控制硬件调度 kernel 的顺序，以尽可能提高通信计算重叠场景的重叠率。
英伟达文档 cuda-environment-variables 说明了 CUDA_DEVICE_MAX_CONNECTIONS 默认为 8，可以设置为 1 到 32 的整数值，表示计算和拷贝队列的数量。（同样还有 CUDA_DEVICE_MAX_COPY_CONNECTIONS 表示对拷贝队列的设置（优先级高于 CUDA_DEVICE_MAX_CONNECTION）。）论坛回复 指出这个环境变量表示有多少个硬件队列和 CUDA 流发生映射关系。（如果这个值不够大则流之间存在虚假依赖。这样一来，设置为 1 就能完全保证 kernel 执行顺序和发起顺序一致。设置为 32 则尽可能设法让不同的流并发执行 kernels。）"></span><header class=post-header><h2 class=post-title itemprop="name headline"><a href=/posts/ai-infra/CUDA_DEVICE_MAX_CONNECTIONS/ itemprop=url class=post-title-link>CUDA_DEVICE_MAX_CONNECTIONS</a></h2><div class=post-meta-container><div class=post-meta-items><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar"></i>
</span><span class=post-meta-item-text title=发表于>发表于：
</span><time title="创建时间：2025-09-07 00:00:00 +0800 CST" itemprop="dateCreated datePublished" datetime="2025-09-07 00:00:00 +0800 CST">2025-09-07</time></span></div></div></header><div class=post-body itemprop=articleBody><h1 id=环境变量解释>环境变量解释
<a class=header-anchor href=#%e7%8e%af%e5%a2%83%e5%8f%98%e9%87%8f%e8%a7%a3%e9%87%8a></a></h1><p><code>CUDA_DEVICE_MAX_CONNECTIONS</code> 这个环境变量在 Megatron-LM 源码中经常见到，主要是用来控制硬件调度 kernel 的顺序，以尽可能提高通信计算重叠场景的重叠率。</p><p><a href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cuda-environment-variables title="英伟达文档 cuda-environment-variables" rel="noopener external nofollow noreferrer" target=_blank class=exturl>英伟达文档 cuda-environment-variables<i class="fa fa-external-link-alt"></i></a> 说明了 <code>CUDA_DEVICE_MAX_CONNECTIONS</code> 默认为 8，可以设置为 1 到 32 的整数值，表示计算和拷贝队列的数量。（同样还有 <code>CUDA_DEVICE_MAX_COPY_CONNECTIONS</code> 表示对拷贝队列的设置（优先级高于 <code>CUDA_DEVICE_MAX_CONNECTION</code>）。）<a href=https://forums.developer.nvidia.com/t/cuda-device-max-connections-and-pci-e-traffic/262962/2 title=论坛回复 rel="noopener external nofollow noreferrer" target=_blank class=exturl>论坛回复<i class="fa fa-external-link-alt"></i></a> 指出这个环境变量表示有多少个硬件队列和 CUDA 流发生映射关系。（如果这个值不够大则流之间存在虚假依赖。这样一来，设置为 1 就能完全保证 kernel 执行顺序和发起顺序一致。设置为 32 则尽可能设法让不同的流并发执行 kernels。）</p><h1 id=megatron-lm-1f1b--ep-a2a-overlap>Megatron-LM 1F1B + EP A2A overlap
<a class=header-anchor href=#megatron-lm-1f1b--ep-a2a-overlap></a></h1><p>Megatron-LM 2025.8.1 的提交支持了 1F1B + EP A2A overlap，也对 <code>CUDA_DEVICE_MAX_CONNECTIONS</code> 环境变量有描述。</p><p><a href=https://github.com/NVIDIA/Megatron-LM/blame/d338252b864bf3f24fea0e2f087b73f4f8de7b16/megatron/training/arguments.py#L850 title=代码 rel="noopener external nofollow noreferrer" target=_blank class=exturl>代码<i class="fa fa-external-link-alt"></i></a>：</p><p><img src=/posts/ai-infra/assets/Pasted%20image%2020250907124026.webp></p></div><footer class=post-footer><div readmore=true><div class=readmore-overlay><div class=readmore-overlay-center><div class=post-button><a class=btn href=/posts/ai-infra/CUDA_DEVICE_MAX_CONNECTIONS/#more rel=contents>阅读全文 &#187;</a></div></div></div><div class=no-more-to-read><hr><center>没有啦~</center></div></div><div class=post-eof></div></footer></article></div><div class=post-block><article itemscope itemtype=http://schema.org/Article class=post-content lang><link itemprop=mainEntityOfPage href=https://hxhue.github.io/posts/ai-infra/Megatron-LM-%E8%AE%BA%E6%96%87-PP-%E5%92%8C-VPP-%E7%94%BB%E6%B3%95%E7%96%91%E9%97%AE/><span hidden itemprop=author itemscope itemtype=http://schema.org/Person><meta itemprop=image content="/imgs/371907.jpg"><meta itemprop=name content="🤖"></span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization><meta itemprop=name content="🤖"><meta itemprop=description content="个人博客，主要是零散的笔记。"></span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork><meta itemprop=name content="Megatron-LM 论文 PP 和 VPP 画法疑问"><meta itemprop=description content="问题

见到过两种画法：一是 device 1 调度到 PP*VPP 个微批次之后继续调度，二是 device 1 调度到 PP*VPP 微批次之后就停下了。
来源 1：Megatron-LM 论文

这个是 Megatron 的论文 Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM 第 3 页 https://arxiv.org/pdf/2104.04473 ，同时显示了交错和非交错状态下 1F1B 的调度方式。"></span><header class=post-header><h2 class=post-title itemprop="name headline"><a href=/posts/ai-infra/Megatron-LM-%E8%AE%BA%E6%96%87-PP-%E5%92%8C-VPP-%E7%94%BB%E6%B3%95%E7%96%91%E9%97%AE/ itemprop=url class=post-title-link>Megatron-LM 论文 PP 和 VPP 画法疑问</a></h2><div class=post-meta-container><div class=post-meta-items><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar"></i>
</span><span class=post-meta-item-text title=发表于>发表于：
</span><time title="创建时间：2025-09-07 00:00:00 +0800 CST" itemprop="dateCreated datePublished" datetime="2025-09-07 00:00:00 +0800 CST">2025-09-07</time></span></div></div></header><div class=post-body itemprop=articleBody><h1 id=问题>问题
<a class=header-anchor href=#%e9%97%ae%e9%a2%98></a></h1><p>见到过两种画法：一是 device 1 调度到 PP*VPP 个微批次之后继续调度，二是 device 1 调度到 PP*VPP 微批次之后就停下了。</p><h1 id=来源-1megatron-lm-论文>来源 1：Megatron-LM 论文
<a class=header-anchor href=#%e6%9d%a5%e6%ba%90-1megatron-lm-%e8%ae%ba%e6%96%87></a></h1><p>这个是 Megatron 的论文 Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM 第 3 页 <a href=https://arxiv.org/pdf/2104.04473 title=https://arxiv.org/pdf/2104.04473 rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://arxiv.org/pdf/2104.04473<i class="fa fa-external-link-alt"></i></a> ，同时显示了交错和非交错状态下 1F1B 的调度方式。</p><p><img src=/posts/ai-infra/megatron-lm-pp-vpp-figures/Pasted%20image%2020250907122234.webp></p><h1 id=来源-2pangu-ultra-论文>来源 2：Pangu Ultra 论文
<a class=header-anchor href=#%e6%9d%a5%e6%ba%90-2pangu-ultra-%e8%ae%ba%e6%96%87></a></h1><p>这个是盘古的论文 Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs 第 7 页 <a href=https://arxiv.org/pdf/2504.07866 title=https://arxiv.org/pdf/2504.07866 rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://arxiv.org/pdf/2504.07866<i class="fa fa-external-link-alt"></i></a> 。无论是交错还是非交错 1F1B 均和 Megatron 论文画法有差异。</p><p><img src=/posts/ai-infra/megatron-lm-pp-vpp-figures/Pasted%20image%2020250907122344.webp></p></div><footer class=post-footer><div readmore=true><div class=readmore-overlay><div class=readmore-overlay-center><div class=post-button><a class=btn href=/posts/ai-infra/Megatron-LM-%E8%AE%BA%E6%96%87-PP-%E5%92%8C-VPP-%E7%94%BB%E6%B3%95%E7%96%91%E9%97%AE/#more rel=contents>阅读全文 &#187;</a></div></div></div><div class=no-more-to-read><hr><center>没有啦~</center></div></div><div class=post-eof></div></footer></article></div><div class=post-block><article itemscope itemtype=http://schema.org/Article class=post-content lang><link itemprop=mainEntityOfPage href=https://hxhue.github.io/posts/ai-infra/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E9%80%9A%E4%BF%A1%E8%A6%81%E8%A7%A3%E5%86%B3%E5%93%AA%E4%BA%9B%E9%97%AE%E9%A2%98%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/><span hidden itemprop=author itemscope itemtype=http://schema.org/Person><meta itemprop=image content="/imgs/371907.jpg"><meta itemprop=name content="🤖"></span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization><meta itemprop=name content="🤖"><meta itemprop=description content="个人博客，主要是零散的笔记。"></span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork><meta itemprop=name content="大模型训练通信要解决哪些问题？（个人总结）"><meta itemprop=description content="结构图









加速通信


减少给定操作的通信量

低精度训练
先量化再通信

DeepSeek V3 dispatch 时先把 BF16 量化到 FP8，通信量减少一半。但因为 combine 通信中带有规约，所以不适合用 FP8。
MegaScale-MoE 在 AllGather 派发器的 combine 阶段用 (a) 量化到 FP8 + (b) AlltoAll + (c) FP32 累加器规约 来代替原先的 ReduceScatter 通信，通信量减少一半、loss 正常。ReduceScatter 时每个 rank 只需要收到自己分区的所有数据并完成规约即可（不像 AllGather 接口需要所有数据），用 AlltoAll + 接收后本地规约也能替代。


通信去重

DeepEP




减少通信

尽可能使用效率更高的并行方式，比如用 Ulysses SP 代替 TP 可以从理论上大大降低通信。



保持计算流忙碌


通信计算重叠

Megatron-LM 现在对 DP/TP/CP/PP/EP 均有通信重叠功能

部分选项依赖 CUDA_DEVICE_MAX_CONNECTIONS
部分选项依赖交错 1F1B 调度
Megatron CP 的通信计算重叠强制打开、无选项可以关闭
Megatron 1F1B + EP A2A overlap 是 2025.8 才加入的新功能，但在 2025.3 已有人提出


DualPipe 用前反向两个微批次的前后向来重叠


负载均衡

DP/PP 负载均衡

ByteScale 的工作很有启发性
Ernie 4.5 也做了 DP 上的子序列重排


EP 负载均衡

token drop、辅助 loss、expert bias
专家动态调整，这一类工作很多
样本动态调整，如 NetMoE


CP 负载均衡

Ulysses SP
Megatron CP 未考虑子序列打包的问题，ByteScale 做了一些处理




消除卡间同步

ZeroBubble 后校验


"></span><header class=post-header><h2 class=post-title itemprop="name headline"><a href=/posts/ai-infra/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E9%80%9A%E4%BF%A1%E8%A6%81%E8%A7%A3%E5%86%B3%E5%93%AA%E4%BA%9B%E9%97%AE%E9%A2%98%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/ itemprop=url class=post-title-link>大模型训练通信要解决哪些问题？（个人总结）</a></h2><div class=post-meta-container><div class=post-meta-items><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar"></i>
</span><span class=post-meta-item-text title=发表于>发表于：
</span><time title="创建时间：2025-09-07 00:00:00 +0800 CST" itemprop="dateCreated datePublished" datetime="2025-09-07 00:00:00 +0800 CST">2025-09-07</time></span></div></div></header><div class=post-body itemprop=articleBody><h1 id=结构图>结构图
<a class=header-anchor href=#%e7%bb%93%e6%9e%84%e5%9b%be></a></h1><p><img src=/posts/ai-infra/assets/Pasted%20image%2020250907125720.webp width=600></p><h1 id=加速通信>加速通信
<a class=header-anchor href=#%e5%8a%a0%e9%80%9f%e9%80%9a%e4%bf%a1></a></h1><ul><li>减少给定操作的通信量<ul><li>低精度训练</li><li>先量化再通信<ul><li>DeepSeek V3 dispatch 时先把 BF16 量化到 FP8，通信量减少一半。但因为 combine 通信中带有规约，所以不适合用 FP8。</li><li>MegaScale-MoE 在 AllGather 派发器的 combine 阶段用 <strong>(a)</strong> 量化到 FP8 + <strong>(b)</strong> AlltoAll + <strong>(c)</strong> FP32 累加器规约 来代替原先的 ReduceScatter 通信，通信量减少一半、loss 正常。ReduceScatter 时每个 rank 只需要收到自己分区的所有数据并完成规约即可（不像 AllGather 接口需要所有数据），用 AlltoAll + 接收后本地规约也能替代。</li></ul></li><li>通信去重<ul><li>DeepEP</li></ul></li></ul></li><li>减少通信<ul><li>尽可能使用效率更高的并行方式，比如用 Ulysses SP 代替 TP 可以从理论上大大降低通信。</li></ul></li></ul><h1 id=保持计算流忙碌>保持计算流忙碌
<a class=header-anchor href=#%e4%bf%9d%e6%8c%81%e8%ae%a1%e7%ae%97%e6%b5%81%e5%bf%99%e7%a2%8c></a></h1><ul><li>通信计算重叠<ul><li>Megatron-LM 现在对 DP/TP/CP/PP/EP 均有通信重叠功能<ul><li>部分选项依赖 <code>CUDA_DEVICE_MAX_CONNECTIONS</code></li><li>部分选项依赖交错 1F1B 调度</li><li>Megatron CP 的通信计算重叠强制打开、无选项可以关闭</li><li>Megatron 1F1B + EP A2A overlap 是 2025.8 才加入的新功能，但在 2025.3 已有人提出</li></ul></li><li>DualPipe 用前反向两个微批次的前后向来重叠</li></ul></li><li>负载均衡<ul><li>DP/PP 负载均衡<ul><li>ByteScale 的工作很有启发性</li><li>Ernie 4.5 也做了 DP 上的子序列重排</li></ul></li><li>EP 负载均衡<ul><li>token drop、辅助 loss、expert bias</li><li>专家动态调整，这一类工作很多</li><li>样本动态调整，如 NetMoE</li></ul></li><li>CP 负载均衡<ul><li>Ulysses SP</li><li>Megatron CP 未考虑子序列打包的问题，ByteScale 做了一些处理</li></ul></li></ul></li><li>消除卡间同步<ul><li>ZeroBubble 后校验</li></ul></li></ul></div><footer class=post-footer><div readmore=false><div class=readmore-overlay><div class=readmore-overlay-center><div class=post-button><a class=btn href=/posts/ai-infra/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E9%80%9A%E4%BF%A1%E8%A6%81%E8%A7%A3%E5%86%B3%E5%93%AA%E4%BA%9B%E9%97%AE%E9%A2%98%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/#more rel=contents>阅读全文 &#187;</a></div></div></div><div class=no-more-to-read><hr><center>没有啦~</center></div></div><div class=post-eof></div></footer></article></div><div class=post-block><article itemscope itemtype=http://schema.org/Article class=post-content lang><link itemprop=mainEntityOfPage href=https://hxhue.github.io/posts/ai-infra/DeepSeek-V3-MLA/><span hidden itemprop=author itemscope itemtype=http://schema.org/Person><meta itemprop=image content="/imgs/371907.jpg"><meta itemprop=name content="🤖"></span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization><meta itemprop=name content="🤖"><meta itemprop=description content="个人博客，主要是零散的笔记。"></span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork><meta itemprop=name content="DeepSeek V3 MLA"><meta itemprop=description content="下图标注了 MLA 的一种计算方式，橙色虚线部分可以被包裹到重计算中，入口为 q1，kv1 和 k_rope1（不保存 kv1 和 k_rope1 而是保存 kv1_and_k_rope 也可以，它们的大小一样，没区别）。flash_attn 比较特殊，除了保存输入之外还会保存输出。"></span><header class=post-header><h2 class=post-title itemprop="name headline"><a href=/posts/ai-infra/DeepSeek-V3-MLA/ itemprop=url class=post-title-link>DeepSeek V3 MLA</a></h2><div class=post-meta-container><div class=post-meta-items><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar"></i>
</span><span class=post-meta-item-text title=发表于>发表于：
</span><time title="创建时间：2025-08-21 00:00:00 +0800 CST" itemprop="dateCreated datePublished" datetime="2025-08-21 00:00:00 +0800 CST">2025-08-21
</time></span><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar-check"></i>
</span><span class=post-meta-item-text title=更新于>更新于：
</span><time title=修改时间：2025-10-04T00:00:00+08:00 itemprop=dateModified datetime=2025-10-04T00:00:00+08:00>2025-10-04</time></span></div></div></header><div class=post-body itemprop=articleBody><p>下图标注了 MLA 的一种计算方式，橙色虚线部分可以被包裹到重计算中，入口为 q1，kv1 和 k_rope1（不保存 kv1 和 k_rope1 而是保存 kv1_and_k_rope 也可以，它们的大小一样，没区别）。<code>flash_attn</code> 比较特殊，除了保存输入之外还会保存输出。</p><p>如果要做 TP，需要保持 down_proj 为完整矩阵，在 up_proj 矩阵做列切分，在 o_proj 做行切分。如果要做 CP，需要在 attention 的位置插入 CP 策略。</p><p><img src=/posts/ai-infra/assets/mla-recompute.svg></p><p>MLA 推荐阅读： <a href=https://www.cnblogs.com/rossiXYZ/p/18827618 title=https://www.cnblogs.com/rossiXYZ/p/18827618 rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://www.cnblogs.com/rossiXYZ/p/18827618<i class="fa fa-external-link-alt"></i></a></p></div><footer class=post-footer><div readmore=false><div class=readmore-overlay><div class=readmore-overlay-center><div class=post-button><a class=btn href=/posts/ai-infra/DeepSeek-V3-MLA/#more rel=contents>阅读全文 &#187;</a></div></div></div><div class=no-more-to-read><hr><center>没有啦~</center></div></div><div class=post-eof></div></footer></article></div><div class=post-block><article itemscope itemtype=http://schema.org/Article class=post-content lang><link itemprop=mainEntityOfPage href=https://hxhue.github.io/posts/ai-infra/MoE-Parallel-FoldingETP-%E5%92%8C-DeepEP/><span hidden itemprop=author itemscope itemtype=http://schema.org/Person><meta itemprop=image content="/imgs/371907.jpg"><meta itemprop=name content="🤖"></span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization><meta itemprop=name content="🤖"><meta itemprop=description content="个人博客，主要是零散的笔记。"></span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork><meta itemprop=name content="MoE Parallel Folding、ETP 和 DeepEP"><meta itemprop=description content="DeepEP 假设了本地 node 用 NVLINK 连接的卡数为 8，因此 EP 必须在最内层。Megatron 在 MoEFlexTokenDispatcher 接入 DeepEP，也必须满足这个假设。
在 MoE Parallel Folding 下，MoE 层的 CP 消失，排在 EP 内层的只有 TP，Megatron 是如何处理 ETP > 1 的情况的？它将 routing map 展开了 tp_size 份，直接用 DeepEP 分发（这个时候使用 TP-EP 组而不是 EP 组，大小为 ep_size * tp_size），这样就满足了 DeepEP 的假设，实质上利用 DeepEP 把 TP 的通信也做了。"></span><header class=post-header><h2 class=post-title itemprop="name headline"><a href=/posts/ai-infra/MoE-Parallel-FoldingETP-%E5%92%8C-DeepEP/ itemprop=url class=post-title-link>MoE Parallel Folding、ETP 和 DeepEP</a></h2><div class=post-meta-container><div class=post-meta-items><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar"></i>
</span><span class=post-meta-item-text title=发表于>发表于：
</span><time title="创建时间：2025-08-18 00:00:00 +0800 CST" itemprop="dateCreated datePublished" datetime="2025-08-18 00:00:00 +0800 CST">2025-08-18
</time></span><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar-check"></i>
</span><span class=post-meta-item-text title=更新于>更新于：
</span><time title=修改时间：2025-09-07T00:00:00+08:00 itemprop=dateModified datetime=2025-09-07T00:00:00+08:00>2025-09-07</time></span></div></div></header><div class=post-body itemprop=articleBody><p>DeepEP 假设了本地 node 用 NVLINK 连接的卡数为 8，因此 EP 必须在最内层。Megatron 在 MoEFlexTokenDispatcher 接入 DeepEP，也必须满足这个假设。</p><p>在 <a href=https://arxiv.org/html/2504.14960v2 title="MoE Parallel Folding" rel="noopener external nofollow noreferrer" target=_blank class=exturl>MoE Parallel Folding<i class="fa fa-external-link-alt"></i></a> 下，MoE 层的 CP 消失，排在 EP 内层的只有 TP，Megatron 是如何处理 ETP > 1 的情况的？它将 routing map 展开了 tp_size 份，直接用 DeepEP 分发（<strong>这个时候使用 TP-EP 组而不是 EP 组，大小为 <code>ep_size * tp_size</code></strong>），这样就满足了 DeepEP 的假设，实质上利用 DeepEP 把 TP 的通信也做了。</p><blockquote><p>我认为 MoE Parallel Folding 的优势是：1. 兼容 DeepEP；2. 把 Attention 和 MoE 的 TP 数解耦；3. 为 EP 让出了 CP 的范围，使得 EP 通信组排列更局部，通信效率更高。</p></blockquote><p>一般来说 MoE 层的 ETP 通信是先做 AlltoAll 再做 AllGather。因为 AllGather 会在 TP 组内形成冗余（大家都拿到一模一样的输入），所以应该只在真正需要用到之前才做 AllGather，将 AllGather 放在 AlltoAll 后可以减少 AlltoAll 的通信量（图片来自 MoE Parallel Folding 论文，上文已给出链接）：</p><p><img src=/posts/ai-infra/assets/Pasted%20image%2020250818210810.webp></p></div><footer class=post-footer><div readmore=true><div class=readmore-overlay><div class=readmore-overlay-center><div class=post-button><a class=btn href=/posts/ai-infra/MoE-Parallel-FoldingETP-%E5%92%8C-DeepEP/#more rel=contents>阅读全文 &#187;</a></div></div></div><div class=no-more-to-read><hr><center>没有啦~</center></div></div><div class=post-eof></div></footer></article></div><div class=post-block><article itemscope itemtype=http://schema.org/Article class=post-content lang><link itemprop=mainEntityOfPage href=https://hxhue.github.io/posts/ai-infra/NVIDIA-%E5%8D%9A%E5%AE%A2NCCL-2.12-PXN-%E4%BC%98%E5%8C%96/><span hidden itemprop=author itemscope itemtype=http://schema.org/Person><meta itemprop=image content="/imgs/371907.jpg"><meta itemprop=name content="🤖"></span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization><meta itemprop=name content="🤖"><meta itemprop=description content="个人博客，主要是零散的笔记。"></span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork><meta itemprop=name content="NVIDIA 博客：NCCL 2.12 PXN 优化"><meta itemprop=description content="文章总结

https://developer.nvidia.com/blog/doubling-all2all-performance-with-nvidia-collective-communication-library-2-12/

  
      
      
    内容由 AI 生成"></span><header class=post-header><h2 class=post-title itemprop="name headline"><a href=/posts/ai-infra/NVIDIA-%E5%8D%9A%E5%AE%A2NCCL-2.12-PXN-%E4%BC%98%E5%8C%96/ itemprop=url class=post-title-link>NVIDIA 博客：NCCL 2.12 PXN 优化</a></h2><div class=post-meta-container><div class=post-meta-items><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar"></i>
</span><span class=post-meta-item-text title=发表于>发表于：
</span><time title="创建时间：2025-08-18 00:00:00 +0800 CST" itemprop="dateCreated datePublished" datetime="2025-08-18 00:00:00 +0800 CST">2025-08-18
</time></span><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar-check"></i>
</span><span class=post-meta-item-text title=更新于>更新于：
</span><time title=修改时间：2025-09-30T00:00:00+08:00 itemprop=dateModified datetime=2025-09-30T00:00:00+08:00>2025-09-30</time></span></div></div></header><div class=post-body itemprop=articleBody><h2 id=文章总结>文章总结
<a class=header-anchor href=#%e6%96%87%e7%ab%a0%e6%80%bb%e7%bb%93></a></h2><p><a href=https://developer.nvidia.com/blog/doubling-all2all-performance-with-nvidia-collective-communication-library-2-12/ title=https://developer.nvidia.com/blog/doubling-all2all-performance-with-nvidia-collective-communication-library-2-12/ rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://developer.nvidia.com/blog/doubling-all2all-performance-with-nvidia-collective-communication-library-2-12/<i class="fa fa-external-link-alt"></i></a></p><div class="markdown-alert markdown-alert-caution"><p class=markdown-alert-title><svg class="octicon octicon-stop mr-2" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="M4.47.22A.749.749.0 015 0h6c.199.0.389.079.53.22l4.25 4.25c.141.14.22.331.22.53v6a.749.749.0 01-.22.53l-4.25 4.25A.749.749.0 0111 16H5a.749.749.0 01-.53-.22L.22 11.53A.749.749.0 010 11V5c0-.199.079-.389.22-.53zm.84 1.28L1.5 5.31v5.38l3.81 3.81h5.38l3.81-3.81V5.31L10.69 1.5zM8 4a.75.75.0 01.75.75v3.5a.75.75.0 01-1.5.0v-3.5A.75.75.0 018 4zm0 8a1 1 0 110-2 1 1 0 010 2z"/></svg>内容由 AI 生成</p><p>NCCL 2.12 的核心改进包括：</p><ul><li><strong>PXN (PCI × NVLink)：</strong> 这是一项新功能，它允许 GPU 直接通过 <strong>NVLink 和 PCI</strong> 与节点上的网卡 (NIC) 进行通信，从而绕过 CPU。这带来了更高效的数据传输和更高的带宽。</li><li><strong>优化的消息聚合：</strong> PXN 使得节点上的所有 GPU 能够将它们的数据汇总到一个 GPU 上，以发送到特定目的地。这样，网络层可以将多个消息作为一个整体发送，提高了消息速率并减少了连接开销。</li><li><strong>轨道优化的网络拓扑：</strong> PXN 利用 <strong>NVSwitch 连接</strong>将数据移动到与目的地位于同一网络轨道上的 GPU，避免了通过第二层主干交换机进行的效率较低的流量。</li></ul><p>这些进步使 <strong>all2all 性能提升了两倍以上</strong>，并为复杂的 GPU 拓扑中的模型并行性提供了更大的灵活性。</p></div><p>虽然看上去是三项功能，但是从描述来看都是 PXN 使得它们得以实现，所以实际上主要改进还是 PXN 的引入。</p><h2 id=pxn-对-alltoall-的改进>PXN 对 alltoall 的改进
<a class=header-anchor href=#pxn-%e5%af%b9-alltoall-%e7%9a%84%e6%94%b9%e8%bf%9b></a></h2><blockquote><p>GPU 不是在其本地内存中为本地 NIC 发送数据准备缓冲区，而是在中间 GPU 上准备缓冲区，通过 NVLink 写入。然后它通知管理该 NIC 的 CPU 代理数据已准备好，而不是通知其自己的 CPU 代理。GPU-CPU 同步可能会稍微慢一些，因为它可能需要跨越 CPU 插槽，但数据本身仅使用 NVLink 和 PCI 交换机，确保最大带宽。</p></blockquote><p>原来的 alltoall 传输前是要在发送方准备数据缓冲区的，local rank 0 如果想要发给 peer rank 3 就需要先在自己这里准备数据，然后经过多层路由传输给 peer rank 3。现在是在 local rank 3 上准备数据缓冲区（应该是 RDMA 用），local rank 0 通过 nvlink 将数据传输给 local rank 3，然后 local rank 3 发给 peer rank 3。因为后两者属于同一个 rail，所以只需要经过 L 层次的 switch，减少了路由延迟。</p></div><footer class=post-footer><div readmore=true><div class=readmore-overlay><div class=readmore-overlay-center><div class=post-button><a class=btn href=/posts/ai-infra/NVIDIA-%E5%8D%9A%E5%AE%A2NCCL-2.12-PXN-%E4%BC%98%E5%8C%96/#more rel=contents>阅读全文 &#187;</a></div></div></div><div class=no-more-to-read><hr><center>没有啦~</center></div></div><div class=post-eof></div></footer></article></div><div class=post-block><article itemscope itemtype=http://schema.org/Article class=post-content lang><link itemprop=mainEntityOfPage href=https://hxhue.github.io/posts/cli/bash/diff-%E4%B8%A4%E4%B8%AA%E6%96%87%E4%BB%B6%E5%A4%B9/><span hidden itemprop=author itemscope itemtype=http://schema.org/Person><meta itemprop=image content="/imgs/371907.jpg"><meta itemprop=name content="🤖"></span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization><meta itemprop=name content="🤖"><meta itemprop=description content="个人博客，主要是零散的笔记。"></span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork><meta itemprop=name content="diff 两个文件夹"><meta itemprop=description content="diff -bur folderA/ folderB/
-r 表示递归，-b 表示忽略空白字符，-u 表示输出统一样式（也就是 git diff 常见的样式，会比默认样式容易阅读）。"></span><header class=post-header><h2 class=post-title itemprop="name headline"><a href=/posts/cli/bash/diff-%E4%B8%A4%E4%B8%AA%E6%96%87%E4%BB%B6%E5%A4%B9/ itemprop=url class=post-title-link>diff 两个文件夹</a></h2><div class=post-meta-container><div class=post-meta-items><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar"></i>
</span><span class=post-meta-item-text title=发表于>发表于：
</span><time title="创建时间：2025-07-10 00:00:00 +0800 CST" itemprop="dateCreated datePublished" datetime="2025-07-10 00:00:00 +0800 CST">2025-07-10
</time></span><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar-check"></i>
</span><span class=post-meta-item-text title=更新于>更新于：
</span><time title=修改时间：2025-07-19T00:00:00+08:00 itemprop=dateModified datetime=2025-07-19T00:00:00+08:00>2025-07-19</time></span></div></div></header><div class=post-body itemprop=articleBody><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-bash data-lang=bash><span style=display:flex><span>diff -bur folderA/ folderB/
</span></span></code></pre></div><p><code>-r</code> 表示递归，<code>-b</code> 表示忽略空白字符，<code>-u</code> 表示输出统一样式（也就是 git diff 常见的样式，会比默认样式容易阅读）。</p></div><footer class=post-footer><div readmore=false><div class=readmore-overlay><div class=readmore-overlay-center><div class=post-button><a class=btn href=/posts/cli/bash/diff-%E4%B8%A4%E4%B8%AA%E6%96%87%E4%BB%B6%E5%A4%B9/#more rel=contents>阅读全文 &#187;</a></div></div></div><div class=no-more-to-read><hr><center>没有啦~</center></div></div><div class=post-eof></div></footer></article></div><div class=post-block><article itemscope itemtype=http://schema.org/Article class=post-content lang><link itemprop=mainEntityOfPage href=https://hxhue.github.io/posts/programming/python/2.-PyTorch-C++-%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90/><span hidden itemprop=author itemscope itemtype=http://schema.org/Person><meta itemprop=image content="/imgs/371907.jpg"><meta itemprop=name content="🤖"></span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization><meta itemprop=name content="🤖"><meta itemprop=description content="个人博客，主要是零散的笔记。"></span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork><meta itemprop=name content="PyTorch C++ 代码生成"><meta itemprop=description content='一些问题

为什么有些会生成 at::cuda 名字空间的函数，有些不会？（待解决）
提要

本文说明了 m.impl("index_put.out", ...) 到 at::native::index_put 的调用路径。结合 
  
  
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    PyTorch C++ 函数派发 中 at::native::index_put_ → at::_index_put_impl_ → index_put_stub 的调用路径，补全了从 m.impl 到 stub 的全路径。'></span><header class=post-header><h2 class=post-title itemprop="name headline"><a href=/posts/programming/python/2.-PyTorch-C++-%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90/ itemprop=url class=post-title-link>PyTorch C++ 代码生成</a></h2><div class=post-meta-container><div class=post-meta-items><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar"></i>
</span><span class=post-meta-item-text title=发表于>发表于：
</span><time title="创建时间：2025-06-30 00:00:00 +0800 CST" itemprop="dateCreated datePublished" datetime="2025-06-30 00:00:00 +0800 CST">2025-06-30
</time></span><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar-check"></i>
</span><span class=post-meta-item-text title=更新于>更新于：
</span><time title=修改时间：2025-10-02T00:00:00+08:00 itemprop=dateModified datetime=2025-10-02T00:00:00+08:00>2025-10-02</time></span></div></div></header><div class=post-body itemprop=articleBody><h1 id=一些问题>一些问题
<a class=header-anchor href=#%e4%b8%80%e4%ba%9b%e9%97%ae%e9%a2%98></a></h1><p>为什么有些会生成 at::cuda 名字空间的函数，有些不会？（待解决）</p><h1 id=提要>提要
<a class=header-anchor href=#%e6%8f%90%e8%a6%81></a></h1><p><strong>本文说明了 <code>m.impl("index_put.out", ...)</code> 到 <code>at::native::index_put</code> 的调用路径</strong>。结合
<a href=/posts/programming/python/1.-PyTorch-C++-%E5%87%BD%E6%95%B0%E6%B4%BE%E5%8F%91/ title="PyTorch C++ 函数派发">PyTorch C++ 函数派发</a> 中 <code>at::native::index_put_</code> → <code>at::_index_put_impl_</code> → <code>index_put_stub</code> 的调用路径，补全了从 m.impl 到 stub 的全路径。</p><p><strong>本文说明了 <code>m.impl</code> / <code>at::cuda::index_out</code> → <code>index_stub</code> 的调用路径</strong>。准确来说是介绍了 <code>at::cuda::index_out</code> 调用 meta 和 impl 的过程，meta 中对下标做预处理（包括 kBool 转 kLong 下标），impl 中调用 <code>index_stub</code> 进行计算。和 <code>index_out</code> 不同，<code>index_put_</code> 函数没有出现在 <code>at::cuda</code> 名字空间中，取而代之的是 <code>at::cuda::_index_put_impl_</code>。</p><p>在这两个例子中，能找到的函数有：</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span>理解 native_functions.yaml 中的函数定义在哪里
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>非 structured 情况：
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#719e07>-</span> func: index_put_(...)
</span></span><span style=display:flex><span>  dispatch:
</span></span><span style=display:flex><span>	  CompositeExplicitAutograd: index_put_ # 默认名字空间是 aten
</span></span><span style=display:flex><span>			\ at<span style=color:#719e07>::</span>index_put_ (build<span style=color:#719e07>/</span>aten<span style=color:#719e07>/</span>src<span style=color:#719e07>/</span>ATen<span style=color:#719e07>/</span>Functions.h)
</span></span><span style=display:flex><span>			 \ at<span style=color:#719e07>::</span>_ops<span style=color:#719e07>::</span>index_put_<span style=color:#719e07>::</span>call (build<span style=color:#719e07>/</span>aten<span style=color:#719e07>/</span>src<span style=color:#719e07>/</span>ATen<span style=color:#719e07>/</span>OperatorsEverything.cpp)
</span></span><span style=display:flex><span>			  \ c10<span style=color:#719e07>::</span>Dispatcher<span style=color:#719e07>::</span>singleton()
</span></span><span style=display:flex><span>			   <span style=color:#719e07>|</span>  .findSchemaOrThrow(index_put_<span style=color:#719e07>::</span>name, index_put_<span style=color:#719e07>::</span>overload_name)
</span></span><span style=display:flex><span>			   <span style=color:#719e07>|</span>  .typed<span style=color:#719e07>&lt;</span>index_put_<span style=color:#719e07>::</span>schema<span style=color:#719e07>&gt;</span>().call
</span></span><span style=display:flex><span>				 <span style=color:#719e07>|</span><span style=color:#719e07>struct</span> <span style=color:#268bd2>TORCH_API</span> index_put_ (build<span style=color:#719e07>/</span>aten<span style=color:#719e07>/</span>src<span style=color:#719e07>/</span>ATen<span style=color:#719e07>/</span>MethodOperators.h)
</span></span><span style=display:flex><span>				 <span style=color:#719e07>|</span>  ... name <span style=color:#719e07>=</span> <span style=color:#2aa198>&#34;aten::index_put_&#34;</span>;
</span></span><span style=display:flex><span>				 <span style=color:#719e07>|</span>  ... overload_name <span style=color:#719e07>=</span> <span style=color:#2aa198>&#34;&#34;</span>;
</span></span><span style=display:flex><span>				 \ m.impl(<span style=color:#2aa198>&#34;index_put_&#34;</span>, ...) (build<span style=color:#719e07>/</span>aten<span style=color:#719e07>/</span>src<span style=color:#719e07>/</span>ATen<span style=color:#719e07>/</span>RegisterCompositeExplicitAutogradEverything.cpp)
</span></span><span style=display:flex><span>				  \ at<span style=color:#719e07>::</span>native<span style=color:#719e07>::</span>index_put_ (aten<span style=color:#719e07>/</span>src<span style=color:#719e07>/</span>ATen<span style=color:#719e07>/</span>native<span style=color:#719e07>/</span>TensorAdvancedIndexing.cpp)
</span></span><span style=display:flex><span>				   \ at<span style=color:#719e07>::</span>_index_put_impl_
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>结论是按照 dispatch: yy: xx 字段生成 at<span style=color:#719e07>::</span>xx 函数，最终调用到 at<span style=color:#719e07>::</span>native<span style=color:#719e07>::</span>xx 函数。
</span></span><span style=display:flex><span>                                                       <span style=color:#719e07>^^^^^^^^^^^^^^</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>如果是 structured: True，就对应 at<span style=color:#719e07>::</span>native<span style=color:#719e07>::</span>structured_xx<span style=color:#719e07>::</span>impl，在代码中通常为
</span></span><span style=display:flex><span>at<span style=color:#719e07>::</span>native 名字空间下的 TORCH_IMPL_FUNC(xx)。
</span></span><span style=display:flex><span>                      <span style=color:#719e07>^^^^^^^^^^^^^^^^^^^</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>如果是 structured_delegate: zz，可能得去找 zz 的定义。
</span></span></code></pre></div></div><footer class=post-footer><div readmore=true><div class=readmore-overlay><div class=readmore-overlay-center><div class=post-button><a class=btn href=/posts/programming/python/2.-PyTorch-C++-%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90/#more rel=contents>阅读全文 &#187;</a></div></div></div><div class=no-more-to-read><hr><center>没有啦~</center></div></div><div class=post-eof></div></footer></article></div><div class=post-block><article itemscope itemtype=http://schema.org/Article class=post-content lang><link itemprop=mainEntityOfPage href=https://hxhue.github.io/posts/programming/python/1.-PyTorch-C++-%E5%87%BD%E6%95%B0%E6%B4%BE%E5%8F%91/><span hidden itemprop=author itemscope itemtype=http://schema.org/Person><meta itemprop=image content="/imgs/371907.jpg"><meta itemprop=name content="🤖"></span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization><meta itemprop=name content="🤖"><meta itemprop=description content="个人博客，主要是零散的笔记。"></span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork><meta itemprop=name content="PyTorch C++ 函数派发"><meta itemprop=description content="Stub 注册流程

所有的 stub 定义几乎都在 aten/src/ATen/native/DispatchStub.h 文件，可以慢慢看。里面有段注释：
// Implements instruction set specific function dispatch.
//
// Kernels that may make use of specialized instruction sets (e.g. AVX2) are
// compiled multiple times with different compiler flags (e.g. -mavx2). A
// DispatchStub contains a table of function pointers for a kernel. At runtime,
// the fastest available kernel is chosen based on the features reported by
// cpuinfo.
//
// Example:
//
// In native/MyKernel.h:
//   using fn_type = void(*)(const Tensor& x);
//   DECLARE_DISPATCH(fn_type, stub)
//
// In native/MyKernel.cpp
//   DEFINE_DISPATCH(stub);
//
// In native/cpu/MyKernel.cpp:
//   namespace {
//     // use anonymous namespace so that different cpu versions won't conflict
//     void kernel(const Tensor& x) { ... }
//   }
//   REGISTER_DISPATCH(stub, &amp;kernel);
//
// To call:
//   stub(kCPU, tensor);
//
// TODO: CPU instruction set selection should be folded into whatever
// the main dispatch mechanism is.
//
// Supported device types for registration:
//   - CPU: Central Processing Unit
//   - CUDA: NVIDIA GPUs
//   - HIP: AMD GPUs
//   - MPS: Apple Silicon GPUs (Metal Performance Shaders)
//   - MTIA: Meta Training and Inference Devices
//   - XPU: Intel GPUs
//   - HPU: Reserved for HPU (Intel Gaudi) device types
//   - PrivateUse1: Reserved for private/custom device types
//
// If you want to update the list of supported devices, add a new dispatch_ptr
// member in DispatchStubImpl.h and update the get_call_ptr switch.
// As well you will need to update the inlined list in 'is_device_supported`
//
//
// ignore warnings about DispatchStub::DEFAULT, AVX, AVX2 defined elsewhere
DispatchStub 模板基类定义

见 aten/src/ATen/native/DispatchStub.h。DispatchStub 类型为："></span><header class=post-header><h2 class=post-title itemprop="name headline"><a href=/posts/programming/python/1.-PyTorch-C++-%E5%87%BD%E6%95%B0%E6%B4%BE%E5%8F%91/ itemprop=url class=post-title-link>PyTorch C++ 函数派发</a></h2><div class=post-meta-container><div class=post-meta-items><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar"></i>
</span><span class=post-meta-item-text title=发表于>发表于：
</span><time title="创建时间：2025-06-30 00:00:00 +0800 CST" itemprop="dateCreated datePublished" datetime="2025-06-30 00:00:00 +0800 CST">2025-06-30
</time></span><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar-check"></i>
</span><span class=post-meta-item-text title=更新于>更新于：
</span><time title=修改时间：2025-10-02T00:00:00+08:00 itemprop=dateModified datetime=2025-10-02T00:00:00+08:00>2025-10-02</time></span></div></div></header><div class=post-body itemprop=articleBody><h1 id=stub-注册流程>Stub 注册流程
<a class=header-anchor href=#stub-%e6%b3%a8%e5%86%8c%e6%b5%81%e7%a8%8b></a></h1><p>所有的 stub 定义几乎都在 aten/src/ATen/native/DispatchStub.h 文件，可以慢慢看。里面有段注释：</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#586e75>// Implements instruction set specific function dispatch.
</span></span></span><span style=display:flex><span><span style=color:#586e75>//
</span></span></span><span style=display:flex><span><span style=color:#586e75>// Kernels that may make use of specialized instruction sets (e.g. AVX2) are
</span></span></span><span style=display:flex><span><span style=color:#586e75>// compiled multiple times with different compiler flags (e.g. -mavx2). A
</span></span></span><span style=display:flex><span><span style=color:#586e75>// DispatchStub contains a table of function pointers for a kernel. At runtime,
</span></span></span><span style=display:flex><span><span style=color:#586e75>// the fastest available kernel is chosen based on the features reported by
</span></span></span><span style=display:flex><span><span style=color:#586e75>// cpuinfo.
</span></span></span><span style=display:flex><span><span style=color:#586e75>//
</span></span></span><span style=display:flex><span><span style=color:#586e75>// Example:
</span></span></span><span style=display:flex><span><span style=color:#586e75>//
</span></span></span><span style=display:flex><span><span style=color:#586e75>// In native/MyKernel.h:
</span></span></span><span style=display:flex><span><span style=color:#586e75>//   using fn_type = void(*)(const Tensor&amp; x);
</span></span></span><span style=display:flex><span><span style=color:#586e75>//   DECLARE_DISPATCH(fn_type, stub)
</span></span></span><span style=display:flex><span><span style=color:#586e75>//
</span></span></span><span style=display:flex><span><span style=color:#586e75>// In native/MyKernel.cpp
</span></span></span><span style=display:flex><span><span style=color:#586e75>//   DEFINE_DISPATCH(stub);
</span></span></span><span style=display:flex><span><span style=color:#586e75>//
</span></span></span><span style=display:flex><span><span style=color:#586e75>// In native/cpu/MyKernel.cpp:
</span></span></span><span style=display:flex><span><span style=color:#586e75>//   namespace {
</span></span></span><span style=display:flex><span><span style=color:#586e75>//     // use anonymous namespace so that different cpu versions won&#39;t conflict
</span></span></span><span style=display:flex><span><span style=color:#586e75>//     void kernel(const Tensor&amp; x) { ... }
</span></span></span><span style=display:flex><span><span style=color:#586e75>//   }
</span></span></span><span style=display:flex><span><span style=color:#586e75>//   REGISTER_DISPATCH(stub, &amp;kernel);
</span></span></span><span style=display:flex><span><span style=color:#586e75>//
</span></span></span><span style=display:flex><span><span style=color:#586e75>// To call:
</span></span></span><span style=display:flex><span><span style=color:#586e75>//   stub(kCPU, tensor);
</span></span></span><span style=display:flex><span><span style=color:#586e75>//
</span></span></span><span style=display:flex><span><span style=color:#586e75>// TODO: CPU instruction set selection should be folded into whatever
</span></span></span><span style=display:flex><span><span style=color:#586e75>// the main dispatch mechanism is.
</span></span></span><span style=display:flex><span><span style=color:#586e75>//
</span></span></span><span style=display:flex><span><span style=color:#586e75>// Supported device types for registration:
</span></span></span><span style=display:flex><span><span style=color:#586e75>//   - CPU: Central Processing Unit
</span></span></span><span style=display:flex><span><span style=color:#586e75>//   - CUDA: NVIDIA GPUs
</span></span></span><span style=display:flex><span><span style=color:#586e75>//   - HIP: AMD GPUs
</span></span></span><span style=display:flex><span><span style=color:#586e75>//   - MPS: Apple Silicon GPUs (Metal Performance Shaders)
</span></span></span><span style=display:flex><span><span style=color:#586e75>//   - MTIA: Meta Training and Inference Devices
</span></span></span><span style=display:flex><span><span style=color:#586e75>//   - XPU: Intel GPUs
</span></span></span><span style=display:flex><span><span style=color:#586e75>//   - HPU: Reserved for HPU (Intel Gaudi) device types
</span></span></span><span style=display:flex><span><span style=color:#586e75>//   - PrivateUse1: Reserved for private/custom device types
</span></span></span><span style=display:flex><span><span style=color:#586e75>//
</span></span></span><span style=display:flex><span><span style=color:#586e75>// If you want to update the list of supported devices, add a new dispatch_ptr
</span></span></span><span style=display:flex><span><span style=color:#586e75>// member in DispatchStubImpl.h and update the get_call_ptr switch.
</span></span></span><span style=display:flex><span><span style=color:#586e75>// As well you will need to update the inlined list in &#39;is_device_supported`
</span></span></span><span style=display:flex><span><span style=color:#586e75>//
</span></span></span><span style=display:flex><span><span style=color:#586e75>//
</span></span></span><span style=display:flex><span><span style=color:#586e75>// ignore warnings about DispatchStub::DEFAULT, AVX, AVX2 defined elsewhere
</span></span></span></code></pre></div><h1 id=dispatchstub-模板基类定义><code>DispatchStub</code> 模板基类定义
<a class=header-anchor href=#dispatchstub-%e6%a8%a1%e6%9d%bf%e5%9f%ba%e7%b1%bb%e5%ae%9a%e4%b9%89></a></h1><p>见 aten/src/ATen/native/DispatchStub.h。<code>DispatchStub</code> 类型为：</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-cpp data-lang=cpp><span style=display:flex><span><span style=color:#719e07>template</span> <span style=color:#719e07>&lt;</span><span style=color:#719e07>typename</span> rT, <span style=color:#719e07>typename</span> T, <span style=color:#719e07>typename</span>... Args<span style=color:#719e07>&gt;</span>
</span></span><span style=display:flex><span><span style=color:#719e07>struct</span> <span style=color:#268bd2>DispatchStub</span><span style=color:#719e07>&lt;</span>rT (<span style=color:#719e07>*</span>)(Args...), T<span style=color:#719e07>&gt;</span>;
</span></span></code></pre></div><p>其中主要包含几类方法，一是调用，会根据设备类型来选择函数指针，强制转换后调用：</p></div><footer class=post-footer><div readmore=true><div class=readmore-overlay><div class=readmore-overlay-center><div class=post-button><a class=btn href=/posts/programming/python/1.-PyTorch-C++-%E5%87%BD%E6%95%B0%E6%B4%BE%E5%8F%91/#more rel=contents>阅读全文 &#187;</a></div></div></div><div class=no-more-to-read><hr><center>没有啦~</center></div></div><div class=post-eof></div></footer></article></div><div class=post-block><article itemscope itemtype=http://schema.org/Article class=post-content lang><link itemprop=mainEntityOfPage href=https://hxhue.github.io/posts/cli/%E5%9C%A8%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E4%BD%BF%E7%94%A8-sing-box-%E4%BB%A3%E7%90%86/><span hidden itemprop=author itemscope itemtype=http://schema.org/Person><meta itemprop=image content="/imgs/371907.jpg"><meta itemprop=name content="🤖"></span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization><meta itemprop=name content="🤖"><meta itemprop=description content="个人博客，主要是零散的笔记。"></span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork><meta itemprop=name content="在服务器上使用 sing-box 代理"><meta itemprop=description content="背景

服务器上面不能科学上网。以前都是直接 
  
  
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    用 ssh 做代理转发 的，但是现在在校外只能使用 rvpn，连接非常不稳定，也无法大流量传输文件，只能考虑在服务器上直接上网。"></span><header class=post-header><h2 class=post-title itemprop="name headline"><a href=/posts/cli/%E5%9C%A8%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E4%BD%BF%E7%94%A8-sing-box-%E4%BB%A3%E7%90%86/ itemprop=url class=post-title-link>在服务器上使用 sing-box 代理</a></h2><div class=post-meta-container><div class=post-meta-items><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar"></i>
</span><span class=post-meta-item-text title=发表于>发表于：
</span><time title="创建时间：2025-05-01 00:00:00 +0800 CST" itemprop="dateCreated datePublished" datetime="2025-05-01 00:00:00 +0800 CST">2025-05-01</time></span></div></div></header><div class=post-body itemprop=articleBody><h1 id=背景>背景
<a class=header-anchor href=#%e8%83%8c%e6%99%af></a></h1><p>服务器上面不能科学上网。以前都是直接
<a href=/posts/cli/ssh/ssh-%E4%BB%A3%E7%90%86%E8%BD%AC%E5%8F%91%E8%AE%A9%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E5%A4%96%E7%BD%91/ title="用 ssh 做代理转发">用 ssh 做代理转发</a> 的，但是现在在校外只能使用 rvpn，连接非常不稳定，也无法大流量传输文件，只能考虑在服务器上直接上网。</p><p>平时使用 Windows 上的 v2rayN 上网，考虑将其配置迁移到服务器上。由于服务器是多人共享，我有以下需求：</p><ol><li>不用 systemd 启动。</li><li>配置私有化，且其他人不能看到配置。</li><li>用的时候临时启动，用完之后就退出进程。</li></ol><h1 id=服务器上的准备>服务器上的准备
<a class=header-anchor href=#%e6%9c%8d%e5%8a%a1%e5%99%a8%e4%b8%8a%e7%9a%84%e5%87%86%e5%a4%87></a></h1><ol><li>服务器上 <a href=https://sing-box.sagernet.org/zh/installation/package-manager/ title="下载安装 sing-box" rel="noopener external nofollow noreferrer" target=_blank class=exturl>下载安装 sing-box<i class="fa fa-external-link-alt"></i></a>。</li><li>服务器上创建一个文件夹 sing-box。</li><li>在 v2rayN 中点“设置 > 打开存储所在的位置”，文件夹中的 <code>bin\srss</code> 中包含大量 <code>geo*</code> 文件。将 <code>srss</code> 文件夹拷贝到服务器上的 sing-box 文件夹中。</li></ol><p>这一步下载了 sing-box 并且将一些公开配置复制到了服务器上。</p></div><footer class=post-footer><div readmore=true><div class=readmore-overlay><div class=readmore-overlay-center><div class=post-button><a class=btn href=/posts/cli/%E5%9C%A8%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E4%BD%BF%E7%94%A8-sing-box-%E4%BB%A3%E7%90%86/#more rel=contents>阅读全文 &#187;</a></div></div></div><div class=no-more-to-read><hr><center>没有啦~</center></div></div><div class=post-eof></div></footer></article></div><nav class=pagination><span class="page-number current">1</span>
<a class=page-number href=/posts/page/2/>2</a>
<a class=page-number href=/posts/page/3/>3</a>
<a class=page-number href=/posts/page/4/>4</a>
<a class=page-number href=/posts/page/5/>5</a>
<a class=page-number href=/posts/page/6/>6</a>
<a class=page-number href=/posts/page/7/>7</a>
<a class=page-number href=/posts/page/8/>8</a>
<a class="extend next" rel=next href=/posts/page/2/><i class="fa fa-angle-right"></i></a></nav></div></main><footer class=footer><div class=footer-inner><div class=copyright>&copy;
<span itemprop=copyrightYear>2023 - 2025
</span><span class=with-love><i class="fa fa-heart"></i>
</span><span class=author itemprop=copyrightHolder>🤖</span></div><div class=powered-by>由 <a href=https://gohugo.io title=0.143.1 target=_blank>Hugo</a> & <a href=https://github.com/hugo-next/hugo-theme-next title=4.5.3 target=_blank>Hugo NexT.Gemini</a> 强力驱动</div></div></footer><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js defer></script><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.1.0/medium-zoom.min.js defer></script><script class=next-config data-name=main type=application/json>{"bookmark":{"color":"#222","enable":false,"save":"manual"},"copybtn":true,"darkmode":false,"hostname":"https://hxhue.github.io/","i18n":{"ds_day":" 天前","ds_days":" 天 ","ds_hour":" 小时前","ds_hours":" 小时 ","ds_just":"刚刚","ds_min":" 分钟前","ds_mins":" 分钟","ds_month":" 个月前","ds_years":" 年 ","empty":"没有找到任何搜索结果：${query}","hits":"找到 ${hits} 个搜索结果","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","placeholder":"搜索..."},"lang":"zh-CN","lazyload":false,"localSearch":{"enable":true,"limit":1e3,"path":"/searchindexes.xml","preload":false,"topnperarticle":-1,"trigger":"auto","unescape":false},"motion":{"async":true,"enable":false,"transition":{"collheader":"fadeInLeft","menu_item":"fadeInDown","postblock":"fadeIn","postbody":"fadeInDown","postheader":"fadeInDown","sidebar":"fadeInUp"}},"postmeta":{"comments":{"enable":false,"plugin":"waline"},"views":{"enable":false,"plugin":"busuanzi"}},"root":"/","scheme":"Gemini","sidebar":{"display":"post","offset":12,"padding":18,"position":"left","width":256},"vendor":{"plugins":"cdnjs","router":"https://cdnjs.cloudflare.com/ajax/libs"},"version":"4.5.3"}</script><script type=text/javascript src=/js/main.min.37ba8b54f9d4d784d08028c45eea93b5d4e13eda8ee7fb0d2edd6f3fac66cfd2.js defer></script></body></html>