<!doctype html><html lang=zh-CN data-theme=light><head><meta charset=UTF-8><meta name=viewport content="width=device-width"><meta name=theme-color content="#222" media="(prefers-color-scheme: light)"><meta name=generator content="Hugo 0.143.1"><link rel="shortcut icon" type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=/imgs/icons/favicon_16x16_next.png><link rel=icon type=image/png sizes=32x32 href=/imgs/icons/favicon_32_32_next.png><link rel=apple-touch-icon sizes=180x180 href=/imgs/icons/apple_touch_icon_next.png><meta itemprop=name content="CUDA by Example: Chapter 06-08"><meta itemprop=description content="个人博客，主要是零散的笔记。"><meta itemprop=datePublished zgotmplz><meta itemprop=dateModified zgotmplz><meta itemprop=image content="https://hxhue.github.io/imgs/371907.jpg"><meta itemprop=keywords content="cuda,cuda-by-example"><link type=text/css rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.2/css/all.min.css><link type=text/css rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css><link rel=stylesheet href=/css/main.min.bea76f574a755574e17d42bea39502a74ca3ca4db65807b8c82d3e26dcec8420.css><style type=text/css>.post-footer,.flinks-list-footer hr:after{content:"~ 我可是有底线的哟 ~"}</style><link rel=stylesheet type=text/css href=/css/style.css><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/github-markdown-css@5.3.0/github-markdown-dark.css><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js></script><script>MathJax={tex:{displayMath:[["$$","$$"],["\\[","\\]"]],inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"}}</script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.esm.min.mjs';
  mermaid.initialize({ startOnLoad: false });
  mermaid.mermaidAPI.initialize();
  window.mermaid = mermaid;
</script><script type=text/javascript>(function(){localDB={set:function(e,t,n){if(n===0)return;const s=new Date,o=n*864e5,i={value:t,expiry:s.getTime()+o};localStorage.setItem(e,JSON.stringify(i))},get:function(e){const t=localStorage.getItem(e);if(!t)return void 0;const n=JSON.parse(t),s=new Date;return s.getTime()>n.expiry?(localStorage.removeItem(e),void 0):n.value}},theme={active:function(){const e=localDB.get("theme");if(e==null)return;theme.toggle(e),window.matchMedia("(prefers-color-scheme: dark)").addListener(function(e){theme.toggle(e.matches?"dark":"light")})},toggle:function(e){document.documentElement.setAttribute("data-theme",e),localDB.set("theme",e,2);const t=document.querySelector("iframe.giscus-frame");if(t){const n={setConfig:{theme:e}};t.contentWindow.postMessage({giscus:n},"https://giscus.app")}}},theme.active()})(window)</script><script class=next-config data-name=page type=application/json>{"comments":false,"isHome":false,"isPage":true,"path":"CUDA-by-Example-chapter06-08","permalink":"https://hxhue.github.io/cuda-by-example/CUDA-by-Example-chapter06-08/","title":"CUDA by Example: Chapter 06-08","waline":{"js":[{"alias":"waline","alias_name":"@waline/client","file":"dist/pageview.js","name":"pageview","version":"2.13.0"},{"alias":"waline","alias_name":"@waline/client","file":"dist/comment.js","name":"comment","version":"2.13.0"}]}}</script><script type=text/javascript>document.addEventListener("DOMContentLoaded",()=>{var e=document.createElement("script");e.charset="UTF-8",e.src="",e.async=!1,e.defer=!0,document.head.appendChild(e),e.onload=function(){NexT.utils.fmtBusuanzi()}})</script><title>CUDA by Example: Chapter 06-08 - Bluegill</title><noscript><link rel=stylesheet href=/css/noscript.css></noscript></head><body itemscope itemtype=http://schema.org/WebPage><div class=headband></div><main class=main><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div class=toggle aria-label role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div></div><div class=site-meta><a href=/ class=brand rel=start><i class=logo-line></i><h1 class=site-title>Bluegill</h1><i class=logo-line></i></a><p class=site-subtitle itemprop=description></p></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href=/ class=hvr-icon-pulse rel=section><i class="fa fa-home hvr-icon"></i>首页</a></li><li class="menu-item menu-item-about"><a href=/about/ class=hvr-icon-pulse rel=section><i class="fa fa-user hvr-icon"></i>关于</a></li><li class="menu-item menu-item-archives"><a href=/archives/ class=hvr-icon-pulse rel=section><i class="fa fa-archive hvr-icon"></i>归档</a></li><li class="menu-item menu-item-categories"><a href=/categories/ class=hvr-icon-pulse rel=section><i class="fa fa-th hvr-icon"></i>分类</a></li><li class="menu-item menu-item-tags"><a href=/tags/ class=hvr-icon-pulse rel=section><i class="fa fa-hashtag hvr-icon"></i>标签</a></li><li class="menu-item menu-item-daily"><a href=/daily/ class=hvr-icon-pulse rel=section><i class="fa fa-newspaper hvr-icon"></i>随笔</a></li><li class="menu-item menu-item-discovery"><a href=https://rift-fear-f2c.notion.site/2025-1e354a33cfb1802c841bdf29f2f3dab3 class=hvr-icon-pulse rel=section><i class="fa fa-compass hvr-icon"></i>发现</a></li><li class="menu-item menu-item-search"><a role=button class="popup-trigger hvr-icon-pulse"><i class="fa fa-search fa-fw hvr-icon"></i>搜索</a></li></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon><i class="fa fa-search"></i></span><div class=search-input-container><input autocomplete=off autocapitalize=off maxlength=80 placeholder=搜索... spellcheck=false type=search class=search-input></div><span class=popup-btn-close role=button><i class="fa fa-times-circle"></i></span></div><div class="search-result-container no-result"><div class=search-result-icon><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></div><div class="toggle sidebar-toggle" role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div><aside class=sidebar><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class=sidebar-nav><li class=sidebar-nav-toc>文章目录</li><li class=sidebar-nav-overview>站点概览</li></ul><div class=sidebar-panel-container><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><nav id=TableOfContents><ul><li><a href=#第-6-章-constant-memory-and-events>第 6 章 Constant Memory and Events</a><ul><li><a href=#常量内存>常量内存</a></li><li><a href=#cuda-事件>CUDA 事件</a></li></ul></li><li><a href=#第-7-章-texture-memory>第 7 章 Texture Memory</a><ul><li><a href=#书中的实现方式在-cuda-113-废弃>书中的实现方式（在 CUDA 11.3 废弃）</a><ul><li><a href=#一维纹理内存>一维纹理内存</a></li><li><a href=#二维纹理内存>二维纹理内存</a></li></ul></li><li><a href=#cuda-12-之后的实现>CUDA 12 之后的实现</a></li></ul></li><li><a href=#第-8-章-cuda-和图形编程库的可互操作性>第 8 章 CUDA 和图形编程库的可互操作性</a></li></ul></nav></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author site-overview-item animated" itemprop=author itemscope itemtype=http://schema.org/Person><img class=site-author-image itemprop=image alt=🤖 src=/imgs/371907.jpg><p class=site-author-name itemprop=name>🤖</p><div class=site-description itemprop=description>个人博客，主要是零散的笔记。</div></div><div class="site-state-wrap site-overview-item animated"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/><span class=site-state-item-count>433</span>
<span class=site-state-item-name>日志</span></a></div><div class="site-state-item site-state-categories"><a href=/categories/><span class=site-state-item-count>12</span>
<span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/><span class=site-state-item-count>86</span>
<span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-social site-overview-item animated"><span class=links-of-social-item><a href=https://github.com/hxhue title="Github → https://github.com/hxhue" rel=noopener target=_blank><i class="fab fa-github fa-fw"></i>
Github
</a></span><span class=links-of-social-item><a href=/rss.xml title="RSS 订阅 → /rss.xml" rel=noopener target=_blank><i class="fa fa-rss fa-fw"></i>
RSS 订阅</a></span></div><div class="cc-license animated" itemprop=license><a href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh class=cc-opacity rel=noopener target=_blank title=共享知识><img src=/imgs/cc/big/by_nc_sa.svg alt=共享知识></a></div><div class="links-of-blogroll site-overview-item animated"><div class=links-of-blogroll-title><i class="fa fa-globe fa-fw"></i>
友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=https://shuai.guru/ title=https://shuai.guru/ target=_blank>shuai.guru</a></li></ul></div></div></div></div></aside><div class=sidebar-dimmer></div></header><div class=tool-buttons><div id=toggle-theme class=button title=深浅模式切换><i class="fas fa-adjust"></i></div><div class=back-to-top role=button title=返回顶部><i class="fa fa-arrow-up"></i>
<span>0%</span></div></div><div class=reading-progress-bar></div><noscript><div class=noscript-warning>Theme NexT works best with JavaScript enabled</div></noscript><div class="main-inner post posts-expand"><div class=post-block><article itemscope itemtype=http://schema.org/Article class=post-content lang><link itemprop=mainEntityOfPage href=https://hxhue.github.io/cuda-by-example/CUDA-by-Example-chapter06-08/><span hidden itemprop=author itemscope itemtype=http://schema.org/Person><meta itemprop=image content="/imgs/371907.jpg"><meta itemprop=name content="🤖"></span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization><meta itemprop=name content="🤖"><meta itemprop=description content="个人博客，主要是零散的笔记。"></span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork><meta itemprop=name content="CUDA by Example: Chapter 06-08"><meta itemprop=description content="第 6 章 Constant Memory and Events

常量内存

常量内存是在全局区域声明的。如果漏掉了 __constant__ 关键字，就会将其定义在全局内存区域，尽管存储方式、分配的时机和用 cudaMalloc 申请的内存有一些差异。"></span><header class=post-header><h1 class=post-title itemprop="name headline">CUDA by Example: Chapter 06-08</h1><div class=post-meta-container><div class=post-meta-items><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar"></i>
</span><span class=post-meta-item-text title=发表于>发表于：
</span><time title="创建时间：2024-02-06 00:00:00 +0800 CST" itemprop="dateCreated datePublished" datetime="2024-02-06 00:00:00 +0800 CST">2024-02-06
</time></span><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar-check"></i>
</span><span class=post-meta-item-text title=更新于>更新于：
</span><time title=修改时间：2024-04-08T00:00:00+08:00 itemprop=dateModified datetime=2024-04-08T00:00:00+08:00>2024-04-08</time>
</span><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-folder-open"></i>
</span><span class=post-meta-item-text title=分类于>分类于：
</span><span itemprop=about itemscope itemtype=http://schema.org/Thing><a href=/categories/cuda-by-example itemprop=url rel=index><span itemprop=name>cuda-by-example</span></a></span></span></div><div class=post-meta-items><span class=post-meta-item title=字数><span class=post-meta-item-icon><i class="far fa-file-word"></i>
</span><span class=post-meta-item-text>字数：</span>
<span>1815</span>
</span><span class=post-meta-item title=阅读><span class=post-meta-item-icon><i class="far fa-clock"></i>
</span><span class=post-meta-item-text>阅读：&ap;</span>
<span>4分钟</span></span></div></div></header><div class=post-body itemprop=articleBody><h1 id=第-6-章-constant-memory-and-events>第 6 章 Constant Memory and Events
<a class=header-anchor href=#%e7%ac%ac-6-%e7%ab%a0-constant-memory-and-events></a></h1><h2 id=常量内存>常量内存
<a class=header-anchor href=#%e5%b8%b8%e9%87%8f%e5%86%85%e5%ad%98></a></h2><p>常量内存是在全局区域声明的。如果漏掉了 <code>__constant__</code> 关键字，就会将其定义在全局内存区域，尽管存储方式、分配的时机和用 <code>cudaMalloc</code> 申请的内存有一些差异。</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-c data-lang=c><span style=display:flex><span>__constant__ Sphere s[SPHERES];
</span></span></code></pre></div><p>常量内存的内存拷贝方法比较特殊：</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-c data-lang=c><span style=display:flex><span><span style=color:#268bd2>HANDLE_ERROR</span>( <span style=color:#268bd2>cudaMemcpyToSymbol</span>( s, temp_s,
</span></span><span style=display:flex><span>                                  <span style=color:#719e07>sizeof</span>(Sphere) <span style=color:#719e07>*</span> SPHERES) );
</span></span></code></pre></div><p>CUDA 线程对常量内存是只读的，也就是只有 host 能操作常量内存。通过将反复读取的数据移动到常量内存区域而不是全局内存，可以加速。但是要注意常量内存的大小非常有限（<a href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#features-and-technical-specifications-technical-specifications-per-compute-capability title rel="noopener external nofollow noreferrer" target=_blank class=exturl><i class="fa fa-external-link-alt"></i></a>）。书中的例子只是对 20 个球体做光线追踪。</p><p>常量内存为什么快：</p><ol><li>有 constant cache。</li><li>对常量内存的读取可以被广播到临近的线程。节省读取次数。</li></ol><p>什么叫做临近线程？GPU 中线程执行是按照 warp 分组的。如果同一组的都需要同一个 constant memory 中的数据，那么 GPU 只会产生一个访问请求，这样会很节省带宽。（需要全局内存时难道不能用这个方法节省带宽吗？）</p><blockquote><p>In the CUDA Architecture, a warp refers to a collection of 32 threads that are &ldquo;woven together&rdquo; and get executed in lockstep. <strong>At every line in your program, each thread in a warp executes the same instruction on different data.</strong></p><p>When it comes to handling constant memory, NVIDIA hardware can broadcast a single memory read to each half-warp. A half-warp—not nearly as creatively named as a warp—is a group of 16 threads: half of a 32-thread warp. If every thread in a half-warp requests data from the same address in constant memory, <strong>your GPU will generate only a single read request and subsequently broadcast the data to every thread</strong>.</p></blockquote><p>常量内存广播策略也可能使得性能降低，比如在半 warp 中每个线程访问不同内存地址的时候，请求不仅不能合并，还只能序列化发出：</p><blockquote><p>Unfortunately, there can potentially be a downside to performance when using constant memory. The half-warp broadcast feature is in actuality a double-edged sword. Although it can dramatically accelerate performance when all 16 threads are reading the same address, it actually slows performance to a crawl when all 16 threads read different addresses.</p><p>For example, <strong>if all 16 threads in a half-warp need different data from constant memory, the 16 different reads get serialized, effectively taking 16 times the amount of time to place the request.</strong> If they were reading from conventional global memory, the request could be issued at the same time. In this case, reading from constant memory would probably be slower than using global memory.</p></blockquote><h2 id=cuda-事件>CUDA 事件
<a class=header-anchor href=#cuda-%e4%ba%8b%e4%bb%b6></a></h2><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-c data-lang=c><span style=display:flex><span><span style=color:#dc322f>cudaEvent_t</span>     start, stop;
</span></span><span style=display:flex><span><span style=color:#268bd2>HANDLE_ERROR</span>( <span style=color:#268bd2>cudaEventCreate</span>( <span style=color:#719e07>&amp;</span>start ) );
</span></span><span style=display:flex><span><span style=color:#268bd2>HANDLE_ERROR</span>( <span style=color:#268bd2>cudaEventCreate</span>( <span style=color:#719e07>&amp;</span>stop ) );
</span></span><span style=display:flex><span><span style=color:#268bd2>HANDLE_ERROR</span>( <span style=color:#268bd2>cudaEventRecord</span>( start, <span style=color:#2aa198>0</span> ) );
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#586e75>// ...
</span></span></span><span style=display:flex><span><span style=color:#586e75></span><span style=color:#268bd2>HANDLE_ERROR</span>( <span style=color:#268bd2>cudaEventRecord</span>( stop, <span style=color:#2aa198>0</span> ) );
</span></span><span style=display:flex><span><span style=color:#268bd2>HANDLE_ERROR</span>( <span style=color:#268bd2>cudaEventSynchronize</span>( stop ) );
</span></span><span style=display:flex><span><span style=color:#dc322f>float</span>   elapsedTime;
</span></span><span style=display:flex><span><span style=color:#268bd2>HANDLE_ERROR</span>( <span style=color:#268bd2>cudaEventElapsedTime</span>( <span style=color:#719e07>&amp;</span>elapsedTime,
</span></span><span style=display:flex><span>                                    start, stop ) );
</span></span><span style=display:flex><span><span style=color:#268bd2>printf</span>( <span style=color:#2aa198>&#34;Time to generate:  %3.1f ms</span><span style=color:#cb4b16>\n</span><span style=color:#2aa198>&#34;</span>, elapsedTime );
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#268bd2>HANDLE_ERROR</span>( <span style=color:#268bd2>cudaEventDestroy</span>( start ) );
</span></span><span style=display:flex><span><span style=color:#268bd2>HANDLE_ERROR</span>( <span style=color:#268bd2>cudaEventDestroy</span>( stop ) );
</span></span></code></pre></div><p><code>cudaEventSynchronize</code> 非常重要，因为 host 和 device 之间是异步执行的。</p><p>cudaEvent 发生在 device 上，只适合记录 device 上的执行（也就是 GPU 任务），用它来计算 CPU 上的时间是不正确的。</p><p>测试结果：用了常量内存之后，在 Debug 模式下，书中给的代码执行时间从 645.1ms 和 3.3ms 降低到了 466ms 和 2.7ms。（较大的那个时间可能是 GPU 调度的时间，也可能是 Windows 上双显卡设备中独显从不活跃状态转换成活跃状态的时间。不清楚原因。）没有书里面降低的那么夸张，但是还是有很大的降低。</p><h1 id=第-7-章-texture-memory>第 7 章 Texture Memory
<a class=header-anchor href=#%e7%ac%ac-7-%e7%ab%a0-texture-memory></a></h1><p>纹理内存也是只读和有缓存的。纹理内存要和全局内存绑定，所以说纹理内存对应的位置并不是完全不可以被修改，只是透过纹理内存的访问机制无法被修改而已。</p><blockquote><p><strong>Like constant memory, texture memory is cached on chip</strong>, so in some situations it will provide higher effective bandwidth by reducing memory requests to off-chip DRAM.</p></blockquote><p>纹理内存对<strong>空间中临近地址</strong>的访问有很好的局部性支持。对于一般的 CPU 来说，这样的访问很不利于 cache，但是纹理内存的缓存机制不同。因而当传统的 cache 机制对于要解决的问题不友好时，可以考虑 texture memory。</p><p><img src=/assets/abbe859dbe86f69d4bdb92bc3b3efc65.webp></p><p>书上纹理内存相关的代码不能编译，据说是 CUDA 12 的 API 有了变化。</p><blockquote><p>NVIDIA removed support for texture references in CUDA 12.0. NVIDIA told CUDA programmers that they should switch to texture objects (as they planned to remove texture references) for five years prior to that.</p></blockquote><p><a href=https://stackoverflow.com/a/67197817/ title=https://stackoverflow.com/a/67197817/ rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://stackoverflow.com/a/67197817/<i class="fa fa-external-link-alt"></i></a></p><h2 id=书中的实现方式在-cuda-113-废弃>书中的实现方式（在 CUDA 11.3 废弃）
<a class=header-anchor href=#%e4%b9%a6%e4%b8%ad%e7%9a%84%e5%ae%9e%e7%8e%b0%e6%96%b9%e5%bc%8f%e5%9c%a8-cuda-113-%e5%ba%9f%e5%bc%83></a></h2><h3 id=一维纹理内存>一维纹理内存
<a class=header-anchor href=#%e4%b8%80%e7%bb%b4%e7%ba%b9%e7%90%86%e5%86%85%e5%ad%98></a></h3><p>首先创建 texture。</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-c data-lang=c><span style=display:flex><span><span style=color:#586e75>// 1D 纹理引用
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>texture<span style=color:#719e07>&lt;</span><span style=color:#dc322f>float</span><span style=color:#719e07>&gt;</span> texConstSrc;
</span></span><span style=display:flex><span>texture<span style=color:#719e07>&lt;</span><span style=color:#dc322f>float</span><span style=color:#719e07>&gt;</span> texIn;
</span></span><span style=display:flex><span>texture<span style=color:#719e07>&lt;</span><span style=color:#dc322f>float</span><span style=color:#719e07>&gt;</span> texOut;
</span></span></code></pre></div><p>然后将 texture 用 <code>cudaBindTexture</code> 绑定到已经申请的全局内存上。</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-c data-lang=c><span style=display:flex><span><span style=color:#268bd2>HANDLE_ERROR</span>( <span style=color:#268bd2>cudaBindTexture</span>( <span style=color:#b58900>NULL</span>, texConstSrc,
</span></span><span style=display:flex><span>                               data.dev_constSrc,
</span></span><span style=display:flex><span>                               imageSize ) );
</span></span></code></pre></div><blockquote><p>And since texture references must be declared globally at file scope, we can no longer pass the input and output buffers as parameters to <code>blend_kernel()</code> because the compiler needs to know at compile time which textures <code>tex1Dfetch()</code> should be sampling.</p></blockquote><p>这里的 <code>blend_kernel</code> 是书中要实现的一个函数，而 <code>tex1Dfetch</code> 是 CUDA 的 compiler intrinsic，用来读取纹理内存。</p><p>书中用到了 <code>texIn</code> 和 <code>texOut</code> 两个缓冲区，用来实现 2D 热交换的模拟。但是因为 <code>tex1Dfetch</code> 的纹理引用参数需要在编译时确定（offset 参数可以动态确定），所以使用了一个 bool 标志，以近似于复制粘贴的方式写了两个分支：</p><p><img src=/assets/06675ef859dd72ec109cf5ac10a501bc.webp></p><p>结束的时候要用 <code>cudaUnbindTexture</code> 来解除纹理内存的绑定。</p><h3 id=二维纹理内存>二维纹理内存
<a class=header-anchor href=#%e4%ba%8c%e7%bb%b4%e7%ba%b9%e7%90%86%e5%86%85%e5%ad%98></a></h3><p>声明的时候多了一个维度参数。</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-c data-lang=c><span style=display:flex><span>texture<span style=color:#719e07>&lt;</span><span style=color:#dc322f>float</span>,<span style=color:#2aa198>2</span><span style=color:#719e07>&gt;</span> texConstSrc;
</span></span><span style=display:flex><span>texture<span style=color:#719e07>&lt;</span><span style=color:#dc322f>float</span>,<span style=color:#2aa198>2</span><span style=color:#719e07>&gt;</span> texIn;
</span></span><span style=display:flex><span>texture<span style=color:#719e07>&lt;</span><span style=color:#dc322f>float</span>,<span style=color:#2aa198>2</span><span style=color:#719e07>&gt;</span> texOut;
</span></span></code></pre></div><p>读取则使用 <code>tex2D</code>，和 <code>tex1DFetch</code> 很类似，但是接受两个下标参数（这样就不用自己计算 offset）。</p><p>绑定的方式也发生了变化，增加了 <code>cudaChannelFormatDesc</code> 和 2D 的长度（之前是 1D，所以只需要给出总长度）：</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-c data-lang=c><span style=display:flex><span>cudaChannelFormatDesc desc <span style=color:#719e07>=</span> cudaCreateChannelDesc<span style=color:#719e07>&lt;</span><span style=color:#dc322f>float</span><span style=color:#719e07>&gt;</span>();
</span></span><span style=display:flex><span><span style=color:#268bd2>HANDLE_ERROR</span>( <span style=color:#268bd2>cudaBindTexture2D</span>( <span style=color:#b58900>NULL</span>, texConstSrc,
</span></span><span style=display:flex><span>                                 data.dev_constSrc,
</span></span><span style=display:flex><span>                                 desc, DIM, DIM,
</span></span><span style=display:flex><span>                                 <span style=color:#719e07>sizeof</span>(<span style=color:#dc322f>float</span>) <span style=color:#719e07>*</span> DIM ) );
</span></span></code></pre></div><p>最后同样是要接触纹理内存的绑定。</p><p>根据书中所述，1D 纹理内存和 2D 纹理内存的性能相同。所以应该根据代码编写的方便性来选择。</p><h2 id=cuda-12-之后的实现>CUDA 12 之后的实现
<a class=header-anchor href=#cuda-12-%e4%b9%8b%e5%90%8e%e7%9a%84%e5%ae%9e%e7%8e%b0></a></h2><p>参考 <a href=https://developer.nvidia.com/blog/cuda-pro-tip-kepler-texture-objects-improve-performance-and-flexibility/ title=https://developer.nvidia.com/blog/cuda-pro-tip-kepler-texture-objects-improve-performance-and-flexibility/ rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://developer.nvidia.com/blog/cuda-pro-tip-kepler-texture-objects-improve-performance-and-flexibility/<i class="fa fa-external-link-alt"></i></a></p><blockquote><p>Kepler GPUs and CUDA 5.0 introduce a new feature called texture objects (sometimes called <em>bindless textures</em>, since they don’t require manual binding/unbinding) that greatly improves the usability and programmability of textures.</p></blockquote><p>纹理对象（<code>cudaTextureObject_t</code>）不需要在全局区域声明，不需要绑定和解绑，也不需要在编译时知道要操作的纹理对象，因而更加灵活。</p><p>访问的时候还是用同样的 <code>tex1Dfetch</code> 和 <code>tex2D</code> 函数模板，但是是另外一个重载版本。</p><p><mark>TODO</mark>（操作起来要复杂一点，待补充）</p><h1 id=第-8-章-cuda-和图形编程库的可互操作性>第 8 章 CUDA 和图形编程库的可互操作性
<a class=header-anchor href=#%e7%ac%ac-8-%e7%ab%a0-cuda-%e5%92%8c%e5%9b%be%e5%bd%a2%e7%bc%96%e7%a8%8b%e5%ba%93%e7%9a%84%e5%8f%af%e4%ba%92%e6%93%8d%e4%bd%9c%e6%80%a7></a></h1><p>很多 API 都过时了。主要讲的是 OpenGL，DirectX 也稍微提了一下。</p></div><footer class=post-footer><div class=post-tags><a href=/tags/cuda>cuda
</a><a href=/tags/cuda-by-example>cuda-by-example</a></div><div class=addthis_inline_share_toolbox style=text-align:center></div><hr><div class=post-nav><div class="post-nav-next post-nav-item"><a href=/cuda-by-example/CUDA-by-Example-chapter01-05/ rel=next title="CUDA by Example: Chapter 01-05"><i class="fa fa-chevron-left"></i> CUDA by Example: Chapter 01-05</a></div><div class="post-nav-prev post-nav-item"><a href=/cuda-by-example/CUDA-by-Example-chapter09-12/ rel=prev title="CUDA by Example: Chapter 09-12">CUDA by Example: Chapter 09-12
<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>&copy;
<span itemprop=copyrightYear>2023 - 2025
</span><span class=with-love><i class="fa fa-heart"></i>
</span><span class=author itemprop=copyrightHolder>🤖</span></div><div class=powered-by>由 <a href=https://gohugo.io title=0.143.1 target=_blank>Hugo</a> & <a href=https://github.com/hugo-next/hugo-theme-next title=4.5.3 target=_blank>Hugo NexT.Gemini</a> 强力驱动</div></div></footer><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js defer></script><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.1.0/medium-zoom.min.js defer></script><script class=next-config data-name=main type=application/json>{"bookmark":{"color":"#222","enable":false,"save":"manual"},"copybtn":true,"darkmode":false,"hostname":"https://hxhue.github.io/","i18n":{"ds_day":" 天前","ds_days":" 天 ","ds_hour":" 小时前","ds_hours":" 小时 ","ds_just":"刚刚","ds_min":" 分钟前","ds_mins":" 分钟","ds_month":" 个月前","ds_years":" 年 ","empty":"没有找到任何搜索结果：${query}","hits":"找到 ${hits} 个搜索结果","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","placeholder":"搜索..."},"lang":"zh-CN","lazyload":false,"localSearch":{"enable":true,"limit":1e3,"path":"/searchindexes.xml","preload":false,"topnperarticle":-1,"trigger":"auto","unescape":false},"motion":{"async":true,"enable":false,"transition":{"collheader":"fadeInLeft","menu_item":"fadeInDown","postblock":"fadeIn","postbody":"fadeInDown","postheader":"fadeInDown","sidebar":"fadeInUp"}},"postmeta":{"comments":{"enable":false,"plugin":"waline"},"views":{"enable":false,"plugin":"busuanzi"}},"root":"/","scheme":"Gemini","sidebar":{"display":"post","offset":12,"padding":18,"position":"left","width":256},"vendor":{"plugins":"cdnjs","router":"https://cdnjs.cloudflare.com/ajax/libs"},"version":"4.5.3"}</script><script type=text/javascript src=/js/main.min.37ba8b54f9d4d784d08028c45eea93b5d4e13eda8ee7fb0d2edd6f3fac66cfd2.js defer></script></body></html>