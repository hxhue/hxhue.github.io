<!doctype html><html lang=zh-CN data-theme=light><head><meta charset=UTF-8><meta name=viewport content="width=device-width"><meta name=theme-color content="#222" media="(prefers-color-scheme: light)"><meta name=generator content="Hugo 0.143.1"><link rel="shortcut icon" type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=/imgs/icons/favicon_16x16_next.png><link rel=icon type=image/png sizes=32x32 href=/imgs/icons/favicon_32_32_next.png><link rel=apple-touch-icon sizes=180x180 href=/imgs/icons/apple_touch_icon_next.png><meta itemprop=name content="CUDA by Example: Chapter 09-12"><meta itemprop=description content="个人博客，主要是零散的笔记。"><meta itemprop=datePublished zgotmplz><meta itemprop=dateModified zgotmplz><meta itemprop=image content="https://hxhue.github.io/imgs/371907.jpg"><meta itemprop=keywords content="cuda,cuda-by-example"><link type=text/css rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.2/css/all.min.css><link type=text/css rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css><link rel=stylesheet href=/css/main.min.bea76f574a755574e17d42bea39502a74ca3ca4db65807b8c82d3e26dcec8420.css><style type=text/css>.post-footer,.flinks-list-footer hr:after{content:"~ 我可是有底线的哟 ~"}</style><link rel=stylesheet type=text/css href=/css/style.css><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/github-markdown-css@5.3.0/github-markdown-dark.css><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js></script><script>MathJax={tex:{displayMath:[["$$","$$"],["\\[","\\]"]],inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"}}</script><script type=module>
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.esm.min.mjs';
  mermaid.initialize({ startOnLoad: false });
  mermaid.mermaidAPI.initialize();
  window.mermaid = mermaid;
</script><script type=text/javascript>(function(){localDB={set:function(e,t,n){if(n===0)return;const s=new Date,o=n*864e5,i={value:t,expiry:s.getTime()+o};localStorage.setItem(e,JSON.stringify(i))},get:function(e){const t=localStorage.getItem(e);if(!t)return void 0;const n=JSON.parse(t),s=new Date;return s.getTime()>n.expiry?(localStorage.removeItem(e),void 0):n.value}},theme={active:function(){const e=localDB.get("theme");if(e==null)return;theme.toggle(e),window.matchMedia("(prefers-color-scheme: dark)").addListener(function(e){theme.toggle(e.matches?"dark":"light")})},toggle:function(e){document.documentElement.setAttribute("data-theme",e),localDB.set("theme",e,2);const t=document.querySelector("iframe.giscus-frame");if(t){const n={setConfig:{theme:e}};t.contentWindow.postMessage({giscus:n},"https://giscus.app")}}},theme.active()})(window)</script><script class=next-config data-name=page type=application/json>{"comments":false,"isHome":false,"isPage":true,"path":"CUDA-by-Example-chapter09-12","permalink":"https://hxhue.github.io/cuda-by-example/CUDA-by-Example-chapter09-12/","title":"CUDA by Example: Chapter 09-12","waline":{"js":[{"alias":"waline","alias_name":"@waline/client","file":"dist/pageview.js","name":"pageview","version":"2.13.0"},{"alias":"waline","alias_name":"@waline/client","file":"dist/comment.js","name":"comment","version":"2.13.0"}]}}</script><script type=text/javascript>document.addEventListener("DOMContentLoaded",()=>{var e=document.createElement("script");e.charset="UTF-8",e.src="",e.async=!1,e.defer=!0,document.head.appendChild(e),e.onload=function(){NexT.utils.fmtBusuanzi()}})</script><title>CUDA by Example: Chapter 09-12 - Bluegill</title><noscript><link rel=stylesheet href=/css/noscript.css></noscript></head><body itemscope itemtype=http://schema.org/WebPage><div class=headband></div><main class=main><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div class=toggle aria-label role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div></div><div class=site-meta><a href=/ class=brand rel=start><i class=logo-line></i><h1 class=site-title>Bluegill</h1><i class=logo-line></i></a><p class=site-subtitle itemprop=description></p></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href=/ class=hvr-icon-pulse rel=section><i class="fa fa-home hvr-icon"></i>首页</a></li><li class="menu-item menu-item-about"><a href=/about/ class=hvr-icon-pulse rel=section><i class="fa fa-user hvr-icon"></i>关于</a></li><li class="menu-item menu-item-archives"><a href=/archives/ class=hvr-icon-pulse rel=section><i class="fa fa-archive hvr-icon"></i>归档</a></li><li class="menu-item menu-item-categories"><a href=/categories/ class=hvr-icon-pulse rel=section><i class="fa fa-th hvr-icon"></i>分类</a></li><li class="menu-item menu-item-tags"><a href=/tags/ class=hvr-icon-pulse rel=section><i class="fa fa-hashtag hvr-icon"></i>标签</a></li><li class="menu-item menu-item-daily"><a href=/daily/ class=hvr-icon-pulse rel=section><i class="fa fa-newspaper hvr-icon"></i>随笔</a></li><li class="menu-item menu-item-discovery"><a href=https://rift-fear-f2c.notion.site/2025-1e354a33cfb1802c841bdf29f2f3dab3 class=hvr-icon-pulse rel=section><i class="fa fa-compass hvr-icon"></i>发现</a></li><li class="menu-item menu-item-search"><a role=button class="popup-trigger hvr-icon-pulse"><i class="fa fa-search fa-fw hvr-icon"></i>搜索</a></li></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon><i class="fa fa-search"></i></span><div class=search-input-container><input autocomplete=off autocapitalize=off maxlength=80 placeholder=搜索... spellcheck=false type=search class=search-input></div><span class=popup-btn-close role=button><i class="fa fa-times-circle"></i></span></div><div class="search-result-container no-result"><div class=search-result-icon><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></div><div class="toggle sidebar-toggle" role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div><aside class=sidebar><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class=sidebar-nav><li class=sidebar-nav-toc>文章目录</li><li class=sidebar-nav-overview>站点概览</li></ul><div class=sidebar-panel-container><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><nav id=TableOfContents><ul><li><a href=#第-9-章-原子操作>第 9 章 原子操作</a></li><li><a href=#第-10-章-streams>第 10 章 Streams</a><ul><li><a href=#page-locked-host-memory>Page-Locked Host Memory</a></li><li><a href=#检查-cuda-属性中的-deviceoverlap>检查 CUDA 属性中的 <code>deviceOverlap</code></a></li><li><a href=#为核函数调用和-cudamemcpyasync-指定流>为核函数调用和 <code>cudaMemcpyAsync</code> 指定流</a></li><li><a href=#使用多个流>使用多个流</a></li></ul></li><li><a href=#第-11-章>第 11 章</a><ul><li><a href=#零拷贝cpu-和-gpu-之间的内存映射>零拷贝、CPU 和 GPU 之间的内存映射</a></li><li><a href=#多-gpu-计算>多 GPU 计算</a></li><li><a href=#portable-pinned-memory>Portable Pinned Memory</a></li></ul></li><li><a href=#第-12-章-cuda-工具>第 12 章 CUDA 工具</a><ul><li><a href=#数学库>数学库</a></li><li><a href=#其他>其他</a></li></ul></li></ul></nav></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author site-overview-item animated" itemprop=author itemscope itemtype=http://schema.org/Person><img class=site-author-image itemprop=image alt=🤖 src=/imgs/371907.jpg><p class=site-author-name itemprop=name>🤖</p><div class=site-description itemprop=description>个人博客，主要是零散的笔记。</div></div><div class="site-state-wrap site-overview-item animated"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/><span class=site-state-item-count>433</span>
<span class=site-state-item-name>日志</span></a></div><div class="site-state-item site-state-categories"><a href=/categories/><span class=site-state-item-count>12</span>
<span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/><span class=site-state-item-count>86</span>
<span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-social site-overview-item animated"><span class=links-of-social-item><a href=https://github.com/hxhue title="Github → https://github.com/hxhue" rel=noopener target=_blank><i class="fab fa-github fa-fw"></i>
Github
</a></span><span class=links-of-social-item><a href=/rss.xml title="RSS 订阅 → /rss.xml" rel=noopener target=_blank><i class="fa fa-rss fa-fw"></i>
RSS 订阅</a></span></div><div class="cc-license animated" itemprop=license><a href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh class=cc-opacity rel=noopener target=_blank title=共享知识><img src=/imgs/cc/big/by_nc_sa.svg alt=共享知识></a></div><div class="links-of-blogroll site-overview-item animated"><div class=links-of-blogroll-title><i class="fa fa-globe fa-fw"></i>
友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=https://shuai.guru/ title=https://shuai.guru/ target=_blank>shuai.guru</a></li></ul></div></div></div></div></aside><div class=sidebar-dimmer></div></header><div class=tool-buttons><div id=toggle-theme class=button title=深浅模式切换><i class="fas fa-adjust"></i></div><div class=back-to-top role=button title=返回顶部><i class="fa fa-arrow-up"></i>
<span>0%</span></div></div><div class=reading-progress-bar></div><noscript><div class=noscript-warning>Theme NexT works best with JavaScript enabled</div></noscript><div class="main-inner post posts-expand"><div class=post-block><article itemscope itemtype=http://schema.org/Article class=post-content lang><link itemprop=mainEntityOfPage href=https://hxhue.github.io/cuda-by-example/CUDA-by-Example-chapter09-12/><span hidden itemprop=author itemscope itemtype=http://schema.org/Person><meta itemprop=image content="/imgs/371907.jpg"><meta itemprop=name content="🤖"></span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization><meta itemprop=name content="🤖"><meta itemprop=description content="个人博客，主要是零散的笔记。"></span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork><meta itemprop=name content="CUDA by Example: Chapter 09-12"><meta itemprop=description content="第 9 章 原子操作


  You should know that atomic operations on global memory are supported only on GPUs of compute capability 1.1 or higher. Furthermore, atomic operations on shared memory require a GPU of compute capability 1.2 or higher.
指定计算能力：
nvcc -arch=sm_11
这样就指定了计算能力是 1.1，当有些指令是只有 1.1 才能编译时加这个参数可以确保编译。同时，有了更加精确的生成目标，nvcc 可以执行一些和硬件相关的优化手段，这些优化手段在更早的架构上可能没有。"></span><header class=post-header><h1 class=post-title itemprop="name headline">CUDA by Example: Chapter 09-12</h1><div class=post-meta-container><div class=post-meta-items><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar"></i>
</span><span class=post-meta-item-text title=发表于>发表于：
</span><time title="创建时间：2024-02-02 00:00:00 +0800 CST" itemprop="dateCreated datePublished" datetime="2024-02-02 00:00:00 +0800 CST">2024-02-02
</time></span><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar-check"></i>
</span><span class=post-meta-item-text title=更新于>更新于：
</span><time title=修改时间：2025-04-26T00:00:00+08:00 itemprop=dateModified datetime=2025-04-26T00:00:00+08:00>2025-04-26</time>
</span><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-folder-open"></i>
</span><span class=post-meta-item-text title=分类于>分类于：
</span><span itemprop=about itemscope itemtype=http://schema.org/Thing><a href=/categories/cuda-by-example itemprop=url rel=index><span itemprop=name>cuda-by-example</span></a></span></span></div><div class=post-meta-items><span class=post-meta-item title=字数><span class=post-meta-item-icon><i class="far fa-file-word"></i>
</span><span class=post-meta-item-text>字数：</span>
<span>3727</span>
</span><span class=post-meta-item title=阅读><span class=post-meta-item-icon><i class="far fa-clock"></i>
</span><span class=post-meta-item-text>阅读：&ap;</span>
<span>8分钟</span></span></div></div></header><div class=post-body itemprop=articleBody><h1 id=第-9-章-原子操作>第 9 章 原子操作
<a class=header-anchor href=#%e7%ac%ac-9-%e7%ab%a0-%e5%8e%9f%e5%ad%90%e6%93%8d%e4%bd%9c></a></h1><blockquote><p>You should know that atomic operations on <strong>global memory</strong> are supported only on GPUs of compute capability 1.1 or higher. Furthermore, atomic operations on <strong>shared memory</strong> require a GPU of compute capability 1.2 or higher.</p></blockquote><p>指定计算能力：</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-bash data-lang=bash><span style=display:flex><span>nvcc -arch<span style=color:#719e07>=</span>sm_11
</span></span></code></pre></div><p>这样就指定了计算能力是 1.1，当有些指令是只有 1.1 才能编译时加这个参数可以确保编译。同时，有了更加精确的生成目标，nvcc 可以执行一些和硬件相关的优化手段，这些优化手段在更早的架构上可能没有。</p><p>但是一个硬件上不一定支持给定的计算能力。通过 <code>-arch-ls</code> 可以列出设备支持的计算能力：</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-bash data-lang=bash><span style=display:flex><span>$ nvcc -arch-ls
</span></span><span style=display:flex><span>compute_50
</span></span><span style=display:flex><span>compute_52
</span></span><span style=display:flex><span>compute_53
</span></span><span style=display:flex><span>compute_60
</span></span><span style=display:flex><span>compute_61
</span></span><span style=display:flex><span>compute_62
</span></span><span style=display:flex><span>compute_70
</span></span><span style=display:flex><span>compute_72
</span></span><span style=display:flex><span>compute_75
</span></span><span style=display:flex><span>compute_80
</span></span><span style=display:flex><span>compute_86
</span></span><span style=display:flex><span>compute_87
</span></span><span style=display:flex><span>compute_89
</span></span><span style=display:flex><span>compute_90
</span></span></code></pre></div><p>例子是直方图计算，也就是统计各个值的元素的数量。书中的例子是对随机填充生成的字符数组的计数，每个字符值不超过 256。</p><p>可以用 <code>prop.multiProcessorCount</code> 来查询设备有的 SM 的数量。我们实验室项目则是直接对 blocks 数量使用了一个定数，没有查询 SM 数量。</p><p>第一个直方图的尝试是用 <code>atomicAdd</code> 直接在全局内存上做原子加法。但是这样的性能相当差，甚至在书中所用的硬件上比 CPU 上的计算慢了好几倍。用自己的设备测试，则 CPU、全局内存原子加法、共享内存原子加法耗时分别为 182ms
、62.5ms、30ms。虽然全局内存上的原子加法开销没有那么离谱，但是也比共享内存上的加法慢了很多。</p><p>计算直方图的可以用共享内存优化，先在各个块内用原子加法计算好块中的每个元素的数量，然后一口气用原子加法加到全局内存上去。虽然计算的过程多了，使用原子加法的次数也看起来变多了，但实际上却能节省很多时间！</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-c data-lang=c><span style=display:flex><span>__global__ <span style=color:#dc322f>void</span> <span style=color:#268bd2>histo_kernel</span>( <span style=color:#dc322f>unsigned</span> <span style=color:#dc322f>char</span> <span style=color:#719e07>*</span>buffer,
</span></span><span style=display:flex><span>                              <span style=color:#dc322f>long</span> size,
</span></span><span style=display:flex><span>                              <span style=color:#dc322f>unsigned</span> <span style=color:#dc322f>int</span> <span style=color:#719e07>*</span>histo ) {
</span></span><span style=display:flex><span>    <span style=color:#586e75>// clear out the accumulation buffer called temp
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>    <span style=color:#586e75>// since we are launched with 256 threads, it is easy
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>    <span style=color:#586e75>// to clear that memory with one write per thread
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>    __shared__  <span style=color:#dc322f>unsigned</span> <span style=color:#dc322f>int</span> temp[<span style=color:#2aa198>256</span>];
</span></span><span style=display:flex><span>    temp[threadIdx.x] <span style=color:#719e07>=</span> <span style=color:#2aa198>0</span>;
</span></span><span style=display:flex><span>    <span style=color:#268bd2>__syncthreads</span>();
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#586e75>// calculate the starting index and the offset to the next
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>    <span style=color:#586e75>// block that each thread will be processing
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>    <span style=color:#dc322f>int</span> i <span style=color:#719e07>=</span> threadIdx.x <span style=color:#719e07>+</span> blockIdx.x <span style=color:#719e07>*</span> blockDim.x;
</span></span><span style=display:flex><span>    <span style=color:#dc322f>int</span> stride <span style=color:#719e07>=</span> blockDim.x <span style=color:#719e07>*</span> gridDim.x;
</span></span><span style=display:flex><span>    <span style=color:#719e07>while</span> (i <span style=color:#719e07>&lt;</span> size) {
</span></span><span style=display:flex><span>        <span style=color:#268bd2>atomicAdd</span>( <span style=color:#719e07>&amp;</span>temp[buffer[i]], <span style=color:#2aa198>1</span> );
</span></span><span style=display:flex><span>        i <span style=color:#719e07>+=</span> stride;
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>    <span style=color:#586e75>// sync the data from the above writes to shared memory
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>    <span style=color:#586e75>// then add the shared memory values to the values from
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>    <span style=color:#586e75>// the other thread blocks using global memory
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>    <span style=color:#586e75>// atomic adds
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>    <span style=color:#586e75>// same as before, since we have 256 threads, updating the
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>    <span style=color:#586e75>// global histogram is just one write per thread!
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>    <span style=color:#268bd2>__syncthreads</span>();
</span></span><span style=display:flex><span>    <span style=color:#268bd2>atomicAdd</span>( <span style=color:#719e07>&amp;</span>(histo[threadIdx.x]), temp[threadIdx.x] );
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h1 id=第-10-章-streams>第 10 章 Streams
<a class=header-anchor href=#%e7%ac%ac-10-%e7%ab%a0-streams></a></h1><h2 id=page-locked-host-memory>Page-Locked Host Memory
<a class=header-anchor href=#page-locked-host-memory></a></h2><p>CUDA 提供了一个函数是 <code>cudaHostAlloc</code>，这个函数能够分配 host 端的内存。并且分配的内存是页锁定的（page-locked）或者说是固定的（pinned memory）。固定的内存不能被操作系统换出到外存，永远都存在于内存系统，因而其物理地址保持长期有效性，可以被硬件访问。CUDA 从 CPU 向 GPU 拷贝内存要使用 DMA，如果原先的内存就是页锁定的，则只会拷贝一次。如果原先的内存不是页锁定的，需要先将其拷贝到页锁定的内存上，然后拷贝到 GPU 上。<code>malloc</code> 创建的内存没有页锁定的性质。</p><p><strong>结论：<code>cudaHostAlloc</code> 分配的页锁定的 host 内存更适合和 GPU 之间做数据交换</strong>。</p><blockquote><p>However, you should resist the temptation to simply do a search-and-replace on malloc to convert every one of your calls to use <code>cudaHostAlloc()</code>.</p></blockquote><p>缺点是分配页锁定的内存更容易 OOM，也容易影响到系统上其他的应用程序。</p><blockquote><p>We suggest trying to restrict their use to memory that will be used as a source or destination in calls to <code>cudaMemcpy() </code>and freeing them when they are no longer needed rather than waiting until application shutdown to release the memory.</p></blockquote><p>用 <code>cudaHostAlloc</code> 创建的内存要用 <code>cudaFreeHost</code> 来释放。注意两个函数的命名规则有不一致的地方。</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-c data-lang=c><span style=display:flex><span><span style=color:#268bd2>HANDLE_ERROR</span>( <span style=color:#268bd2>cudaHostAlloc</span>( (<span style=color:#dc322f>void</span><span style=color:#719e07>**</span>)<span style=color:#719e07>&amp;</span>a,
</span></span><span style=display:flex><span>                             size <span style=color:#719e07>*</span> <span style=color:#719e07>sizeof</span>( <span style=color:#719e07>*</span>a ),
</span></span><span style=display:flex><span>                             cudaHostAllocDefault ) );
</span></span><span style=display:flex><span><span style=color:#586e75>// ...
</span></span></span><span style=display:flex><span><span style=color:#586e75></span><span style=color:#268bd2>HANDLE_ERROR</span>( <span style=color:#268bd2>cudaFreeHost</span>( a ) );
</span></span></code></pre></div><div class="markdown-alert markdown-alert-caution"><p class=markdown-alert-title><svg class="octicon octicon-stop mr-2" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="M4.47.22A.749.749.0 015 0h6c.199.0.389.079.53.22l4.25 4.25c.141.14.22.331.22.53v6a.749.749.0 01-.22.53l-4.25 4.25A.749.749.0 0111 16H5a.749.749.0 01-.53-.22L.22 11.53A.749.749.0 010 11V5c0-.199.079-.389.22-.53zm.84 1.28L1.5 5.31v5.38l3.81 3.81h5.38l3.81-3.81V5.31L10.69 1.5zM8 4a.75.75.0 01.75.75v3.5a.75.75.0 01-1.5.0v-3.5A.75.75.0 018 4zm0 8a1 1 0 110-2 1 1 0 010 2z"/></svg>Caution</p><p>我使用书中所给的测试代码发现 <code>cudaHostAlloc</code> 分配内存的拷贝反而更慢。将 <code>SIZE</code> 改小之后观察到 <code>cudaHostAlloc</code> 分配的内存比用 <code>malloc</code> 分配的稍微快了一些。可能是因为系统内存不够大，用页锁定内存反而影响性能了。</p></div><h2 id=检查-cuda-属性中的-deviceoverlap>检查 CUDA 属性中的 <code>deviceOverlap</code>
<a class=header-anchor href=#%e6%a3%80%e6%9f%a5-cuda-%e5%b1%9e%e6%80%a7%e4%b8%ad%e7%9a%84-deviceoverlap></a></h2><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-c data-lang=c><span style=display:flex><span>cudaDeviceProp prop;
</span></span><span style=display:flex><span><span style=color:#dc322f>int</span> whichDevice;
</span></span><span style=display:flex><span><span style=color:#268bd2>HANDLE_ERROR</span>( <span style=color:#268bd2>cudaGetDevice</span>( <span style=color:#719e07>&amp;</span>whichDevice ) );
</span></span><span style=display:flex><span><span style=color:#268bd2>HANDLE_ERROR</span>( <span style=color:#268bd2>cudaGetDeviceProperties</span>( <span style=color:#719e07>&amp;</span>prop, whichDevice ) );
</span></span><span style=display:flex><span><span style=color:#719e07>if</span> (<span style=color:#719e07>!</span>prop.deviceOverlap) {
</span></span><span style=display:flex><span>  <span style=color:#586e75>// ...
</span></span></span><span style=display:flex><span><span style=color:#586e75></span>}
</span></span></code></pre></div><p>这段代码检查当前设备是否具有 <code>deviceOverlap</code> 功能。有这个功能的设备能够同时执行 kernel 并且在 CPU 和 GPU 之间异步拷贝数据。</p><blockquote><p>A GPU supporting <strong>device overlap</strong> possesses the capacity to <strong>simultaneously execute a CUDA C kernel while performing a copy between device and host memory</strong>.</p></blockquote><p>书中创建了 <code>cudaEvent_t</code> 但是没有销毁，自己写的话还是要记得销毁。 <a href=https://stackoverflow.com/a/10943498/ title=https://stackoverflow.com/a/10943498/ rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://stackoverflow.com/a/10943498/<i class="fa fa-external-link-alt"></i></a></p><h2 id=为核函数调用和-cudamemcpyasync-指定流>为核函数调用和 <code>cudaMemcpyAsync</code> 指定流
<a class=header-anchor href=#%e4%b8%ba%e6%a0%b8%e5%87%bd%e6%95%b0%e8%b0%83%e7%94%a8%e5%92%8c-cudamemcpyasync-%e6%8c%87%e5%ae%9a%e6%b5%81></a></h2><p><strong><code>cudaMemcpyAsync</code> 要求 host 端的内存必须是 pinned memory</strong>，这和 <code>cudaMemcpy</code> 不强制要求内存类型不同。</p><p><code>cudaMemcpy</code> 是同步的，当函数执行完成时，可以认为数据已经完成了复制。而 <code>cudaMemcpyAsync</code> 是异步的，只能保证它比同一个 <code>cudaStream_t</code> 中的后加入的任务先完成，并不能保证返回时就完成了（甚至可能函数返回时任务还没有开始）。</p><p>下面这个核函数调用也有 stream 参数（以往书中例子尖括号里都是只有两个参数，现在有 4 个），这时的核函数调用也是异步调用：</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-c data-lang=c><span style=display:flex><span>kernel<span style=color:#719e07>&lt;&lt;&lt;</span>N<span style=color:#719e07>/</span><span style=color:#2aa198>256</span>,<span style=color:#2aa198>256</span>,<span style=color:#2aa198>0</span>,stream<span style=color:#719e07>&gt;&gt;&gt;</span>( dev_a, dev_b, dev_c );
</span></span></code></pre></div><p>创建流是很简单的（但是记得用完要销毁）：</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-c data-lang=c><span style=display:flex><span><span style=color:#dc322f>cudaStream_t</span> stream;
</span></span><span style=display:flex><span><span style=color:#268bd2>HANDLE_ERROR</span>( <span style=color:#268bd2>cudaStreamCreate</span>( <span style=color:#719e07>&amp;</span>stream ) );
</span></span></code></pre></div><p>用 <code>­cudaStreamSynchronize</code> 函数去同步一个流，也就是等待这个流上的所有操作都完成。（类似的，如果将 CUDA event 当成一个瞬间完成的低开销标记，同步一个 CUDA event 是不是也是等待这个 event 在流上的执行/标记完成呢？）</p><p>流的类型是指针而不是整数，所以将 0 赋值给流其实是指定了空指针。</p><h2 id=使用多个流>使用多个流
<a class=header-anchor href=#%e4%bd%bf%e7%94%a8%e5%a4%9a%e4%b8%aa%e6%b5%81></a></h2><blockquote><p>In fact, the execution timeline can be even more favorable than this; some newer NVIDIA GPUs support simultaneous kernel execution and two memory copies, one to the device and one from the device.</p></blockquote><p>先看结果：</p><div class="markdown-alert markdown-alert-note"><p class=markdown-alert-title><svg class="octicon octicon-info mr-2" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1116 0A8 8 0 010 8zm8-6.5a6.5 6.5.0 100 13 6.5 6.5.0 000-13zM6.5 7.75A.75.75.0 017.25 7h1a.75.75.0 01.75.75v2.75h.25a.75.75.0 010 1.5h-2a.75.75.0 010-1.5h.25v-2h-.25a.75.75.0 01-.75-.75zM8 6a1 1 0 110-2 1 1 0 010 2z"/></svg>Note</p><p>在我的电脑上，basic_single_stream.cu 和 basic_double_stream.cu 耗时 26ms，而 basic_double_stream_correct.cu 耗时 21.4ms。</p></div><p>书的作者所用硬件的耗时优化结果如下（和我的差不多都是节省了 20% 时间）：</p><blockquote><p>The new code runs in 48ms, a 21 percent improvement over our original, naïve double-stream implementation.</p></blockquote><p>书中的 basic_double_stream.cu 将 chunks 拆分成了两组，偶数使用 stream0，奇数使用 stream1，但是最后的性能和使用单个流没有区别。</p><p>下图左边是错误的编码方式，右边是正确的编码方式：</p><p><img src=/assets/e2af4d188a4a3774c2ef96b8589fb58a.webp></p><p>由于任务是按照顺序加入到 stream 中的，而 stream 只保证同步关系，和硬件的执行无关，所以左边的写法会出现硬件中一个流的任务阻塞了另外一个流的任务的情况。毕竟严格的顺序执行并不违背同步关系。</p><p><img src=/assets/1483c6573eb752b4db5e52292ca44856.webp></p><p>考虑左边的代码，在上图，由于前三个 <code>cudaMemcpyAsync</code> 都在第二个流的首个操作，即 <code>cudaMemcpyAsync</code> 之前，所以第二个流会等待这三个 <code>cudaMemcpyAsync</code> 执行完才会开始。但是第三个 <code>cudaMemcpyAsync</code> 是第一个流的最后一条任务，所以两个流时间上是顺序执行的，和一个流的执行时间没有区别！</p><p>如果 GPU 只支持一次执行一个拷贝和 kernel 操作，那么按照后一种交错指定任务的写法，只有流 1 数据拷贝回 CPU 的操作能够和流 2 的 kernel A 运行并行化。但是有些比较好的 GPU 支持一次执行一个 kernel 操作和多个数据拷贝操作，这会让节约的时间更多。</p><p>注意在计算完成之后还要对用到的两个流同步。（<code>cudaStreamSynchronize</code>）</p><h1 id=第-11-章>第 11 章
<a class=header-anchor href=#%e7%ac%ac-11-%e7%ab%a0></a></h1><h2 id=零拷贝cpu-和-gpu-之间的内存映射>零拷贝、CPU 和 GPU 之间的内存映射
<a class=header-anchor href=#%e9%9b%b6%e6%8b%b7%e8%b4%9dcpu-%e5%92%8c-gpu-%e4%b9%8b%e9%97%b4%e7%9a%84%e5%86%85%e5%ad%98%e6%98%a0%e5%b0%84></a></h2><p>查询内存映射支持：</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-c data-lang=c><span style=display:flex><span>cudaDeviceProp  prop;
</span></span><span style=display:flex><span><span style=color:#dc322f>int</span> whichDevice;
</span></span><span style=display:flex><span><span style=color:#268bd2>HANDLE_ERROR</span>( <span style=color:#268bd2>cudaGetDevice</span>( <span style=color:#719e07>&amp;</span>whichDevice ) );
</span></span><span style=display:flex><span><span style=color:#268bd2>HANDLE_ERROR</span>( <span style=color:#268bd2>cudaGetDeviceProperties</span>( <span style=color:#719e07>&amp;</span>prop, whichDevice ) );
</span></span><span style=display:flex><span><span style=color:#719e07>if</span> (prop.canMapHostMemory <span style=color:#719e07>!=</span> <span style=color:#2aa198>1</span>) {
</span></span><span style=display:flex><span>    <span style=color:#268bd2>printf</span>( <span style=color:#2aa198>&#34;Device can not map memory.</span><span style=color:#cb4b16>\n</span><span style=color:#2aa198>&#34;</span> );
</span></span><span style=display:flex><span>    <span style=color:#719e07>return</span> <span style=color:#2aa198>0</span>;
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>启用内存映射：</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-c data-lang=c><span style=display:flex><span><span style=color:#268bd2>HANDLE_ERROR</span>( <span style=color:#268bd2>cudaSetDeviceFlags</span>( cudaDeviceMapHost ) );
</span></span></code></pre></div><p>在申请 host 内存时启用内存映射（注意现在用的 flag 不是 default 了，<code>cudaHostAllocMapped</code> 表示内存映射，<code>cudaHostAllocWriteCombined</code> 则是选择了 CPU 的 cache 方式，这种方式对于读少的情况有利）：</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-c data-lang=c><span style=display:flex><span><span style=color:#268bd2>HANDLE_ERROR</span>( <span style=color:#268bd2>cudaHostAlloc</span>( (<span style=color:#dc322f>void</span><span style=color:#719e07>**</span>)<span style=color:#719e07>&amp;</span>a,
</span></span><span style=display:flex><span>                          size<span style=color:#719e07>*</span><span style=color:#719e07>sizeof</span>(<span style=color:#dc322f>float</span>),
</span></span><span style=display:flex><span>                          cudaHostAllocWriteCombined <span style=color:#719e07>|</span> cudaHostAllocMapped ) );
</span></span></code></pre></div><p>cudaHostAlloc 申请的指针总是在 CPU 内存区域的。现在启用映射之后，可以将其转换成 GPU 区域的指针，有了 GPU 区域的指针才能在 GPU 上读写：</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-c data-lang=c><span style=display:flex><span><span style=color:#268bd2>HANDLE_ERROR</span>( <span style=color:#268bd2>cudaHostGetDevicePointer</span>( <span style=color:#719e07>&amp;</span>dev_a, a, <span style=color:#2aa198>0</span> ) );
</span></span><span style=display:flex><span><span style=color:#586e75>//                                                 ^ 这个参数是 flag
</span></span></span></code></pre></div><p>内存映射有一点特殊，核函数对映射区域的修改需要同步才对 CPU 可见（因为本身 GPU 和 CPU 就是异步的），在 host 端对 GPU 同步需要调用 <code>cudaDeviceSynchronize</code>：</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-c data-lang=c><span style=display:flex><span>dot<span style=color:#719e07>&lt;&lt;&lt;</span>blocksPerGrid,threadsPerBlock<span style=color:#719e07>&gt;&gt;&gt;</span>( size, dev_a, dev_b,
</span></span><span style=display:flex><span>                                        dev_partial_c );
</span></span><span style=display:flex><span><span style=color:#268bd2>HANDLE_ERROR</span>( <span style=color:#268bd2>cudaDeviceSynchronize</span>() );
</span></span></code></pre></div><div class="markdown-alert markdown-alert-caution"><p class=markdown-alert-title><svg class="octicon octicon-stop mr-2" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="M4.47.22A.749.749.0 015 0h6c.199.0.389.079.53.22l4.25 4.25c.141.14.22.331.22.53v6a.749.749.0 01-.22.53l-4.25 4.25A.749.749.0 0111 16H5a.749.749.0 01-.53-.22L.22 11.53A.749.749.0 010 11V5c0-.199.079-.389.22-.53zm.84 1.28L1.5 5.31v5.38l3.81 3.81h5.38l3.81-3.81V5.31L10.69 1.5zM8 4a.75.75.0 01.75.75v3.5a.75.75.0 01-1.5.0v-3.5A.75.75.0 018 4zm0 8a1 1 0 110-2 1 1 0 010 2z"/></svg>Caution</p><p><code>cudaThreadSynchronize</code> 已经是过时的函数（书中使用），过时原因是其名称不能正确反映其功能。现在应该用 <code>cudaDeviceSynchronize</code>。</p></div><p>之前书里的 dot 计算主要做的是：</p><ol><li>申请设备内存</li><li>将数据用 <code>cudaMemcpy</code> 的方式拷贝到设备内存</li><li>调用 kernel 计算</li><li>用 <code>cudaMemcpy</code> 将数据拷贝回 CPU</li><li>完成其他处理</li><li>释放资源</li></ol><p>为什么这个过程不需要同步呢？因为 <code>cudaMemcpy</code> 是在 kernel 之后发起的，虽然 kernel 的执行是异步的，但是 <code>cudaMemcpy</code> 的执行是同步的。CPU 要等待 <code>cudaMemcpy</code>，而 <code>cudaMemcpy</code> 要等待 kernel，所以达成了同步关系。</p><p>使用 <code>cudaMemcpyAsync</code> 还要注意同步其他流。这个时候也达成了同步。</p><p>但是这里的新例子中 kernel 之后没有天然的同步措施，所以要使用 <code>cudaDeviceSynchronize</code> 对设备同步，保证 CPU 能够看到正确的结果。</p><p><strong>什么时候用内存映射</strong>？当典型的只拷贝一次到设备内存，计算完成之后就拷贝回去时，这种方法比较高效。但是要注意：如果对内存区域读写次数很多，这种方法反而更慢；映射内存必须是页锁定内存，给系统内存会带来负担。</p><blockquote><p>In cases where inputs and outputs are used exactly once, we will even see a performance enhancement when using zero-copy memory with a discrete GPU.</p></blockquote><p>书中还有提到在核显的时候用内存映射肯定是有提升的。（但是核显一般不是 N 卡）</p><h2 id=多-gpu-计算>多 GPU 计算
<a class=header-anchor href=#%e5%a4%9a-gpu-%e8%ae%a1%e7%ae%97></a></h2><p>首先是要用 <code>cudaGetDeviceCount</code> 查询 GPU 数量。</p><p><strong>然后每个 GPU 设备要专门用一个 CPU 线程来控制</strong>。可能如果不用多个线程，<code>cudaSetDevice</code> 就会冲突，无法使用多个设备。</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-c data-lang=c><span style=display:flex><span>CUTThread   <span style=color:#268bd2>thread</span> <span style=color:#719e07>=</span> <span style=color:#268bd2>start_thread</span>( routine, <span style=color:#719e07>&amp;</span>(data[<span style=color:#2aa198>0</span>]) );
</span></span><span style=display:flex><span><span style=color:#268bd2>routine</span>( <span style=color:#719e07>&amp;</span>(data[<span style=color:#2aa198>1</span>]) );
</span></span><span style=display:flex><span><span style=color:#268bd2>end_thread</span>( <span style=color:#268bd2>thread</span> );
</span></span></code></pre></div><p>这里的例子中 <code>routine</code> 是一个函数，而 <code>start_thread</code> 和 <code>end_thread</code> 都是本书给出的，<strong>实际上是调用了 pthread 库</strong>。</p><h2 id=portable-pinned-memory>Portable Pinned Memory
<a class=header-anchor href=#portable-pinned-memory></a></h2><div class="markdown-alert markdown-alert-caution"><p class=markdown-alert-title><svg class="octicon octicon-stop mr-2" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="M4.47.22A.749.749.0 015 0h6c.199.0.389.079.53.22l4.25 4.25c.141.14.22.331.22.53v6a.749.749.0 01-.22.53l-4.25 4.25A.749.749.0 0111 16H5a.749.749.0 01-.53-.22L.22 11.53A.749.749.0 010 11V5c0-.199.079-.389.22-.53zm.84 1.28L1.5 5.31v5.38l3.81 3.81h5.38l3.81-3.81V5.31L10.69 1.5zM8 4a.75.75.0 01.75.75v3.5a.75.75.0 01-1.5.0v-3.5A.75.75.0 018 4zm0 8a1 1 0 110-2 1 1 0 010 2z"/></svg>Caution</p><p>Once you have set the device on a particular thread, you cannot call <code>cudaSetDevice()</code> again, even if you pass the same device identifier.</p></div><div class="markdown-alert markdown-alert-note"><p class=markdown-alert-title><svg class="octicon octicon-info mr-2" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1116 0A8 8 0 010 8zm8-6.5a6.5 6.5.0 100 13 6.5 6.5.0 000-13zM6.5 7.75A.75.75.0 017.25 7h1a.75.75.0 01.75.75v2.75h.25a.75.75.0 010 1.5h-2a.75.75.0 010-1.5h.25v-2h-.25a.75.75.0 01-.75-.75zM8 6a1 1 0 110-2 1 1 0 010 2z"/></svg>Note</p><p>如果不使用 cudaSetDevice，那么就用默认设备 0。根据书上所说，一旦对当前线程设置了设备就不能再次设置，否则会出错。但实际上我对同一个设备调用多次也没有出错。</p></div><p>本章并没有马上申请映射内存，而是使用了很早之前讲过的先复制到 GPU 计算完成再复制到 CPU 的方案。这是因为本章涉及了多设备，同时也涉及了多线程。使用一般方法申请的 pinned memory 只是对于作为资源申请者的那一个线程是页锁定的，对于其他线程来说是非页锁定的，这会造成在使用 cudaMemcpy 时不必要的拷贝开销。要想申请 portable pinned memory，也就是对所有线程可见，需要额外添加 <code>cudaHostAllocPortable</code> 标志：</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-c data-lang=c><span style=display:flex><span><span style=color:#268bd2>HANDLE_ERROR</span>( <span style=color:#268bd2>cudaHostAlloc</span>( (<span style=color:#dc322f>void</span><span style=color:#719e07>**</span>)<span style=color:#719e07>&amp;</span>b, N<span style=color:#719e07>*</span><span style=color:#719e07>sizeof</span>(<span style=color:#dc322f>float</span>),
</span></span><span style=display:flex><span>                          cudaHostAllocWriteCombined <span style=color:#719e07>|</span>
</span></span><span style=display:flex><span>                          cudaHostAllocPortable      <span style=color:#719e07>|</span>
</span></span><span style=display:flex><span>                          cudaHostAllocMapped ) );
</span></span></code></pre></div><h1 id=第-12-章-cuda-工具>第 12 章 CUDA 工具
<a class=header-anchor href=#%e7%ac%ac-12-%e7%ab%a0-cuda-%e5%b7%a5%e5%85%b7></a></h1><h2 id=数学库>数学库
<a class=header-anchor href=#%e6%95%b0%e5%ad%a6%e5%ba%93></a></h2><p>CUDA Toolkit 包含这两个库：CUFFT 和 CUBLAS。</p><p>CUFFT：快速傅里叶变换库。</p><p>CUBLAS：基本线性代数子程序库。</p><blockquote><p>This library, named CUBLAS, is also freely available and supports a large subset of the <strong>full BLAS package</strong>.（BLAS 是用 FORTRAN 实现的库。）</p></blockquote><h2 id=其他>其他
<a class=header-anchor href=#%e5%85%b6%e4%bb%96></a></h2><p>GPU Computing SDK：在 CUDA driver 和 CUDA Toolkit 之外。其中也附有一些示例代码，这些代码都是学习用，并不是 SOTA。</p><p>NVIDIA Performance Primitives (NPP)：主要用于图形和视频处理。</p><p>调试工具：cuda-gdb 和 cuda-memcheck（感觉有点像 valgrind 在 GPU 上的改造？）。NVIDIA Parallel Nsight 是最开始在 Windows 上支持 CUDA 调试的工具。</p><p>Profiler：CUDA Visual Profiler。</p></div><footer class=post-footer><div class=post-tags><a href=/tags/cuda>cuda
</a><a href=/tags/cuda-by-example>cuda-by-example</a></div><div class=addthis_inline_share_toolbox style=text-align:center></div><hr><div class=post-nav><div class="post-nav-next post-nav-item"><a href=/cuda-by-example/CUDA-by-Example-chapter06-08/ rel=next title="CUDA by Example: Chapter 06-08"><i class="fa fa-chevron-left"></i> CUDA by Example: Chapter 06-08</a></div><div class="post-nav-prev post-nav-item"></div></div></footer></article></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>&copy;
<span itemprop=copyrightYear>2023 - 2025
</span><span class=with-love><i class="fa fa-heart"></i>
</span><span class=author itemprop=copyrightHolder>🤖</span></div><div class=powered-by>由 <a href=https://gohugo.io title=0.143.1 target=_blank>Hugo</a> & <a href=https://github.com/hugo-next/hugo-theme-next title=4.5.3 target=_blank>Hugo NexT.Gemini</a> 强力驱动</div></div></footer><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js defer></script><script type=text/javascript src=https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.1.0/medium-zoom.min.js defer></script><script class=next-config data-name=main type=application/json>{"bookmark":{"color":"#222","enable":false,"save":"manual"},"copybtn":true,"darkmode":false,"hostname":"https://hxhue.github.io/","i18n":{"ds_day":" 天前","ds_days":" 天 ","ds_hour":" 小时前","ds_hours":" 小时 ","ds_just":"刚刚","ds_min":" 分钟前","ds_mins":" 分钟","ds_month":" 个月前","ds_years":" 年 ","empty":"没有找到任何搜索结果：${query}","hits":"找到 ${hits} 个搜索结果","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","placeholder":"搜索..."},"lang":"zh-CN","lazyload":false,"localSearch":{"enable":true,"limit":1e3,"path":"/searchindexes.xml","preload":false,"topnperarticle":-1,"trigger":"auto","unescape":false},"motion":{"async":true,"enable":false,"transition":{"collheader":"fadeInLeft","menu_item":"fadeInDown","postblock":"fadeIn","postbody":"fadeInDown","postheader":"fadeInDown","sidebar":"fadeInUp"}},"postmeta":{"comments":{"enable":false,"plugin":"waline"},"views":{"enable":false,"plugin":"busuanzi"}},"root":"/","scheme":"Gemini","sidebar":{"display":"post","offset":12,"padding":18,"position":"left","width":256},"vendor":{"plugins":"cdnjs","router":"https://cdnjs.cloudflare.com/ajax/libs"},"version":"4.5.3"}</script><script type=text/javascript src=/js/main.min.37ba8b54f9d4d784d08028c45eea93b5d4e13eda8ee7fb0d2edd6f3fac66cfd2.js defer></script></body></html>